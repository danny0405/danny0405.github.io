<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>玉兆的博客</title>
  <subtitle>心如止水</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://danny0405.github.io/"/>
  <updated>2018-01-23T15:05:29.371Z</updated>
  <id>https://danny0405.github.io/</id>
  
  <author>
    <name>chenyuzhao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink 分布式快照的设计-存储</title>
    <link href="https://danny0405.github.io/2017/12/24/Flink-%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E7%9A%84%E8%AE%BE%E8%AE%A1-%E5%AD%98%E5%82%A8/"/>
    <id>https://danny0405.github.io/2017/12/24/Flink-分布式快照的设计-存储/</id>
    <published>2017-12-24T08:04:08.000Z</published>
    <updated>2018-01-23T15:05:29.371Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>分布式快照是整个 Flink 计算框架中非常核心的模块，Flink 的 checkpoint、状态存储都依赖于其分布式快照；不仅框架自身借助于 chandy-lamda 算法，实现了算子状态的快照和恢复，也对用户暴露了一套简洁的状态存储的 API，用户无需关心快照自身的容错/扩展/一致性，这些 Flink 都已对用户透明；由于分布式快照这块的设计比较复杂，因此将拆成两篇文章来介绍，本篇文章主要介绍分布式快照存储部分的设计，下一篇会介绍快照的流程和细节。</p>
<p>代码参考版本: 1.5-SNAPSHOT</p>
<h2 id="核心模块"><a href="#核心模块" class="headerlink" title="核心模块"></a>核心模块</h2><p>Flink 的分布式快照存储部分设计抽象出了大致 5 个层次：</p>
<ul>
<li>最底层是快照的物理存储，包括内存和文件系统两种形式</li>
<li>再上层是 CheckpointStreamFactory：封装了具体的存储交互，也就是内存/文件系统读写</li>
<li>再上层是 StateBackend：封装了工作状态的存储逻辑，包括内存和 RocksDB 两种形式</li>
<li>再上层是 KeyedStateBackend：封装了快照的读写细节，快照分区策略等</li>
<li>再上层是 State：封装了与 KeyedStateBackend 交互时状态的 val 序列化/反序列化 等逻辑</li>
</ul>
<p><em>还有一个 StateContext 比较特殊，它不提供快照功能，只提供临时的状态读写，下面会讲到</em><br>核心模块的总体交互图:<br><img src="checkpoint-sys-interact.png" alt="checkpoint-sys-interact.png"><br><a id="more"></a></p>
<h3 id="StateSnapshotContext"><a href="#StateSnapshotContext" class="headerlink" title="StateSnapshotContext"></a>StateSnapshotContext</h3><p>StateSnapshotContext 接口对用户提供了不被托管的工作状态 <code>Raw State</code> 的读写， Flink 的快照包括两种类型：被系统 StateBackend 托管的、不被托管自己写的。StateSnapshotContext 提供了算子自己写的快照流，它有一个默认实现 <code>StateSnapshotContextSynchronousImpl</code>，用户可以通过该实现拿到如下信息：</p>
<ul>
<li>checkpoint-id: 当前 checkpoint 的 id，是一个单调递增的 long 型值</li>
<li>checkpoint-timestamp: 当前 checkpoint 的时间</li>
<li>KeyedStateCheckpointOutputStream: 带 key 的状态 CheckpointStateOutputStream</li>
<li>OperatorStateCheckpointOutputStream: 不带 key 的算子自己的 CheckpointStateOutputStream</li>
</ul>
<p>其中 KeyedStateCheckpointOutputStream/OperatorStateCheckpointOutputStream 是在内部 CheckpointStreamFactory 基础上提供的高层抽象，可以将快照内容写入，而屏蔽了底层的实现细节[包括快照的 partition、句柄的管理等]，下面会一起介绍。</p>
<p><strong>特别注意的是</strong>，KeyedStateCheckpointOutputStream/OperatorStateCheckpointOutputStream 继承了 NonClosingCheckpointOutputStream 接口，也就是内部输出流一旦打开，就不允许调用 close 接口直接关闭，它们的 close 由系统来管理。</p>
<p>KeyedStateCheckpointOutputStream/OperatorStateCheckpointOutputStream 属于框架之外【不通过 StateBackend 管理】的状态，也叫 <em>Raw State</em> ，通过它们读写的 state <em>不会被定期快照</em>，同时它们封装了状态 <em>分区</em> 的逻辑，可以对状态 partition，这里做下介绍：</p>
<h4 id="Raw-State-Stream"><a href="#Raw-State-Stream" class="headerlink" title="Raw State/Stream"></a>Raw State/Stream</h4><p><em>Raw State Stream 是不被 Flink 框架管理的，用户自己写的 keyed-operator 状态</em>，用户通过 StateSnapshotContext 拿到流之后便可以进行状态存取了。</p>
<p><strong>带 key 的 raw 状态流</strong>： KeyedStateCheckpointOutputStream，使用 KeyGroup 来管理分区，每个分区对应一个 KeyGroup，每个 KeyGroup 的写入状态 offset 在底层的存储媒介上连续，多个分片组合的 KeyGroup 使用 KeyGroupRange 来管理，KeyGroup 和 offset 对应关系由 KeyGroupRangeOffsets 管理。文件、KeyGroup、offset 的关系如下图：<br><img src="keygroup-offset-relation.png" alt="keygroup-offset-relation.png"></p>
<p><strong>算子自己写 raw 状态流</strong>：OperatorStateCheckpointOutputStream，直接保存了 partition 和 offset 的对应关系，每个 partition 在底层存储的写入 offset 也是连续的。</p>
<h3 id="CheckpointStreamFactory-CheckpointOutputStream"><a href="#CheckpointStreamFactory-CheckpointOutputStream" class="headerlink" title="CheckpointStreamFactory/CheckpointOutputStream"></a>CheckpointStreamFactory/CheckpointOutputStream</h3><p>CheckpointStreamFactory/CheckpointOutputStream 接口提供了一套基层的状态存储的基础实现，StateBackend 的状态存取都是封装在这个实现之上的。<br>Checkpoint 的 CheckpointStateOutputStream 统一由 CheckpointStreamFactory 来管理，接口类 <code>CheckpointStreamFactory</code> 定义了获取 CheckpointStateOutputStream 的接口函数，也通过内部类的方式定义了抽象类 <code>CheckpointStateOutputStream</code>，依据 checkpoint 底层存储媒介的不同，CheckpointStateOutputStream 大致有以下几种类型:</p>
<ul>
<li>FsCheckpointStreamFactory/FsCheckpointStateOutputStream: 将 checkpoint 写入[分布式]文件系统中</li>
<li>MemCheckpointStreamFactory/MemoryCheckpointOutputStream: 将 checkpoint 写入内存</li>
<li>FsSavepointStreamFactory: 与 FsCheckpointStreamFactory 基础实现一致，只是复用了 savepoint 目录作为快照目录</li>
</ul>
<p>CheckpointStreamFactory 的继承关系如下图:<br><img src="checkpoint-stream-factory-extend.png" alt="checkpoint-stream-factory-extend"><br>可以看到，CheckpointStreamFactory 的实现支持了文件系统和内存两种，特别地，对于 FsCheckpointStreamFactory，支持按 buffer 写入文件中，具体的策略是：</p>
<ul>
<li>支持用户自定义 buffer 大小，默认的最小大小为 4kb</li>
<li>支持写单个 byte：如果 buffer 已满，会先执行 flush 操作，将 buffer 中的数据清空并全部 spit 到磁盘，在写入该 byte</li>
<li>支持写 byte[]：如果 byte[] 的长度小于 buffer 的 1/2，先写 buffer，[如果写满的话依然执行 flush 操作]，后继续写 buffer</li>
<li>支持关闭文件流并拿到文件句柄，期间有任何异常发生都会清空内存数据及磁盘状态文件</li>
<li>底层状态文件的 path pattern 为：file-scheme/checkpoint-root-dir/job-id/chk-id/uuid</li>
</ul>
<p>CheckpointOutputStream 继承自内部的 FSDataOutputStream，FSDataOutputStream 继承自 java/OutputStream，具体的继承关系如下:<br><img src="checkpoint-stream-extend.png" alt="checkpoint-stream-extend"><br>上图中 FSDataOutputStream 是继承关系中被继承最多的抽象类，也是最核心的接口，它定义了核心接口的语义实现：</p>
<ul>
<li>getPos：获取写出流的当前 index [从 0 开始计数]</li>
<li>flush：将所有 buffer 中数据 flush 到输出流中，单次 flush 并不保证数据的完成性，而是由 <code>sync()</code> 和 <code>close()</code> 接口保证</li>
<li>sync：强制 flush 所有数据到物理磁盘</li>
<li>close：关闭输出流，该方法执行后，必须保证已写入的数据持久化并且可见，这就意味着，此方法必须阻塞等待数据的持久化语义实现，如果 close 期间发生异常，持久化/可见性 语义将得不到保证</li>
</ul>
<p>这里所介绍的通过 CheckpointStreamFactory 拿到的 XXXStateCheckpointOutputStream 属于内部使用的基础输出流，它只提供 byte/byte[] 写入类型，因此它主要被用作以下三种用途：</p>
<ul>
<li>JobManager checkpoint 存储以及 metadata 的恢复</li>
<li>被 keyed- state backend 用来做底层状态存储</li>
<li>被 operator state backend 用来做底层状态存储</li>
</ul>
<p>上面提到 Flink 内部通过 StateSnapshotContext 拿到高层抽象 KeyedStateCheckpointOutputStream/OperatorStateCheckpointOutputStream，其背后的调用链是：<br>-&gt; <code>StateSnapshotContext.getRawXXXOperatorStateOutput()</code><br>-&gt; <code>CheckpointStreamFactory.createCheckpointStateOutputStream()</code><br>-&gt; <code>CheckpointStateOutputStream.write()/flush()</code><br>-&gt; <code>HadoopDataOutputStream/LocalDataOutputStream.write()</code></p>
<p>KeyedStateCheckpointOutputStream/OperatorStateCheckpointOutputStream 面向用户，用户并不会接触到 HadoopDataOutputStream/LocalDataOutputStream 这些底层实现，而后者继承自 CheckpointStreamFactory 中定义的 CheckpointStateOutputStream。并且通过 KeyedStateCheckpointOutputStream/OperatorStateCheckpointOutputStream 写入的状态不被托管，<strong>不会快照</strong>。</p>
<p><em>如果是受托管的状态，就需要考虑快照，涉及更多的细节</em></p>
<p>比如数据自身的<em>序列化/反序列化</em>、状态的存储方式、快照的形式等等，这些通过 CheckpointStreamFactory 提供的基础服务流是办不到的，Flink 提供了另外一层抽象来解决这个问题：</p>
<p>那就是：StateBackend</p>
<h3 id="StateBackend"><a href="#StateBackend" class="headerlink" title="StateBackend"></a>StateBackend</h3><p>一个 StateBackend 定义了流程序状态的存储和快照的形式，不同于 RawState ，被 StateBackend 管理的状态会定期进行分布式快照，典型的基础实现[仅仅存储的方式]有：</p>
<ul>
<li>MemoryStateBackend：将工作状态保存在 TaskManager 内存中，并快照到 JobManager 的内存</li>
<li>FsStateBackend：将工作状态保存在 TaskManager 内存中，并快照到文件系统[HDFS/Ceph/S3/GCS]</li>
<li>RocksDBStateBackend：将工作状态保存在 RocksDB 中，并快照到文件系统[类似于 FsStateBackend]</li>
</ul>
<p>具体的继承关系如下：<br><img src="state-backend-extend.png" alt="state-backend-extend.png"></p>
<p>从上图可以了解到，StateBackend 提供了工厂方法来创建 keyed-/operator state backend，keyed-/operator state backend 定义了工作状态的存储、快照的具体策略，并且走的是另外一套接口和继承：<code>KeyedStateBackend</code>。这个接口与 StateBackend 没有任何关联，它是真正用户可以接触到的 backend，用户可以通过 KeyedStateBackend 接口拿到不同类型的 <code>State</code>，来读写状态数据。</p>
<p>StateBackend 也提供了 CheckpointStreamFactory 工厂方法，通过 CheckpointStreamFactory 可以拿到 CheckpointStateOutputStream 作为 keyed-/operator state backend 的基础写出流。不同类型的基础写出流通过不同的 StateBackend 的实现拿到。</p>
<h3 id="KeyedStateBackend"><a href="#KeyedStateBackend" class="headerlink" title="KeyedStateBackend"></a>KeyedStateBackend</h3><p>上面提到的 KeyedStateBackend 已经是 statebackend 最高层的封装，也是用户可以直接接触的 backend，KeyedStateBackend 的继承关系如下：<br><img src="keyed-state-backend-extend.png" alt="keyed-state-backend-extend.png"></p>
<p>可以看到，KeyedStateBackend 暴露了一整套获取各种类型 <code>State</code> 的接口，用户通过不同的 <code>State</code> 进行状态的读写，而无需再关心状态本身的 partition、快照的逻辑。partition、快照的逻辑封装在不同的 KeyedStateBackend 实现中。<code>State</code> 这一层的实现我们先放一放，这里先介绍 KeyedStateBackend 的实现。</p>
<p><em>这里介绍 KeyedStateBackend 的实现</em></p>
<h4 id="RocksDBKeyedStateBackend"><a href="#RocksDBKeyedStateBackend" class="headerlink" title="RocksDBKeyedStateBackend"></a>RocksDBKeyedStateBackend</h4><p>RocksDBKeyedStateBackend 将 state 数据存储进 RocksDB 并且在做 checkpoint 的时候将 state 写到 CheckpointStreamFactory 提供的输出流中。</p>
<h5 id="KeyGroup-Prefix"><a href="#KeyGroup-Prefix" class="headerlink" title="KeyGroup Prefix"></a>KeyGroup Prefix</h5><p><em>后面讲 State Partiton 的时候会介绍 KeyGroup 的概念，建议先跳到后面了解下</em><br>与 HeapKeyedStateBackend 不同的是，RocksDBKeyedStateBackend 底层存储 key 是 keyGroupID + 逻辑key + namespace 的组合，这种存储简化了 keyGroup 的获取逻辑，但是稍微浪费了存储。HeapKeyedStateBackend 内存中只存储逻辑 key，快照时 keyGroup id 只会写一次。<br>keyGroupPrefixByteCount 存在的意义：它指定了 KeyGroup 的 ID 占用的字节数。<br>下面这段代码的目的是将 key-group 的 id 取出：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">keyGroup</span><span class="params">()</span> </span>&#123;</div><div class="line">  <span class="keyword">int</span> result = <span class="number">0</span>;</div><div class="line">  <span class="comment">//big endian decode</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; keyGroupPrefixByteCount; ++i) &#123;</div><div class="line">    result &lt;&lt;= <span class="number">8</span>;</div><div class="line">    result |= (currentSubIterator.currentKey[i] &amp; <span class="number">0xFF</span>);</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> result;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h5 id="增量快照"><a href="#增量快照" class="headerlink" title="增量快照"></a>增量快照</h5><p>RocksDBStateBackend 增量快照实际的快照数据包含两部分：1. meta 数据、2. state 数据。</p>
<p>meta 数据直接通过 CheckpointStreamFactory 写出到快照目录。</p>
<p>state 数据的写入，首先会对 RocksDB 自身做一次 snapshot，后再将 checkpoint 写出到 CheckpointStateOutputStream，将 RocksDB <em>快照</em> 写出到 CheckpointStateOutputStream 的实现细节：</p>
<hr>
<ol>
<li>在 RocksDB 的 base 目录下新建一个 chk-checkpointId 的 RocksDB <em>快照</em> 目录，并将 <em>快照</em> 保存下来【实际为创建 hard link】，这部分的 <em>快照</em> 包含两种文件， sst 文件 + misc 文件，接下来就是将这部分文件写出到 CheckpointStateOutputStream</li>
<li>拿到上次 checkpoint 的 sst 文件列表【实际为 stateHandleID -&gt; StreamStateHandle 映射关系】</li>
<li>遍历 1 中 <em>快照</em> 的文件列表：如果是 sst 文件，会根据 2 中的映射关系查询上次是否已做过快照，如果有，跳过快照，只将文件名【和快照句柄占位符 PlaceholderStreamStateHandle】保存下来，否则会将 sst 文件写出到 CheckpointStateOutputStream 并保存 文件名 -&gt; 快照句柄 映射关系；如果是 misc 文件，直接将文件写出到 CheckpointStateOutputStream，并保存 文件名 -&gt; 快照句柄 映射关系</li>
</ol>
<hr>
<p><em>ps: 以上每个文件都要读取内容再写出到一个单独的输出文件【占位符除外】</em></p>
<h5 id="全量快照"><a href="#全量快照" class="headerlink" title="全量快照"></a>全量快照</h5><p>RocksDBStateBackend 全量快照的整体思路是通过 CheckpointStreamFactory 拿到 CheckpointStateOutputStream 并将 RocksDB 自身的 snapshot 全量写出，主要分为以下 5 个步骤：</p>
<hr>
<ol>
<li>拿到 RocksDB 自身的 snapshot 对象</li>
<li>通过 CheckpointStreamFactory 拿到 CheckpointStateOutputStream 作为快照写出流</li>
<li>分别将快照的 meta 信息和数据写到 2 的输出流中</li>
<li>拿到 2 输出流的句柄，获得状态 offset，将 k-v 数据读取到 RocksDB 中，这里要注意的是快照时留下的 meta 起始标志位【标志一个新的 state 起始或者一个 keyGroup 结束】，快照恢复时需要复原</li>
<li>将 RocksDB 的快照对象及一些辅助资源释放</li>
</ol>
<hr>
<p><em>ps: 以上只写一个输出文件</em></p>
<p><em>这里描述 3 的实现细节</em><br>meta 信息通过辅助类 KeyedBackendSerializationProxy 来写入，依次写入：version 号、boolean/是否压缩、keySerializer/keySerializer-config、state 快照数量、states 快照 meta 信息列表，meta 信息列表通过辅助类 KeyedBackendStateMetaInfoSnapshotReaderWriters【主要封装了state key 和 namespace 的序列化逻辑】 写出。</p>
<p>kv 状态数据通过 RocksDBMergeIterator 遍历迭代，对于每个 key group 依次写 state-id、k-v pair、<em>END_OF_KEY_GROUP_MARK</em>、<strong>循环</strong>，如下图所示：<br><img src="rocksdb-full-spt-data-write-format.png" alt="rocksdb-full-spt-data-write-format.png"><br><em>meta 数据：state-id 和 END_OF_KEY_GROUP_MARK 的前一个 key 会设置标志位 FIRST_BIT_IN_BYTE_MASK</em></p>
<h5 id="增量快照恢复"><a href="#增量快照恢复" class="headerlink" title="增量快照恢复"></a>增量快照恢复</h5><p><strong>TODO: 句柄从哪传来的？快照策略部分重点讲解</strong><br>RocksDBStateBackend 增量快照的恢复主要依赖于增量快照拿到的快照句柄 IncrementalKeyedStateHandle，具体实现如下：</p>
<hr>
<ol>
<li>创建恢复的目录：RocksDB-basedir/uuid 和恢复 db</li>
<li>从 IncrementalKeyedStateHandle 拿到 <em>state 快照</em> 的句柄并从文件中将 sst + misc 文件的内容读取【按 buffer 读取】并复制到 1 中的目录下</li>
<li>从 IncrementalKeyedStateHandle 拿到 <em>meta 快照</em> 的句柄并读取 meta 信息【主要是 state 名字也就是 column 名称信息】</li>
<li>将恢复目录下的数据录入到 RocksDB 中，这里包含两种情况：多个恢复句柄或 key-group 不完全的状态数据【<strong>场景：算子并发度缩小</strong>】：由于 RocksDB 中的 key 为 keyGroupID + 逻辑key + namespace 的组合【介绍 State 的时候会细说】，这种场景下需要对 keyGroup 的合法性进行校验，只保留 keyGroup 合法的 val，因此这里的做法是首先 seek 到起始 keyGroup 偏移位置，再逐条校验并录入；只有一个恢复句柄或 key-group 完全的状态数据：直接在 baseInstance 下创建恢复目录的文件 hard link 即可</li>
<li>将恢复目录删掉</li>
</ol>
<hr>
<h5 id="全量快照恢复"><a href="#全量快照恢复" class="headerlink" title="全量快照恢复"></a>全量快照恢复</h5><p>RocksDBStateBackend 全量快照的恢复简单直接，只需要将快照的写入规则读取执行一遍即可，具体实现如下：</p>
<hr>
<ol>
<li>创建 RocksDB 实例</li>
<li>遍历传入的 KeyedStateHandle 句柄，读取输入流</li>
<li>在 2 的每次迭代使用工具类 KeyedBackendSerializationProxy 恢复 meta 信息【column 信息】</li>
<li>在 2 的每次迭代从 KeyedStateHandle 中拿到 key-group -&gt; offset 映射，并将数据逐一读取进 RocksDB 实例</li>
<li>将恢复目录删掉</li>
</ol>
<hr>
<h5 id="迭代器封装"><a href="#迭代器封装" class="headerlink" title="迭代器封装"></a>迭代器封装</h5><p>RocksDB 原生的迭代器 RocksIterator 只能指定 column，将 value 以 byte[] 形式读取，而全量快照中对于 多 key-group、多 state 的迭代遍历，有以下两种需求：</p>
<ul>
<li>key 的解序列化</li>
<li>多 key-group/state 同时迭代<br>key 的解序列化通过封装 RocksIteratorWrapper 来解决；多个 state 同时迭代的需求通过 RocksDBMergeIterator 来完成。</li>
</ul>
<p>RocksDBMergeIterator 对多 k-v stats 的迭代顺序：先按照 KeyGroup ID 排序，在按照 state-id 排序。这里按照这样的模式来排序，做快照恢复的时候在按照相同的顺序恢复。</p>
<h4 id="HeapKeyedStateBackend"><a href="#HeapKeyedStateBackend" class="headerlink" title="HeapKeyedStateBackend"></a>HeapKeyedStateBackend</h4><p>与 RocksDBKeyedStateBackend 不同的是，HeapKeyedStateBackend 将 state 存储于内存之中。HeapKeyedStateBackend 通过 StateTale 抽象来表示一个 state 存储。</p>
<h5 id="StateTable"><a href="#StateTable" class="headerlink" title="StateTable"></a>StateTable</h5><p>由于是存储于内存之中，HeapKeyedStateBackend 将一个 state 看做一张表格，也相当于 RocksDBKeyedStateBackend 中的一个 column，StateTable 封装了快照的 meta 信息，比如 state名称、key/namespace serializer 等。</p>
<p>StateTable 的实现有两种：<code>NestedMapsStateTable</code> 和 <code>CopyOnWriteStateTable</code>, 前者支持高效率的 State 存取但是不支持异步快照。后者牺牲了部分存储空间但是支持异步快照。</p>
<p><em>HeapKeyedStateBackend 快照的部分这里就不介绍了，读者请自行研究</em></p>
<h3 id="State"><a href="#State" class="headerlink" title="State"></a>State</h3><p>除了前面介绍的 KeyedStateCheckpointOutputStream/OperatorStateCheckpointOutputStream，各种类型的 <code>State</code> 是 KeyedStateBackend 通过接口暴露给用户的与底层 KeyedStateBackend/StateBackend 交互的最外层抽象，用户无需再关系底层的状态存储规则、具体的物理实现、快照的方式、容错、state partiton 等通用的策略。</p>
<p>先来看继承关系：<br><img src="state-extend.png" alt="state-extend.png"></p>
<p>可以看到 InternalKVState 是其中最为核心的接口，但是这里继承关系稍微有些复杂和混乱，Flink 为什么演化出 State/InternalKVState 两套接口呢？原因是 InternalKVState 提供了只对 Flink 引擎暴露的接口比如 namespace set/get、val get、namespace merging，这些接口并不稳定，Flink 引擎希望对上层应用屏蔽，而通过 State 接口暴露出来的方法普通用户是可以使用的。</p>
<p>用户拿到 InternalKVState 的具体实现 HeapXXXState/RocksDBXXXState 后，便可以对背后的存储进行状态读写了，这里不再详细介绍。</p>
<h3 id="StateDescriptor"><a href="#StateDescriptor" class="headerlink" title="StateDescriptor"></a>StateDescriptor</h3><p><code>State</code> 既然是暴露给用户的，那么就需要有一些属性需要指定：state 名称、val serializer、state type info。这些属性被封装在 StateDescriptor 抽象中。用户通过 AbstractKeyedStateBackend 的接口 <code>getXXXState()</code> 将 StateDescriptor 传给底层 State 实现。</p>
<p>StateDescriptor 的继承关系如下：<br><img src="state-descriptor-extend.png" alt="state-descriptor-extend.png"></p>
<h3 id="State-partition-动态伸缩"><a href="#State-partition-动态伸缩" class="headerlink" title="State partition/动态伸缩"></a>State partition/动态伸缩</h3><p>在介绍 KeyedStateBackend 的实现之后，我们再来介绍 Flink 是如何让快照支持算子动态扩并发的，这样更有利于对 KeyedStateBackend 实现部分的理解。</p>
<p>由于 KeyedStateBackend 是 task/op 级别的实例，每个 task/op 会拿着自己的 KeyedStateBackend 去做快照，我们都知道，每个 task/op 都会在运行时会处理部分的 key element【对于 KeyedStream 来说】，相应的 task-op 写出的快照状态也是只包含对应 key element 的部分，当 task-op 对应的 Operator 扩大/缩小 并发之后，之前快照的状态 key 就没有办法与当前的算子再一一对应了，这样就需要一些思路一定程度上解决这个问题。</p>
<p>其实无法一一对应并不是问题的关键，只要当前 key 分发到的 task/op 从相应的包含当前 key 的快照中恢复过来，继续计算的状态就是正确的。对于算子来说，并发的改变无外乎两种情况：</p>
<hr>
<ol>
<li>task/op 增加并发：这种场景，原先一个 task: op-task-1 处理的部分 key 可能扩充到多个 task: op-task-2 op-task-3 去处理，相应的，op-task-1 分出去的 key 的处理算子 op-task-2、op-task-3 做恢复的时候就需要 op-task-1 原先的快照</li>
<li>task/op 减少并发：这种场景，原先多个 task op-task-1 op-task-2 处理的部分 key 可能交给一个算子 op-task-3 来处理，这时候，op-task-3 的状态恢复就需要 op-task-1 和 op-task-2 的快照</li>
</ol>
<hr>
<p>想要 key 状态存储对算子的并发透明，就不能依赖于算子自身的并发来进行状态存储，而必须是一个常量值，Flink 选取了算子的 max parallelism 将 key 的存储划分到多个 bucket，一个 key bucket 也被叫做 KeyGroup ，每层 op 处理的所有 element 都会按照对应的 key 分到相应的 KeyGroup 下，op 对应的每个并发 task 都会分到一组 KeyGroup，也就是 KeyGroupRange，KeyGroupRange 中的 KeyGroup id 连续，这层 op 所有 task 的 KeyGroupRange 连起来的 KeyGroup 数量就是该 op 的 max parallelism。</p>
<p>快照恢复时，按照 task/op 当前的 KeyGroupRange 选取快照的状态句柄【上次快照的状态 KeyGroupRange 和当前 KeyGroupRange 有交集的部分】进行状态恢复；在处理 element 时，按照 element 所属 KeyGroup 分到对应的 task/op，这样两边向恒定值 KeyGroup 靠拢，便实现了状态恢复对算子并发的透明化。</p>
<p>这里梳理一些策略【在 AbstractKeyedStateBackend 中】：</p>
<hr>
<ol>
<li><p>Element 被分到哪个 KeyGroup 的策略：先对 element key 取 hashcode 再取 murmurhash 再对算子的 max parallelism 做除余 hash。所以 element 被分到哪个 KeyGroup 之和 element 的 key 有关。</p>
</li>
<li><p>task/op 被分配的 KeyGroupRange 范围策略：将 op 的 max parallelism 按照 op 当前的并发 cur parallelism 划分成连续的 id range，task/op 按照自己所属的并发 task 索引 id 被分配对应的 id range。</p>
</li>
<li><p>Element 被分给 task/op 处理的路由策略：先选出对应的 KeyGroup，再按照 2 的策略找到对应的 task/op</p>
</li>
</ol>
<hr>
<p>附一张图解释这个策略：<br><img src="key-group.png" alt="key-group.png"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;分布式快照是整个 Flink 计算框架中非常核心的模块，Flink 的 checkpoint、状态存储都依赖于其分布式快照；不仅框架自身借助于 chandy-lamda 算法，实现了算子状态的快照和恢复，也对用户暴露了一套简洁的状态存储的 API，用户无需关心快照自身的容错/扩展/一致性，这些 Flink 都已对用户透明；由于分布式快照这块的设计比较复杂，因此将拆成两篇文章来介绍，本篇文章主要介绍分布式快照存储部分的设计，下一篇会介绍快照的流程和细节。&lt;/p&gt;
&lt;p&gt;代码参考版本: 1.5-SNAPSHOT&lt;/p&gt;
&lt;h2 id=&quot;核心模块&quot;&gt;&lt;a href=&quot;#核心模块&quot; class=&quot;headerlink&quot; title=&quot;核心模块&quot;&gt;&lt;/a&gt;核心模块&lt;/h2&gt;&lt;p&gt;Flink 的分布式快照存储部分设计抽象出了大致 5 个层次：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最底层是快照的物理存储，包括内存和文件系统两种形式&lt;/li&gt;
&lt;li&gt;再上层是 CheckpointStreamFactory：封装了具体的存储交互，也就是内存/文件系统读写&lt;/li&gt;
&lt;li&gt;再上层是 StateBackend：封装了工作状态的存储逻辑，包括内存和 RocksDB 两种形式&lt;/li&gt;
&lt;li&gt;再上层是 KeyedStateBackend：封装了快照的读写细节，快照分区策略等&lt;/li&gt;
&lt;li&gt;再上层是 State：封装了与 KeyedStateBackend 交互时状态的 val 序列化/反序列化 等逻辑&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;还有一个 StateContext 比较特殊，它不提供快照功能，只提供临时的状态读写，下面会讲到&lt;/em&gt;&lt;br&gt;核心模块的总体交互图:&lt;br&gt;&lt;img src=&quot;checkpoint-sys-interact.png&quot; alt=&quot;checkpoint-sys-interact.png&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Flink源码" scheme="https://danny0405.github.io/categories/Flink%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="Flink内部实现" scheme="https://danny0405.github.io/tags/Flink%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>Flink WindowOperator 的设计</title>
    <link href="https://danny0405.github.io/2017/10/18/Flink-WIndowOperator-%E7%9A%84%E8%AE%BE%E8%AE%A1/"/>
    <id>https://danny0405.github.io/2017/10/18/Flink-WIndowOperator-的设计/</id>
    <published>2017-10-18T14:02:24.000Z</published>
    <updated>2018-01-23T06:11:39.111Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>窗口计算是流式计算里比较常见的场景，不同的计算框架在解决窗口计算方面提出了自己的思路，storm 目前还停留在内存计算的基础上，并且尚不支持事件时间滑动；spark-streaming 通过 struct-streaming 实现了事件/处理 时间的精确窗口计算，借助 rdd 的特性很好的保证了窗口计算的精确性；flink 在 data-flow 模型的设计前提下，友好的支持了事件/处理时间的窗口触发，同时借助分布式的状态存储，为窗口中间结果保存提供了较好的扩展性支持</p>
<h2 id="核心抽象"><a href="#核心抽象" class="headerlink" title="核心抽象"></a>核心抽象</h2><p>这部分会介绍围绕 WindowOperator 的核心抽象，主要涉及窗口计算中的一些核心角色</p>
<h3 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h3><p>Window 代表一个有限的元素集合，并且由多个 窗格/pane 组成，一个窗口有一个 maximux timestamp，代表某个时刻所有的窗口元素都已到达。窗口的实现有两个：</p>
<ul>
<li>TimeWindow: 具有时间窗口属性的窗口，会维护一个起始和终止时间</li>
<li>GlobalWindow: 没有起始/终止时间的限制，所有的元素都会进一个窗口</li>
</ul>
<h3 id="WindowFunction"><a href="#WindowFunction" class="headerlink" title="WindowFunction"></a>WindowFunction</h3><p>在 Trigger 触发的时候，调用的函数，WindowFunction 接口有抽象实现 <code>RichWindowFunction</code>，用来提供对运行时环境 RuntimeContext 的访问及 <code>open()</code> <code>close()</code>  这样的生命周期管理方法</p>
<p>按照是否按照grouping key 来处理窗口元素，WindowFunction 可以被分为两类：</p>
<ul>
<li>ProcessAllWindowFunction/AllWindowFunction: 每次处理 window 内所有元素</li>
<li>ProcessWindowFunction/WindowFunction: 每次处理一个窗格/pane</li>
</ul>
<p>继承关系如下：<br><img src="windowfunc-extend.png" alt="windowfunc-extend.png"><br>flink 内部在计算的时候使用的是 InternalWindowFunction，在基础 function 的基础上封装了一些运行时环境和配置。</p>
<h3 id="WindowAssigner"><a href="#WindowAssigner" class="headerlink" title="WindowAssigner"></a>WindowAssigner</h3><p>一个 WindowAssigner 主要将一个或多个 window 抽象派发给一个元素/消息，在一次 window 的操作中，窗口内的元素会被按照它对应的 key 以及分配的 window 做 group by，一个 key + window 对应的元素集被称作一个窗格/pane。当一个触发器/Trigger 决策出某个 pane 应该触发对应的窗口函数/WindowFunction 时，窗口函数会被应用到 pane 的元素集上输出结果元素</p>
<h4 id="WindowAssigner-的核心接口"><a href="#WindowAssigner-的核心接口" class="headerlink" title="WindowAssigner 的核心接口"></a>WindowAssigner 的核心接口</h4><ul>
<li>assignWindows: 将对应的 window/windows 分配给指定的元素</li>
<li>getDefaultTrigger: 获取指定的 Trigger</li>
<li>isEventTime: 是否基于 event time 将 window/windows 分配给元素</li>
</ul>
<h4 id="WindowAssigner-的实现"><a href="#WindowAssigner-的实现" class="headerlink" title="WindowAssigner 的实现"></a>WindowAssigner 的实现</h4><p>WindowAssigner 的实现可以按照几个维度来划分，具体继承关系如下图：<br><img src="window-assigner-extend.png" alt="window-assigner-extend.png"><br>可以看到一些特性：</p>
<ul>
<li>session 类型的窗口对应的 WindowAssigner 是可以被 merge 的，其它不可以</li>
<li>sliding window 对应的 assigner 分配的 windows 间是可以覆盖的</li>
</ul>
<h3 id="Trigger"><a href="#Trigger" class="headerlink" title="Trigger"></a>Trigger</h3><p>Trigger 决定了一个窗格 pane 中的元素触发计算窗口函数的时机，按照上面对 WindowAssigner 的描述，一个元素可以被分到多个 window，按照 key-window 做 grouping 的元素集组成一个 pane，每个 pane 都有自己的 Trigger 实例。Trigger 管理了 event-timer/processing-timer 的注册；同时在处理元素以及对应的 timer event 在 WindowOperator 上触发时<em>决策</em>是否真正触发计算[ FIRE/CONTINUE ]</p>
<h4 id="Trigger-的核心接口"><a href="#Trigger-的核心接口" class="headerlink" title="Trigger 的核心接口"></a>Trigger 的核心接口</h4><ul>
<li>onElement: 处理每个添加到 pane 中的元素</li>
<li>onEventTime: TriggerContext 注册的 event-time timer 被触发时调用</li>
<li>onProcessingTime: TriggerContext 注册的 processing-time timer 被触发时调用</li>
<li>onMerge: window 被 merge 时触发</li>
</ul>
<p>Trigger 内部定义了 TriggerContext 抽象，用来管理 event/processing-time timer 的注册和状态存储</p>
<h4 id="Trigger-的实现"><a href="#Trigger-的实现" class="headerlink" title="Trigger 的实现"></a>Trigger 的实现</h4><p>继承关系如下图:<br><img src="trigger-extend.png" alt="trigger-extend.png"><br>典型的 trigger 实现用途：</p>
<ul>
<li>ContinuousEventTimeTrigger：指定的 interval 以及 event window 的边界小于当前 watermark 时触发 onEventTime 计算</li>
<li>ContinuousProessingTimeTrigger：指定的 interval 触发 onProcessingTime 计算</li>
<li>CountTrigger: 指定的 maxCount 触发，出发后 maxCount 清零重新累加</li>
<li>DeltaTrigger: 提供 delta 函数和历史 datapoint 存储，每个元素消费时触发 delta 函数计算</li>
<li>EventTimeTrigger：event window 的边界小于当前 watermark 时触发 onEventTime 计算</li>
<li>ProessingTimeTrigger：event window 的边界小于当前 watermark 时触发 onProcessingTime 计算</li>
</ul>
<h3 id="WindowOperator"><a href="#WindowOperator" class="headerlink" title="WindowOperator"></a>WindowOperator</h3><p>WindowOperator 是窗口计算算子的核心抽象，使用上面描述的 WindowAssigner 和 Trigger 来完成窗口计算。当一个元素到达时，会通过 KeySelector 分配一个 key，并且通过 WindowAssigner 将该元素分配到多个窗口，也就是多个 pane，后 Trigger 会决策 pane 中的元素被 fire 窗口计算的时机，这里调用的函数为 InternalWindowFunction。我们用一张图来描述这种组成关系：<br><img src="window-op-comp.png" alt="window-op-comp.png"></p>
<h4 id="WindowOperator-时间事件触发管理"><a href="#WindowOperator-时间事件触发管理" class="headerlink" title="WindowOperator 时间事件触发管理"></a>WindowOperator 时间事件触发管理</h4><p>我们都知道流式计算中对于计算时间主要分为两类：event-time、processing-time，对不同的时间类型，在引擎的内部需要一套管理时间事件触发的机制，典型的 processing-time 按照 interval 定时触发[ 或及时触发单不一定精准 ]，event-time 按照特定的事件触发。<br>在 flink 内部，由以下抽象来管理时间事件的触发：</p>
<ul>
<li>InternalTimer: 内部代表一个 timer，封装了对应的 key 、namespace[ 关于 key, namespace 的概念会在分布式快照/StateBackend 设计的部分介绍，这里只需了解，key 代表 timer 触发时的活跃元素 key，namespace 代表特定的窗口，通过 key-namespace 的组合可以唯一确定一个 pane/窗格，触发计算 ] 以及 timestamp，</li>
<li>InternalTimeService: 用来管理 time 以及相应的 timer，会提供注册/销毁 event-timer/ processing-timer 的接口以及获取当前 processing-time、watermark 的接口，同时会通过回调接口触发相应的 event-time/processing-time 回调。目前的唯一实现是 HeapInternalTimerService，其在堆中维护了两个队列分别存储已注册的 event-timer 和 peocessing-timer</li>
<li>InternalTimeServiceManager: 用来管理 TaskManager 中所有一个算子的 InternalTimeService，算子可以使用 name 属性申请不同的 InternalTimeService</li>
<li>ProcessingTimeService：从 StreamTask 中传递过来的管理 processing-timer 注册和回调的服务</li>
</ul>
<hr>
<p>它们之间的关系可以用下图来解释：<br> <img src="timer-event-relation.png" alt="timer-event-relation.png"><br><strong>事件注册/触发流程</strong></p>
<ul>
<li><strong>event-time event</strong>: 每个窗格pane[或者 key-window pair] 的 event timer 通过<br>-&gt; <code>WindowOperator.processElement()</code><br>-&gt; <code>Trigger.onElement()</code><br>-&gt; <code>InternalTimeService.registerProcessingTimeTimer()</code> 来注册；<br>通过<br>-&gt; <code>WindowOperator.processElement()</code><br>-&gt; <code>Trigger.onElement()</code><br>或者<br>-&gt; <code>InternalProcessingTimeService.TriggerTask.run()</code><br>-&gt; <code>InternalTimeService.onProcessingTime()</code><br>-&gt; <code>WindowOperator.onProcessingTime()</code><br>-&gt; <code>Trigger.onProcessingTime()</code> 来触发</li>
<li><strong>processing-time event</strong>: 每个窗格pane[或者 key-window pair] 的 processing-time timer 通过<br>-&gt; <code>WindowOperator.processElement()</code><br>-&gt; <code>Trigger.onElement()</code><br>-&gt; <code>InternalTimeService.registerEventTimeTimer()</code> 来注册；<br>通过<br>-&gt; <code>WindowOperator.processElement()</code><br>-&gt; <code>Trigger.onElement()</code><br>或者<br>-&gt; <code>AbstractStreamOperator.processWatermark()</code><br>-&gt; <code>InternalTimeServiceManager.advanceWatermark()</code><br>-&gt; <code>InternalTimeService.advanceWatermark()</code><br>-&gt; <code>WindowOperator.onEventTime()</code> 来触发</li>
</ul>
<p><em>ps: 这里的 InternalTimeServiceManager 在运行时是 operator 粒度，也就是 task 粒度的实例</em></p>
<h4 id="WindowOperator-窗口状态的更新和计算触发"><a href="#WindowOperator-窗口状态的更新和计算触发" class="headerlink" title="WindowOperator 窗口状态的更新和计算触发"></a>WindowOperator 窗口状态的更新和计算触发</h4><p>上面已经梳理了窗口事件触发的时机，进一步，触发了窗口事件后，会开始窗口结果查询并传递给 InternalWindowFunction，来触发一次窗口的函数计算，并最终将结果输出到下游算子，窗口的内容在 <code>WindowOperator.processElement()</code> 中更新，在 <code>WindowOperator.onEventTime()</code> 和 <code>WindowOperator.onProcessingTime()</code> 中触发窗口计算<br><strong>窗口值更新过程</strong></p>
<ul>
<li>在处理每个消息元素时，通过 WindowAssigner 计算元素需要分到什么样时间跨度的 window 里，对于每个分到的 window，会将本次的消息元素追加到 window 对应的 value 结果中，并会调用 <code>Trigger.onElement()</code> 检测是否触发了窗口事件，如果触发，取出对应的 value 结果，触发窗口计算</li>
<li>如果 WindowAssigner 是具有合并窗口的属性[ session窗口 ]，会将划分出的窗口逐一添加到一个名为 MergingWindowSet 的工具类[ 内部会触发 namespace/window 对应的 value 合并 以及 trigger 触发时间属性的合并，下面会专门讲解 ]</li>
</ul>
<p><strong>窗口计算触发</strong><br><em>这里只介绍 event-time 事件触发，processing-time 事件类似</em></p>
<ul>
<li>取出 event-timer 对应的 window 的 value 结果[ 一个窗格的累加值 ]，调用 <code>Trigger.onEventTime()</code> 检测是否触发了窗口事件，如果触发，取出对应的 value 结果，触发窗口计算，值得注意的是: InternalTimeService 在将 event-timer 事件发送给 WindowOperator 的时候会同时设置 event-timer 窗口的活跃 key[ 前面提到过一个 key 和 window 可以唯一确定一个 pane ]</li>
<li>如果 window 已经过期，将对应的 value 结果清理掉</li>
</ul>
<h4 id="WindowOperator-可合并窗口的更新和计算"><a href="#WindowOperator-可合并窗口的更新和计算" class="headerlink" title="WindowOperator 可合并窗口的更新和计算"></a>WindowOperator 可合并窗口的更新和计算</h4><p><em>可合并窗口的管理会稍微复杂一些，这里单独介绍</em><br>窗口的合并由一个辅助工具类 MergingWindowSet 来完成，它维护了一个 window -&gt; state-window 的映射关系，什么叫 state-window 呢？它其实是可合并的窗口的唯一的 namespace，用来作为存储合并窗口 value 结果的依据，前面提到 key-window 会作为一个 pane value 结果存储结果的唯一标识，state-window + 触发的活跃 key 可以唯一确定一个合并后窗口里的计算窗格<br>除了合并窗口的时间边界，合并窗口的关键在于状态的合并，这里有两个角色维护了需要合并的状态：</p>
<ul>
<li>Trigger: 部分 trigger 自己维护了一些状态，比如下次触发时间等</li>
<li>WindowOperator: 窗口算子的 value 结果</li>
</ul>
<p>Flink 使用 State/状态存储 来保存状态，在窗口合并时，只需将多个窗口的存储状态合并即可，对应的是 InternalMergingState 的 mergeNamesspaces 操作[ Flink 使用 InternalXXXState 来管理算子内部的 active key，活跃 key 的变动对用户透明 ]</p>
<p>MergingWindowSet 内部将 WindowAssigner 分配的窗口合并后，通过回调函数 MergeFunction 来触发状态的合并</p>
<p>附一张图解释这个过程：<br><img src="merging-window-set.png" alt="merging-window-set.png"></p>
<p>在 MergingWindowSet 中维护了每个 window 和对应的 state-window 的对应关系，当触发 window 的窗口计算时，先找到对应的 state-window，取出对应的状态作为该窗口对应的状态结果</p>
<h3 id="WindowStream"><a href="#WindowStream" class="headerlink" title="WindowStream"></a>WindowStream</h3><p>WindowStream 暴露了一些常用的流处理接口，生成对应的 WindowOperator，这里就不赘述了[ 之前的博客 flink 逻辑计划生成部分详细讲解了 DataStream 和 Operator 之间的关系 ]</p>
<h2 id="WindowOperator-的快照和恢复"><a href="#WindowOperator-的快照和恢复" class="headerlink" title="WindowOperator 的快照和恢复"></a>WindowOperator 的快照和恢复</h2><p>WindowOperator 的状态恢复包括三部分：用户函数 UDF 的计算状态、窗口计算结果/window-state 以及触发 timers</p>
<p>udf 的快照和恢复封装在 WindowOperator 的父类 AbstractUdfStreamOperator 中，</p>
<ul>
<li>udf快照：flink 将 snapshotState 接口暴露给用户，用户可自行时间快照逻辑，flink 拿到快照后将其序列化存储到状态存储</li>
<li>udf恢复：直接反序列化状态存储并调用 udf 的 restoreState 接口将状态恢复</li>
</ul>
<p>window-state 以及 trigger 的状态快照/恢复由 AbstractStreamOperator 来管理[ window-state 的状态又包括 keyed/非 keyed/opsrator-state 三部分，之后的博客会专门讲分布式快照设计 ]</p>
<p>timers 的恢复在 AbstractStreamOperator 做 snapshot 的时候一起完成，底层是通过 InternalTimeServiceManager 来实现</p>
<p>这一块儿的设计有一个小的 trick，在 AbstractStreamOperator 内部，统一的快照走<br><code>snapshotState(long checkpointId, long timestamp, CheckpointOptions checkpointOptions)</code> 接口，其内部会调用另一个接口：<code>snapshotState(StateSnapshotContext context)</code>，<br>这个接口默认实现了 InternalTimeServiceManager 的 timers 的快照，继承了 AbstractStreamOperator 的子类可以继承这个接口完成额外的状态快照，丰富自己的功能，比如 AbstractUdfStreamOperator 就在默认 timers 快照的基础上，又快照了 udf 的状态<br>恢复的的策略类似，子类可以重载 <code>initializeState(StateInitializationContext context)</code> 来实现自己的恢复策略</p>
]]></content>
    
    <summary type="html">
    
      本文将介绍 flink 1.3.1 版本 WindowOperator 的设计，主要涉及核心的抽象，角色功能以及之间的交互，因为 WindowOperator 的状态存储用到了 flink 的状态存储，所以这里会略带涉及状态存储的知识，具体细节会单独开一篇文章介绍。
    
    </summary>
    
      <category term="Flink源码" scheme="https://danny0405.github.io/categories/Flink%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="Flink内部实现" scheme="https://danny0405.github.io/tags/Flink%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>flink kafka connector 设计与实现</title>
    <link href="https://danny0405.github.io/2017/03/29/flink-kafka-connector/"/>
    <id>https://danny0405.github.io/2017/03/29/flink-kafka-connector/</id>
    <published>2017-03-29T10:40:48.000Z</published>
    <updated>2018-01-23T06:11:21.379Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Kafka 作为成熟的消息队列目前在大数据场景中应用的越来越广泛，其高性能与多 producer push 多 consumer pull 的消息流非常适合流式计算场景；同时 kafka 是较好的支持回溯消费的队列之一，为各种流式计算框架设计 at-least-once 甚至 exact-once 语义的计算模型提供了可能</p>
<p>本文将介绍 flink 1.2 的 0.9.0 版本 kafka connector，主要讲解消费模型，快照策略/恢复等，尤其是为什么 flink  kafka connector 借助于 flink 内核的分布式快照算法做到了 exact-once 语义</p>
<h2 id="核心组件介绍"><a href="#核心组件介绍" class="headerlink" title="核心组件介绍"></a>核心组件介绍</h2><p>先来看下 flink-kafka-connector 都有哪些核心组件以支撑数据访问</p>
<p>先来看下整体的组件继承关系：</p>
<p><img src="component-extend.png" alt="component-extend.png"></p>
<p>从上图可以直观的看出，FlinkKafkaConsumerBase 是核心类，它继承自 SourceFunction 接口，通过运行 <code>run()</code> 方法获取数据，背后又通过维护一个 AbstractFetcher 成员获取数据，AbstractFetcher 维护了一个消费线程 KafkaConsumerThread 来不断与 Kafka 交互；同时我们观察到 AbstractFetcher 为每个消费的 kafka partition 维护了一个状态成员 KafkaTopicPartitionState，用于保存 partition 当前消费到的 offset 以及已经 commit 到 kafka 的 offset 信息</p>
<h2 id="消息及-WaterMark-分发策略"><a href="#消息及-WaterMark-分发策略" class="headerlink" title="消息及 WaterMark 分发策略"></a>消息及 WaterMark 分发策略</h2><p>上面我们说到每个 AbstractFetcher 封装了具体的获取数据的逻辑，与此同时，它也提供了三种模式来控制自己生产时间戳 timestamp 和 watermark 的模式：</p>
<ul>
<li>NO_TIMESTAMPS_WATERMARKS: fetcher 不生产 timestamp 和 watermarks</li>
<li>PERIODIC_WATERMARKS: fetcher 阶段性定时生产 watermarks</li>
<li>PUNCTUATED_WATERMARKS: fetcher 生产标记 watermark 【按照特定的消息字段值触发】</li>
</ul>
<p>而 <em>对应</em> 这三种不同的模式，AbstractFetcher 会维护三种不同的 KafkaTopicPartitionState：</p>
<ul>
<li>KafkaTopicPartitionState: 只维护 partiton 相关信息</li>
<li>KafkaTopicPartitionStateWithPeriodicWatermarks: 维护了一个 AssignerWithPeriodicWatermarks 引用</li>
<li>KafkaTopicPartitionStateWithPunctuatedWatermarks: 维护了一个 AssignerWithPunctuatedWatermarks 引用</li>
</ul>
<p>如前面的继承关系所示，KafkaTopicPartitionState 只维护基本的一些 partition 的信息，而后两者分别维护了获取 partition 的 timestamp 和 watermark 的策略，也就是：AssignerWithPeriodicWatermarks 和 AssignerWithPunctuatedWatermarks</p>
<p>AbstractFetcher 通过 <code>run()</code> 方法来获取数据，<code>run()</code> 方法又会调用其 <code>runFetchLoop()</code> 方法来不断拉取数据。Kafka09Fetcher 作为 AbstractFetcher 的实现类之一，其重载的方法 <code>runFetchLoop()</code> 会先启动消费线程 KafkaConsumerThread，后不断 while 循环从 HandOver poll 数据，【这一块的消费模型我们后面介绍】，接着将这些数据发送给下游算子，发射的逻辑封装在 AbstractFetcher 的方法 <code>emitRecord()</code>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">emitRecord</span><span class="params">(T record, KafkaTopicPartitionState&lt;KPH&gt; partitionState, <span class="keyword">long</span> offset)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">   <span class="keyword">if</span> (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) &#123;</div><div class="line">      <span class="comment">// fast path logic, in case there are no watermarks</span></div><div class="line"></div><div class="line">      <span class="comment">// emit the record, using the checkpoint lock to guarantee</span></div><div class="line">      <span class="comment">// atomicity of record emission and offset state update</span></div><div class="line">      <span class="keyword">synchronized</span> (checkpointLock) &#123;</div><div class="line">         sourceContext.collect(record);</div><div class="line">         partitionState.setOffset(offset);</div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line">   <span class="keyword">else</span> <span class="keyword">if</span> (timestampWatermarkMode == PERIODIC_WATERMARKS) &#123;</div><div class="line">      emitRecordWithTimestampAndPeriodicWatermark(record, partitionState, offset, Long.MIN_VALUE);</div><div class="line">   &#125;</div><div class="line">   <span class="keyword">else</span> &#123;</div><div class="line">      emitRecordWithTimestampAndPunctuatedWatermark(record, partitionState, offset, Long.MIN_VALUE);</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>我们可以看到，依据生产时间戳 timestamp 和 watermark 的模式的不同，会走不同的发射逻辑，我们只分析 PERIODIC_WATERMARKS 和 PUNCTUATED_WATERMARKS 两种策略</p>
<h3 id="消息分发策略"><a href="#消息分发策略" class="headerlink" title="消息分发策略"></a>消息分发策略</h3><p>不管是 PERIODIC_WATERMARKS 还是 PUNCTUATED_WATERMARKS 策略，均会做这两件事情：</p>
<ul>
<li>通过调用 <code>sourceContext.collectWithTimestamp()</code> 来收集消息和时间戳</li>
<li>将消息对应的 offset 信息保存到对应的 KafkaPartitionState【有不同实现类型】</li>
</ul>
<p>那么问题的关键就是 sourceContext，SourceContext Interface 被定义在 SourceFunction 中作为嵌套 Interface，它统一管理源算子的消息和 watermark 的发送</p>
<p>那么 SourceContext 都有哪些具体的类型呢？通过什么方式来控制？</p>
<p>在 StreamSource 的 <code>run()</code> 方法中，我们看到有这样的逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">this</span>.ctx = StreamSourceContexts.getSourceContext(</div><div class="line">   timeCharacteristic, getProcessingTimeService(), lockingObject, collector, watermarkInterval);</div></pre></td></tr></table></figure>
<p>可以看到 StreamSource 的类型主要通过 timeCharacteristic 来控制:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> &lt;OUT&gt; SourceFunction.<span class="function">SourceContext&lt;OUT&gt; <span class="title">getSourceContext</span><span class="params">(</span></span></div><div class="line">      TimeCharacteristic timeCharacteristic, ProcessingTimeService processingTimeService,</div><div class="line">      Object checkpointLock, Output&lt;StreamRecord&lt;OUT&gt;&gt; output, <span class="keyword">long</span> watermarkInterval) &#123;</div><div class="line"></div><div class="line">   <span class="keyword">final</span> SourceFunction.SourceContext&lt;OUT&gt; ctx;</div><div class="line">   <span class="keyword">switch</span> (timeCharacteristic) &#123;</div><div class="line">      <span class="keyword">case</span> EventTime:</div><div class="line">         ctx = <span class="keyword">new</span> ManualWatermarkContext&lt;&gt;(checkpointLock, output);</div><div class="line">         <span class="keyword">break</span>;</div><div class="line">      <span class="keyword">case</span> IngestionTime:</div><div class="line">         ctx = <span class="keyword">new</span> AutomaticWatermarkContext&lt;&gt;(processingTimeService, checkpointLock, output, watermarkInterval);</div><div class="line">         <span class="keyword">break</span>;</div><div class="line">      <span class="keyword">case</span> ProcessingTime:</div><div class="line">         ctx = <span class="keyword">new</span> NonTimestampContext&lt;&gt;(checkpointLock, output);</div><div class="line">         <span class="keyword">break</span>;</div><div class="line">      <span class="keyword">default</span>:</div><div class="line">         <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(String.valueOf(timeCharacteristic));</div><div class="line">   &#125;</div><div class="line">   <span class="keyword">return</span> ctx;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>我们这里列一下不同的 时间特性 下的 collect 策略：</p>
<ul>
<li>EventTime: 直接按照传入的 element 和 timestamp 封装成 StreamRecord emit 出去</li>
<li>IngestionTime: 将传进来的 timestamp 丢弃，并采用当前的系统时间赋给 StreamRecord emit 出去，同时会 check 当前的系统时间是否达到下次 watermark 的触发时间，如果达到会 emit 一个 watermark 到下游，同时 AutomaticWatermarkContext 会注册一个 timertask，按照 watermark 的间隔时间定时 check 系统时间，如果达到触发 watermark 的条件也会发射一个 watermark 到下游，设置两个触发逻辑的目的是尽量保证 watermark 在符合条件时 <em>及时</em> 触发发送到下游</li>
<li>ProcessingTime: 直接将传入的 element 封装成 StreamRecord 发射到下游，忽略传入的时间戳</li>
</ul>
<h3 id="WaterMark-策略"><a href="#WaterMark-策略" class="headerlink" title="WaterMark 策略"></a>WaterMark 策略</h3><p>其实上面再介绍 IngestionTime 的时候已经介绍了一部分的 watermark 的触发策略，毕竟 StreamSource 是控制消息和 watermark 发射的总入口，影响 watermark 发射的最终逻辑一共有两个：</p>
<ul>
<li>时间特性：IngestionTime 时间处理特性下 StreamSource 会定时触发 watermark 的发射</li>
<li>用户指定的 watermark 特性：前面介绍的三种</li>
</ul>
<p>现在我们再回到 AbstractFetcher，来看下用户指定不同的 watermark 特性对 watermark 发射的影响</p>
<h4 id="PERIODIC-WATERMARKS"><a href="#PERIODIC-WATERMARKS" class="headerlink" title="PERIODIC_WATERMARKS"></a>PERIODIC_WATERMARKS</h4><p>如果用户指定这样的 watermark 特性，AbstractFetcher 会另起一个 timertask，按照 watermark interval 去定时调用 SourceContext 的 <code>emitWatermark()</code> 接口，我们看 SourceContext 的实现可以知道，只有 EventTime 对应的 ManualWatermarkContext 才会将 watermark emit 出去，其它会自动忽略掉</p>
<p>也就是说 PERIODIC_WATERMARKS 策略搭配 EventTime 的时间处理特性才有意义</p>
<p>这里获取的 watermark 是当前所有消费 partition 的 KafkaPartitionState 获取到的 watermark 时间的最小值，是和消息本身相关联的</p>
<p>ps: 个人觉得 AutomaticWatermarkContext 的 watermark 策略应该统一移到 PERIODIC_WATERMARKS。这样模块划分更清晰，用户也更容易理解，不过 flink 这么安排也是有它的合理性：PROCESSING_TIME 时间处理特性下，用户只关心源算子当前的处理时间，和消息本身无关，所有由 AutomaticWatermarkContext 统一控制</p>
<h4 id="PUNCTUATED-WATERMARKS"><a href="#PUNCTUATED-WATERMARKS" class="headerlink" title="PUNCTUATED_WATERMARKS"></a>PUNCTUATED_WATERMARKS</h4><p>如果指定 PUNCTUATED_WATERMARKS 作为 watermark 特性，也就是用户依据消息中的触发字段来触发 watermark，这时候发射逻辑如下：</p>
<ul>
<li>用户每读取一条消息，就判断是否可以出发 watermark</li>
<li>如果可触发，获取每个 partition 的 watermark 并取时间戳最小的 watermark</li>
<li>如果这个最小时间戳大于上次发射的 watermark 的时间戳，则通过 SourceContext 发射出去</li>
</ul>
<p>由上面分析可以知道，PUNCTUATED_WATERMARKS 也是只对 EVENT_TIME 时间特性有意义</p>
<h2 id="Checkpoint-及恢复"><a href="#Checkpoint-及恢复" class="headerlink" title="Checkpoint 及恢复"></a>Checkpoint 及恢复</h2><h3 id="快照策略"><a href="#快照策略" class="headerlink" title="快照策略"></a>快照策略</h3><p>策略集中在 FlinkKafkaConsumerBase 的 <code>snapshotState()</code>  方法中，</p>
<h4 id="发起快照策略"><a href="#发起快照策略" class="headerlink" title="发起快照策略"></a>发起快照策略</h4><ul>
<li>清空之前的快照状态</li>
<li>从维护的 KafkaTopicPartitionState 获取每个 partition 当前 <em>处理到的</em> offset</li>
<li>记录下来用于后面 commit 到 kafka 用</li>
<li>将 offset 信息快照到状态存储</li>
</ul>
<h4 id="一轮快照结束后的-commit-策略"><a href="#一轮快照结束后的-commit-策略" class="headerlink" title="一轮快照结束后的 commit 策略"></a>一轮快照结束后的 commit 策略</h4><p>将之前快照的 offset 通过 AbstractFetcher commit 到 kafka</p>
<p>ps: AbstractUdfStreamOperator 管理了快照背后的逻辑 [ 存储及序列化 ]</p>
<h3 id="恢复策略"><a href="#恢复策略" class="headerlink" title="恢复策略"></a>恢复策略</h3><p>从状态存储中恢复之前快照的 offset 信息并 set 到对应的 KafkaTopicPartitionState 中</p>
<h2 id="HandOver"><a href="#HandOver" class="headerlink" title="HandOver"></a>HandOver</h2><p>前面提到消息的消费是由 fetcher 中的 HandOver 和 KafkaConsumerThread 共同完成的，有的同学可能有疑问，KafkaConsumerThread 直接消费就好，为什么要有一个 HandOver 呢？</p>
<p>HandOver 的存在有两个目的：</p>
<ul>
<li>控制消费队列的大小</li>
<li>不会阻塞 offset commit 到 kafka 的逻辑</li>
</ul>
<p>HandOver 实际是一个锁控制器，当它收到 commit offset 请求时会优先处理 commit 的逻辑，同时如果 HandOver 中有未消费的一批消息，会阻塞消费线程的消费</p>
<p>附一张图：</p>
<p><img src="handover.png" alt="handover.png"></p>
]]></content>
    
    <summary type="html">
    
      本文将介绍 flink 1.2 的 0.9.0 版本 kafka connector，主要讲解消费模型，快照策略/恢复等，尤其是为什么 flink  kafka connector 借助于 flink 内核的分布式快照算法做到了 exact-once 语义
    
    </summary>
    
      <category term="Flink源码" scheme="https://danny0405.github.io/categories/Flink%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="Flink组件" scheme="https://danny0405.github.io/tags/Flink%E7%BB%84%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>flink-AsyncIO-剖析</title>
    <link href="https://danny0405.github.io/2017/03/29/flink-AsyncIO-%E5%89%96%E6%9E%90/"/>
    <id>https://danny0405.github.io/2017/03/29/flink-AsyncIO-剖析/</id>
    <published>2017-03-29T10:29:27.000Z</published>
    <updated>2018-01-23T06:11:29.233Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>AsyncIO 是由 flink 1.2 引入的新特性，其目的是优化流处理过程中外部服务访问的耗时造成的瓶颈问题。写过流处理程序的童鞋可能都遇到过这样的场景：如果每条消息都要访问一次外部服务，并且一次 IO 时间较长【e.g 大于 20ms】；亦或者每条消息访问多个外部服务：比如 redis 过滤后写到最终输出，比如双写下游服务等。 这时候整个流处理的瓶颈通常就是外部 IO 了，因为纯 cpu 计算通常是比较快的，一般都在 ms 级【正则匹配、json 解析之类的除外】</p>
<p>AsyncIO 技术通过发送批量请求平摊网络 IO 开销的方式一定上缓解了这个问题，另外得益于 flink 框架对 watermark 和 checkpoint 的良好支持，AsyncIO 一定程度上保证了消息的顺序性及 exact-once 语义</p>
<p>既然涉及到 exact-once 语义，这里会以 Kafka Connector 为例讲解 flink 是如何保证 exact-once 语义的</p>
<h2 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h2><p>关于涉及思路，官方已经讲的很细致了，这里只是引用下：</p>
<blockquote>
<p><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65870673" target="_blank" rel="external">FLIP-12: Asynchronous I/O Design and Implementation</a></p>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/stream/asyncio.html" target="_blank" rel="external">Asynchronous I/O for External Data Access</a></p>
</blockquote>
<p>建议大家先看下这两篇文章，在看下面的源码分析就比较好理解了…</p>
<h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><h3 id="AsyncDataStream"><a href="#AsyncDataStream" class="headerlink" title="AsyncDataStream"></a>AsyncDataStream</h3><p>flink 对于 DataStream 的操作最终都会抽象为算子操作【flink 逻辑计划中有介绍】，AsyncIO 也不例外，它对应的算子为 AsyncWaitOperator【下面会介绍】</p>
<p>依据业务需求不同，下游输出包括两种模式：有序输出、无序输出</p>
<p>AsyncDataStream 是一个工具类，它提供工厂方法方便的生成 AsyncWaitOperator，依据不同的参数设置生成对应策略的 AsyncWaitOperator，主要参数如下：</p>
<ul>
<li>asyncFunc：需要用户定义的异步处理函数</li>
<li>timeout：虽然是异步访问，但是也需要 timeout 来控制访问的有效性，防止将异步队列堵死</li>
<li>bufSize：AsyncWaitOperator 内部维护的异步访问队列大小，默认是 100，也就是默认最多维护 100 个还未返回成功的请求</li>
<li>outputMode：上面说的有序和无序两种</li>
</ul>
<p><em>ps: 以上参数用户都是可以指定的</em></p>
<p><em>为什么会有 bufSize 限制，下面结合 AsyncWaitOperator 的线程模型会给出解释</em></p>
<h3 id="AsyncWaitOperator"><a href="#AsyncWaitOperator" class="headerlink" title="AsyncWaitOperator"></a>AsyncWaitOperator</h3><p>从名字就可以看出，AsyncWaitOperator 支持异步的外部 IO 访问。对于每条输出到算子的消息，flink 都会生成一个 entry: AsyncCollector，AsyncCollector 会被加入到 AsyncWaitOperator 内部队列中，并且会通过回调函数的方式传递给用户定义的 function: AsyncFunction，用户可以在 AsyncFunction 中对 AsyncCollector 触发行为，通常有两种：</p>
<ul>
<li>异步访问成功，用户处理并标记 AsyncCollector 为完成状态：complete，最终通过单独的线程 Emitter 将消息 emit 到下游，标记 complete  和 emitter 发射消息均是互斥访问 AsyncWaitOperator 的队列来完成的，通过锁来控制并发</li>
<li>访问失败，通知 AsyncCollector 有异常</li>
</ul>
<p>对于下游输出顺序，AsyncWaitOperator 的处理过程如下：</p>
<ul>
<li>有序：消息的发射顺序严格等同于消息的接收顺序，及 FIFO</li>
<li>无序：分两种情况：有 watermark 的情况下，在各 watermark 间隔内发射的消息彼此之间无需，但是跨 watermark 的消息还是会保证小猴顺序；没有 waterark 的情况下就完全不保证任何顺序性了</li>
</ul>
<h4 id="参数配置"><a href="#参数配置" class="headerlink" title="参数配置"></a>参数配置</h4><p>AsyncWaitOperator 的参数配置集中在构造器和 <code>setup()</code> 方法中，我们稍微总结下这个方法：</p>
<ul>
<li>获取执行 task 的 checkpointLock，此 lock 用来互斥 AsyncWaitOperator 内部队列的 checkpoint 和 队列元素的添加，没有这个 lock，就无法保证快照的准确性【保证快照时，队列里的元素都被写入 checkpoint】</li>
<li>获取输入元素的序列化器</li>
<li>初始化<em>单线程</em>线程池，此线程池被用于 AsyncWaitOperator 内部队列</li>
<li>初始化内部队列的类型：<code>OrderedStreamElementQueue</code> 和 <code>UnorderedStreamElementQueue</code> 分别对应有序和无序两种下游输出模式，下面会有详细介绍</li>
</ul>
<h4 id="算子打开"><a href="#算子打开" class="headerlink" title="算子打开"></a>算子打开</h4><p>AsyncWaitOperator 算子打开的逻辑在 <code>open()</code> 方法中，总结其逻辑：</p>
<ul>
<li>如果之前有快照队列元素，重新重快照恢复，并处理这些元素</li>
<li>初始化 Emitter 线程【下面会介绍】</li>
</ul>
<p>通过前面章节可以知道算子的快照状态初始化流程 ：</p>
<p>StreamTask.invoke() -&gt; AbstractStreamOperator.initializeState()</p>
<h4 id="处理消息-WaterMark"><a href="#处理消息-WaterMark" class="headerlink" title="处理消息/WaterMark"></a>处理消息/WaterMark</h4><p>AsyncWaitOperator 会将消息或 watermark 封装成 StreamElementQueueEntry 加入到内部队列，我们先来看下 StreamElementQueueEntry 的继承关系：</p>
<p><img src="queue-entry-extend.png" alt="queue-entry-extend.png"></p>
<h5 id="处理消息"><a href="#处理消息" class="headerlink" title="处理消息"></a>处理消息</h5><p>处理消息的逻辑集中在 <code>processElement()</code> 方法中</p>
<p>消息首先被封装成 StreamElementQueueEntry 的子类型 ：StreamElementQueueEntry，StreamElementQueueEntry 中维护了一个 FlinkCompletableFuture 实例，可以用来异步保存用户定义的 AsyncFunction 的回调结果</p>
<p>每个 StreamElementQueueEntry 都会注册一个 timer task，达到用户配置的超时时间后调用：</p>
<p><code>StreamRecordQueueEntry.collect(Throwable error)</code></p>
<p>在内部会将 FlinkCompletableFuture 内部的 scala future 标志位 failed，用户可以检测并做相应的逻辑处理</p>
<p>然后 StreamElementQueueEntry  会被加入 AsyncWaitOperator 的内部队列中，如果队列已满，会一直尝试加入，但是如果没有获取 checkpointingLock 会被阻塞</p>
<p>最后调用用户 <code>AsyncFunciton.invoke()</code> 执行异步调用</p>
<h5 id="处理-Watermark"><a href="#处理-Watermark" class="headerlink" title="处理 Watermark"></a>处理 Watermark</h5><p>比较简单，直接实例化一个 WatermarkQueueEntry 实例加入到内部队列中</p>
<h5 id="快照状态"><a href="#快照状态" class="headerlink" title="快照状态"></a>快照状态</h5><p>快照的逻辑集中在 <code>snapshotState()</code> 方法中，简单总结如下：</p>
<ul>
<li>清空原有的状态数据</li>
<li>将内部队列中的 entry 以及 当前被阻塞的 entry 【如果有】加入到快照中</li>
</ul>
<h3 id="AsyncWaitOperator-内部队列"><a href="#AsyncWaitOperator-内部队列" class="headerlink" title="AsyncWaitOperator 内部队列"></a>AsyncWaitOperator 内部队列</h3><p>上面我们说依据下游发送的顺序性，队列分为有序和无序两种，我们先来看有序队列</p>
<h4 id="有序队列-OrderedStreamElementQueue"><a href="#有序队列-OrderedStreamElementQueue" class="headerlink" title="有序队列 OrderedStreamElementQueue"></a>有序队列 OrderedStreamElementQueue</h4><p>有序队列的内部维护了一个 ArrayDeque 来保存记录，它维护了队列 FIFO 的顺序性，获取消息的时候会阻塞等待对首元素的操作完成，才会返回，添加消息的操作完全是阻塞的</p>
<p>它的内部维护了如下重要成员：</p>
<ul>
<li>capacity：内部 ArrayDeque 的大小</li>
<li>executor：执行回调用</li>
<li>lock：协调自身的方法调用</li>
<li>queue：内部队列 ArrayDeque</li>
</ul>
<p>实现都比较简单，特别要留意的是添加元素的逻辑：</p>
<ul>
<li>每个元素都被加到队列尾部</li>
<li>同时会为每个条目维护的 FlinkCompletableFuture 实例注册一个 complete 回调，对于 OrderedStreamElementQueue 来说，这个回调的作用就是通知 队首 元素已完成执行【会有一些阻塞等待的操作】</li>
</ul>
<h4 id="无序队列-UnorderedStreamElementQueue"><a href="#无序队列-UnorderedStreamElementQueue" class="headerlink" title="无序队列 UnorderedStreamElementQueue"></a>无序队列 UnorderedStreamElementQueue</h4><p>比起 OrderedStreamElementQueue，这个队列模型显得更有意思一些，为了实现跨 Watermark 相对有序、 Watermark 内部无序的特性，它维护了一些数据结构，我们来详细分析它的实现：</p>
<p>我们先来看它的重要成员：</p>
<ul>
<li>capacity：队列大小</li>
<li>executor：执行回调的线程池</li>
<li>unCompletedQueue：尚未完成的条目集合队列，不同的集合之间通过 WatermarkQueueEntry 隔开，同一集合内维护了一个 Watermark 间隔内接收到的元素</li>
<li>completedQueue：已完成的条目队列：统一维护已完成的 StreamRecordQueueEntry</li>
<li>firstSet：unCompletedQueue 中最早加入的元素集合</li>
<li>lastSet：unCompletedQueue 中最晚加入的元素集合</li>
</ul>
<p><em>接下来我们分析它的重要接口，来说明它为什么可以满足跨 Watermark 相对有序的特性</em></p>
<h5 id="阻塞添加-Entry"><a href="#阻塞添加-Entry" class="headerlink" title="阻塞添加 Entry"></a>阻塞添加 Entry</h5><p>如果维护的元素总数大于 capacity，则会阻塞，否则进入添加逻辑：</p>
<ul>
<li>如果是 watermark，将 lastSet 重置为大小为 capacity 的空集合，此时如果 firstSet 是空集合，将 firstSet 置为只包含此 watermark 的单元素集合，否则将此 watermark 封装成单元素集合加入到 unCompletedQueue 的队列尾部，最终再将 lastSet 添加到 unCompletedQueue 的尾部 ps: 总结一句话就是，不管 unCompletedQueue 当前是否为空，都在 unCompletedQueue 连续加入 watermark 单元素集合 和 大小为 capacity 的空集，如果为空时会初始化 firstSet 为包含 WatermarkQueueEntry 的单元素集合</li>
<li>如果是消息元素，加入到 lastSet 中</li>
<li>加入队列的每个条目维护的 FlinkCompletableFuture 实例都被注册一个 complete 回调，回调的逻辑如下：<ul>
<li>从 firstSet 中 remove 掉这个元素，并加入到 CompletedQueue 中</li>
<li>如果 firstSet 中的条目都已完成回调，从 unCompletedQueue 队首拿一个集合作为新的 firstSet</li>
<li><em>大家一定很奇怪，为什么要从 firstSet 中 remove 掉元素，如果这个回调条目不再 firstSet 中呢，岂不落空？重点来了，如果 firstSet 被置为 unCompletedQueue 中的新的队首元素，会马上遍历一遍集合元素，check 已完成后，从集合去除并加入到  CompletedQueue 中</em></li>
<li>最后会 signal 等待的 condition，有元素已完成</li>
</ul>
</li>
</ul>
<p>附一张图来解释这个过程：</p>
<p><img src="unordered_queue.png" alt="unordered_queue.png"></p>
<p>解释下这张图：</p>
<ul>
<li>每条新的元素记录都会生成一个 StreamRecordQueueEntry 并加入到 lastSet 集合中</li>
<li>每个新的 watermark 会被单独封装成集合加入到队列尾部</li>
<li>集合之间的并发访问是外部服务的 async client 控制的，比如上图中的 lastSet 和 es 中的 元素可能在一个 batch 请求中</li>
<li>每个 batch 请求成功后，该 batch 的元素触发回调，会优先处理 firstSet 中的元素，看是否存在：如果存在，从 firstSet 中移除并加入到 CompleteQueue 中；如果不存在，跳过不做任何处理，直到 firstSet 中的元素全部接受成功回调，将 unCompletedQueue 中的元素弹出作为新的 firstSet，并马上触发一次 check，将成功的元素加入 CompleteQueue [ 上面解释过一次 ]</li>
</ul>
<h3 id="Emitter"><a href="#Emitter" class="headerlink" title="Emitter"></a>Emitter</h3><p>上面介绍 AsyncWaitOperator 的时候有介绍过 Emitter 会单独开一个线程，负责发送异步队列中已完成的条目元素到下游，我们来看下它的构造参数：</p>
<ul>
<li>checkpointLock：checkpoint 锁，checkpoint 和 emit 操作之间必须互斥，因为可能会改变异步队列未完成的元素的部分【要做快照的正是这部分】</li>
<li>输出 Output</li>
<li>StreamElementQueue：上面介绍的有序或无序中的一种</li>
<li>timestampedCollector：附带时间戳的 Collector</li>
</ul>
<p>我们来看它的核心 run 方法：</p>
<ul>
<li>阻塞从 StreamElementQueue 中获取已完成元素发射到下游</li>
<li>如果是 WaterMark 元素，将元素从队列中弹出</li>
<li>如果是 RecordCollection，利用 timestampedCollector 附带统一时间戳发射到下游，后将 StreamRecordQueueEntry 弹出队列</li>
</ul>
<h3 id="Checkpoint-快照及恢复"><a href="#Checkpoint-快照及恢复" class="headerlink" title="Checkpoint 快照及恢复"></a>Checkpoint 快照及恢复</h3><h4 id="快照内容"><a href="#快照内容" class="headerlink" title="快照内容"></a>快照内容</h4><p>AsyncWaitOperator 算子的快照策略集中在 snapshotState 方法中，我们总结下其逻辑：</p>
<ul>
<li>清空旧的算子状态</li>
<li>调用内部队列的 <code>values()</code> 返回队列的条目集合，并从条目中取出 StreamElement 快照到状态存储</li>
<li>如果当前有正在阻塞的条目也会写入状态存储</li>
<li>如果快照中途发生任何异常，清空快照状态</li>
</ul>
<p>这里的关键是 <code>values()</code> 方法，我们来看不同内部队列的返回值：</p>
<ul>
<li>UnorderedStreamElementQueue: 返回 completedQueue + uncompletedQueue + firstSet 中所有条目，也就是已完成的回调和未完成的回调条目都会统一返回</li>
<li>OrderedStreamElementQueue: 返回内部 queue 的全部条目，同样包含已完成和未完成的回调条目</li>
</ul>
<h4 id="恢复策略"><a href="#恢复策略" class="headerlink" title="恢复策略"></a>恢复策略</h4><p>恢复策略集中在 AsyncWaitOperator 算子的 open() 方法中，我们会看到它会将快照中的元素全部再处理一次，包括已完成回调的快照元素</p>
<p><em>所以，我们可以得出这样的结论</em>：</p>
<ul>
<li>已完成的元素，如果还未发射到下游【仍在内部队列中】，仍然会被再处理一次再发射到下游，也就是这部分元素会访问两次外部服务</li>
<li>每条元素只会被发射一次到下游</li>
</ul>
<p>到这里 AsyncIO 的故障恢复策略已经基本明晰</p>
<p><em>下一章节我们重点分析下数据源算子 kafka-connector 在 EXACTLY_ONCE 处理语义下的恢复策略，结合上面的分析会有全面完整的认识！</em></p>
]]></content>
    
    <summary type="html">
    
      AsyncIO 是由 flink 1.2 引入的新特性，其目的是优化流处理过程中外部服务访问的耗时造成的瓶颈问题
    
    </summary>
    
      <category term="Flink源码" scheme="https://danny0405.github.io/categories/Flink%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="Flink内部实现" scheme="https://danny0405.github.io/tags/Flink%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>flink 对用户代码异常处理</title>
    <link href="https://danny0405.github.io/2017/02/09/flink%E5%AF%B9%E7%94%A8%E6%88%B7%E4%BB%A3%E7%A0%81%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"/>
    <id>https://danny0405.github.io/2017/02/09/flink对用户代码异常处理/</id>
    <published>2017-02-09T05:51:54.000Z</published>
    <updated>2018-01-23T06:12:19.146Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>flink 的架构在 flink 基本组件一节已经介绍过，其中的 TaskManager 负责监护 task 的执行，对于每个 task，flink 都会启动一个线程去执行，那么当用户的代码抛出异常时，flink 的处理逻辑是什么呢？</p>
<h2 id="异常后的组件通信"><a href="#异常后的组件通信" class="headerlink" title="异常后的组件通信"></a>异常后的组件通信</h2><p>一个 flink task 的线程 Runnable 类是 Task.java，我们观察到它的 <code>run()</code> 方法整个被一个大的 try catch 包住，这里重点关注 catch 用户异常之后的部分：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Task line612</span></div><div class="line">		<span class="keyword">catch</span> (Throwable t) &#123;</div><div class="line"></div><div class="line">			<span class="comment">// ----------------------------------------------------------------</span></div><div class="line">			<span class="comment">// the execution failed. either the invokable code properly failed, or</span></div><div class="line">			<span class="comment">// an exception was thrown as a side effect of cancelling</span></div><div class="line">			<span class="comment">// ----------------------------------------------------------------</span></div><div class="line"></div><div class="line">			<span class="keyword">try</span> &#123;</div><div class="line">				<span class="comment">// transition into our final state. we should be either in DEPLOYING, RUNNING, CANCELING, or FAILED</span></div><div class="line">				<span class="comment">// loop for multiple retries during concurrent state changes via calls to cancel() or</span></div><div class="line">				<span class="comment">// to failExternally()</span></div><div class="line">				<span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">					ExecutionState current = <span class="keyword">this</span>.executionState;</div></pre></td></tr></table></figure>
<p>简单总结其逻辑：</p>
<ul>
<li><p>如果当前的执行状态是 <code>ExecutionState.RUNNING</code> 或者 <code>ExecutionState.DEPLOYING</code>，表明是从正常运行到异常状态的过度，这时候判断是否是主动 Cancel 执行，如果是主动 Cancel，执行 StreamTask 的 <code>cancel()</code> 方法， 并通知观察者它的状态已变成：<code>ExecutionState.CANCELED</code>；如果不是主动 Cancel，表明是用户异常触发，这时候同样执行 StreamTask 的 <code>cancel()</code> 方法，然后通知观察者它的状态变成：<code>ExecutionState.FAILED</code>，这里的 cancel 方法留给 flink 内部的算子来实现，对于普通 task ，会停止消费上游数据，对于 source task，会停止发送源数据</p>
</li>
<li><p>对于用户异常来说，通知观察者的状态应该为 <code>ExecutionState.FAILED</code>，<em>我们下面详细分析</em></p>
</li>
<li><p>finally 的部分会释放掉这个 task 占有的所有资源，包括线程池、输入 InputGate 及 写出 ResultPartition 占用的全部 BufferPool、缓存的 jar 包等，最后通知 TaskManager 这个 Job 的 这个 task 已经执行结束：</p>
<p><code>notifyFinalState()</code></p>
</li>
</ul>
<ul>
<li>如果异常逻辑发生了任何其它异常，说明是 TaskManager 相关环境发生问题，这个时候会杀死 TaskManager</li>
</ul>
<h3 id="通知TaskManager"><a href="#通知TaskManager" class="headerlink" title="通知TaskManager"></a>通知TaskManager</h3><p>上面提到，finally 的最后阶段会通知 TaskManager，我们来梳理逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//TaskManager line444</span></div><div class="line"><span class="comment">// removes the task from the TaskManager and frees all its resources</span></div><div class="line">        <span class="function"><span class="keyword">case</span> <span class="title">TaskInFinalState</span><span class="params">(executionID)</span> </span>=&gt;</div><div class="line">          unregisterTaskAndNotifyFinalState(executionID)</div><div class="line"></div><div class="line"><span class="comment">//TaskManager line1228</span></div><div class="line"><span class="function"><span class="keyword">private</span> def <span class="title">unregisterTaskAndNotifyFinalState</span><span class="params">(executionID: ExecutionAttemptID)</span>: Unit </span>= &#123;</div><div class="line"></div><div class="line">    val task = runningTasks.remove(executionID)</div><div class="line">    <span class="keyword">if</span> (task != <span class="keyword">null</span>) &#123;</div><div class="line"></div><div class="line">      <span class="comment">// the task must be in a terminal state</span></div><div class="line">      <span class="keyword">if</span> (!task.getExecutionState.isTerminal) &#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          task.failExternally(<span class="keyword">new</span> Exception(<span class="string">"Task is being removed from TaskManager"</span>))</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> e: Exception =&gt; log.error(<span class="string">"Could not properly fail task"</span>, e)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line"><span class="comment">//TaskManager line1251</span></div><div class="line">     self ! decorateMessage(</div><div class="line">        UpdateTaskExecutionState(</div><div class="line">          <span class="keyword">new</span> TaskExecutionState(</div><div class="line">            task.getJobID,</div><div class="line">            task.getExecutionId,</div><div class="line">            task.getExecutionState,</div><div class="line">            task.getFailureCause,</div><div class="line">            accumulators)</div><div class="line">        )</div><div class="line">      )</div><div class="line"></div><div class="line"><span class="comment">//ExecutionGraph line1189</span></div><div class="line">				<span class="keyword">case</span> FAILED:</div><div class="line">					attempt.markFailed(state.getError(userClassLoader));</div><div class="line">					<span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line"></div><div class="line"><span class="comment">//Execution line658</span></div><div class="line">	<span class="function"><span class="keyword">void</span> <span class="title">markFinished</span><span class="params">(Map&lt;AccumulatorRegistry.Metric, Accumulator&lt;?, ?&gt;&gt; flinkAccumulators, Map&lt;String, Accumulator&lt;?, ?&gt;&gt; userAccumulators)</span> </span>&#123;</div><div class="line"></div><div class="line">		<span class="comment">// this call usually comes during RUNNING, but may also come while still in deploying (very fast tasks!)</span></div><div class="line">		<span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">			ExecutionState current = <span class="keyword">this</span>.state;</div><div class="line"></div><div class="line">			<span class="keyword">if</span> (current == RUNNING || current == DEPLOYING) &#123;</div><div class="line"></div><div class="line">				<span class="keyword">if</span> (transitionState(current, FINISHED)) &#123;</div><div class="line">					<span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line"><span class="comment">//Execution line991</span></div><div class="line">			<span class="keyword">try</span> &#123;</div><div class="line">				vertex.notifyStateTransition(attemptId, targetState, error);</div><div class="line">			&#125;</div><div class="line">			<span class="keyword">catch</span> (Throwable t) &#123;</div><div class="line">				LOG.error(<span class="string">"Error while notifying execution graph of execution state transition."</span>, t);</div><div class="line">			&#125;</div><div class="line"></div><div class="line"><span class="comment">//ExecutionGraph line1291</span></div><div class="line">	<span class="function"><span class="keyword">void</span> <span class="title">notifyExecutionChange</span><span class="params">(JobVertexID vertexId, <span class="keyword">int</span> subtask, ExecutionAttemptID executionID, ExecutionState</span></span></div><div class="line">							newExecutionState, Throwable error)</div><div class="line">	&#123;</div><div class="line">     <span class="comment">//...</span></div><div class="line">        <span class="comment">// see what this means for us. currently, the first FAILED state means -&gt; FAILED</span></div><div class="line">		<span class="keyword">if</span> (newExecutionState == ExecutionState.FAILED) &#123;</div><div class="line">			fail(error);</div><div class="line">		&#125;</div><div class="line"></div><div class="line"><span class="comment">//ExecutionGraph line845     </span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">fail</span><span class="params">(Throwable t)</span> </span>&#123;</div><div class="line">		<span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">			JobStatus current = state;</div><div class="line">			<span class="comment">// stay in these states</span></div><div class="line">			<span class="keyword">if</span> (current == JobStatus.FAILING ||</div><div class="line">				current == JobStatus.SUSPENDED ||</div><div class="line">				current.isGloballyTerminalState()) &#123;</div><div class="line">				<span class="keyword">return</span>;</div><div class="line">			&#125; <span class="keyword">else</span> <span class="keyword">if</span> (current == JobStatus.RESTARTING &amp;&amp; transitionState(current, JobStatus.FAILED, t)) &#123;</div><div class="line">				<span class="keyword">synchronized</span> (progressLock) &#123;</div><div class="line">					postRunCleanup();</div><div class="line">					progressLock.notifyAll();</div><div class="line"></div><div class="line">					LOG.info(<span class="string">"Job &#123;&#125; failed during restart."</span>, getJobID());</div><div class="line">					<span class="keyword">return</span>;</div><div class="line">				&#125;</div><div class="line">			&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transitionState(current, JobStatus.FAILING, t)) &#123;</div><div class="line">				<span class="keyword">this</span>.failureCause = t;</div><div class="line"></div><div class="line">				<span class="keyword">if</span> (!verticesInCreationOrder.isEmpty()) &#123;</div><div class="line">					<span class="comment">// cancel all. what is failed will not cancel but stay failed</span></div><div class="line">					<span class="keyword">for</span> (ExecutionJobVertex ejv : verticesInCreationOrder) &#123;</div><div class="line">						ejv.cancel();</div><div class="line">					&#125;</div><div class="line">				&#125; <span class="keyword">else</span> &#123;</div><div class="line">					<span class="comment">// set the state of the job to failed</span></div><div class="line">					transitionState(JobStatus.FAILING, JobStatus.FAILED, t);</div><div class="line">				&#125;</div><div class="line"></div><div class="line">				<span class="keyword">return</span>;</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			<span class="comment">// no need to treat other states</span></div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>总结其逻辑：</p>
<ul>
<li>在一些合法性 check 之后，TaskManager 会给自己发送一条路由消息：<code>UpdateTaskExecutionState</code>，TaskManager 继而将这条消息转发给 JobManager</li>
<li>JobManager 会标志 Job 状态为 FAILING 并通知 JobCli，并且立即停止所有 task 的执行，这时候 CheckpointCoordinator 在执行 checkpoint 的时候感知到 task 失败状态会立即返回，停止 checkpoint</li>
</ul>
<h2 id="异常后的资源释放"><a href="#异常后的资源释放" class="headerlink" title="异常后的资源释放"></a>异常后的资源释放</h2><p>主要包括以下资源：</p>
<ul>
<li>网络资源：InputGate 和 ResultPartiton 的内存占用</li>
<li>其他内存：通过 MemoryManager 申请的资源</li>
<li>缓存资源：lib 包和其他缓存</li>
<li>线程池：Task 内部持有</li>
</ul>
]]></content>
    
    <summary type="html">
    
      flink 的架构在 flink 基本组件一节已经介绍过，其中的 TaskManager 负责监护 task 的执行，对于每个 task，flink 都会启动一个线程去执行，那么当用户的代码抛出异常时，flink 的处理逻辑是什么呢？
    
    </summary>
    
      <category term="Flink源码" scheme="https://danny0405.github.io/categories/Flink%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="Flink容错" scheme="https://danny0405.github.io/tags/Flink%E5%AE%B9%E9%94%99/"/>
    
  </entry>
  
  <entry>
    <title>flink scheduler</title>
    <link href="https://danny0405.github.io/2017/02/09/flink-scheduler/"/>
    <id>https://danny0405.github.io/2017/02/09/flink-scheduler/</id>
    <published>2017-02-09T05:48:52.000Z</published>
    <updated>2018-01-23T06:12:31.728Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前面已经介绍了一系列的 flink 任务抽象、网络传输、可靠性机制等细节，有了这些铺垫，终于可以开心的介绍 flink 的任务调度机制了，也是不易^_^</p>
<p>因为没有这些铺垫，就无法明白 flink 为什么要设计这样的一套调度机制！所以本章节讲解时会多穿插一些为什么</p>
<h2 id="资源组"><a href="#资源组" class="headerlink" title="资源组"></a>资源组</h2><h3 id="资源组模型"><a href="#资源组模型" class="headerlink" title="资源组模型"></a>资源组模型</h3><p>flink 的一个 Instance 可以被划分出多个 Slot，通过初始参数可以指定，他们既可以是 SimpleSlot，也可以是同时跑多个 task 的 SharedSlot，为了约束 task 之间的运行时的绑定关系，flink 抽象出了 SlotSharingGroup 和 CoLocationGroup 的概念。</p>
<p>一个 SlotSharingGroup 规定了一个 Job 的 DAG 图中的哪些 JobVertex 的 sub task 可以部署到一个 SharedSlot 上，这是一个软限制，并不是一定会满足，只是调度的时候有位置偏好，而 CoLocationGroup 是在 SlotSharingGroup 的基础上的硬限制，它限定了 CoLocationGroup 中的 JobVertex 中的 sub task 运行必须是一一对应的：假如 CoLocationGrou 限定了 JobVertex A 和 B ，那么 A 的编号为 i 的 sub task 必须和 B 的编号为 i 的 sub task 跑在一起。假如一个 job 的运算逻辑包括 source -&gt; head -&gt; tail -&gt; sink，那么它的 task 运行时限制关系见下图：</p>
<p><img src="flink-slot-group.png" alt="flink-slot-group.png"></p>
<h3 id="资源组-1"><a href="#资源组-1" class="headerlink" title="资源组"></a>资源组</h3><h4 id="SlotSharingGroup"><a href="#SlotSharingGroup" class="headerlink" title="SlotSharingGroup"></a>SlotSharingGroup</h4><p>上面已经提到 SlotSharingGroup 具有绑定 JobVertex 的 sub task 运行的作用，用户可以自己为 JobVertex 定义一个 SlotSharingGroup，如果不定义的话使用名为 default 的 SlotSharingGroup，定义的接口如下：</p>
<p><code>someStream.filter(...).slotSharingGroup(&quot;name&quot;);</code></p>
<h4 id="ColocationGroup"><a href="#ColocationGroup" class="headerlink" title="ColocationGroup"></a>ColocationGroup</h4><p>ColocationGroup 通过 CoLocationConstraint 来管理一个 SharedSlot 上的 sub task</p>
<p>用户同样可以通过 api 定义 ColocationGroup：</p>
<h3 id="资源Slot"><a href="#资源Slot" class="headerlink" title="资源Slot"></a>资源Slot</h3><p>一个 TaskManager 在初始化时可以指定自己最大持有的 Slot 数，包括 SharedSlot 和 SimpleSlot。</p>
<p>flink 使用 slot 作为资源抽象【主要是 cpu 和 memory】，一个 Instance 可以持有多个 SharedSlot，一个 SharedSlot 可以并行执行多个 sub task，对于 PIPELINED 来说，一种典型的模式就是一个 SharedSlot 同时执行一个 job 每个 JobVertex 上的一个并行 task，这样不仅可以尽量保证每个 Instance 上的任务负载尽量均匀，也能最大化的利用 PIPELINED 的流水线处理特性优化网络传输。</p>
<p>flink 的 slot 有两种：SharedSlot 和 SimpleSlot，前者可以绑定执行多个 sub task，后者代表一个 task 的资源占用。</p>
<h4 id="SharedSlot"><a href="#SharedSlot" class="headerlink" title="SharedSlot"></a>SharedSlot</h4><p>一个 SharedSlot 可以拥有多个 SimpleSlot，也可以包含嵌套的 SharedSlot【ColocationConstraint】，这样便形成了树形结构，SimpleSlot 和 SharedSlot 继承自共同的接口：Slot，它们都包含如下的关键信息：</p>
<ul>
<li>jobID：被哪个 job 占有</li>
<li>groupID：属于哪个 SlotSharingGroup</li>
<li>instance：属于哪个 TaskManager，或者属于哪个物理节点</li>
<li>status：当前分配状态，共有四种状态 ALLOCATED_AND_ALIVE、CANCELLED、RELEASED、unknown</li>
</ul>
<p><em>只有定义了 SlotSharingGroup 时才会通过 SharedSlot 来绑定 sub task 的执行</em></p>
<h4 id="SimpleSlot"><a href="#SimpleSlot" class="headerlink" title="SimpleSlot"></a>SimpleSlot</h4><p>SimpleSlot 是执行单个 task 的 slot 抽象，它既可以在 TaskManager 上独立存在，也可以作为 SharedSlot 的子节点，内部封装了一个 task 的一次 Execution</p>
<p>继承关系下如下图：</p>
<p>SharedSlot 可以视作管理 SimpleSlot 的工具，那么 SharedSlot 自身又由什么方式管理呢？</p>
<h4 id="SlotSharingGroupAssignment"><a href="#SlotSharingGroupAssignment" class="headerlink" title="SlotSharingGroupAssignment"></a>SlotSharingGroupAssignment</h4><p>flink 通过抽象 SlotSharingGroupAssignment 来管理 SharedSlot，这里的资源以  JobVertex 微粒度划分 group，也就是一个 JobVertex 占有一个资源 group。</p>
<h5 id="Slot初始划分"><a href="#Slot初始划分" class="headerlink" title="Slot初始划分"></a>Slot初始划分</h5><p>SlotSharingGroupAssignment 是如何添加一个初始的 SharedSlot 节点的呢？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//SlotSharingGroupAssignment line174</span></div><div class="line"><span class="function"><span class="keyword">private</span> SimpleSlot <span class="title">addSharedSlotAndAllocateSubSlot</span><span class="params">(SharedSlot sharedSlot, Locality locality,</span></span></div><div class="line">													JobVertexID groupId, CoLocationConstraint constraint) &#123;</div><div class="line">		<span class="comment">// sanity checks</span></div><div class="line">		<span class="keyword">if</span> (!sharedSlot.isRootAndEmpty()) &#123;</div><div class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"The given slot is not an empty root slot."</span>);</div><div class="line">		&#125;</div></pre></td></tr></table></figure>
<p>总结其逻辑：</p>
<ul>
<li>这里必须是一个根节点 SharedSlot【没有父亲节点和子节点】，也就是 TaskManager 被配置的一个 Slot</li>
<li>如果没有强制的 task 位置绑定【ColocationConstraint】，从根 SharedSlot 上分配一个 SImpleSlot，编号为递增的 simple slot 个数</li>
<li>如果有 ColocationConstraint 限制，为传入的 SharedSlot 生成一个子 SharedSlot 并分配 SimpleSlot 注册到 ColocationConstraint 中</li>
<li>如果申请到 slot【simple or shared】，设置位置偏好：LOCAL、NON_LOCAL、UNCONSTRAINED【相对于持有的 Instance 来说】，并且将这个 SharedSlot 加入到其它 JobVertex 的可调度资源队列中，也就是说其它的 JobVertex 都可以讲在这个 SharedSlot 上部署自己的 sub task</li>
</ul>
<h5 id="为Task分配-SharedSlot"><a href="#为Task分配-SharedSlot" class="headerlink" title="为Task分配 SharedSlot"></a>为Task分配 SharedSlot</h5><p>最底层的分配策略：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//SlotSharingGroupAssignment line408</span></div><div class="line"><span class="function"><span class="keyword">private</span> Pair&lt;SharedSlot, Locality&gt; <span class="title">getSlotForTaskInternal</span><span class="params">(AbstractID groupId,</span></span></div><div class="line">																Iterable&lt;Instance&gt; preferredLocations,</div><div class="line">																<span class="keyword">boolean</span> localOnly)</div><div class="line">	&#123;</div><div class="line">		<span class="comment">// check if there is anything at all in this group assignment</span></div><div class="line">		<span class="keyword">if</span> (allSlots.isEmpty()) &#123;</div><div class="line">			<span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="comment">// get the available slots for the group</span></div><div class="line">		Map&lt;Instance, List&lt;SharedSlot&gt;&gt; slotsForGroup = availableSlotsPerJid.get(groupId);</div></pre></td></tr></table></figure>
<p>总结其逻辑：</p>
<ul>
<li>判断对应的 group 是否有资源，如果没有将 SlotSharingGroupAssignment 目前所有的 SharedSlot 槽位视为可用资源</li>
<li>如果对偏好位置有要求，从 group 里筛选是否有满足的 SharedSlot，如果有设置 Locality 为 LOCAL 并返回，同时从该 group 的资源组中移除该 SharedSlot</li>
<li>如果对位置有要求但是没有找到符合的 SharedSlot，则从资源组里选择第一个可用的 SharedSlot，并将 Locality 设置为 <code>Locality.NON_LOCA</code></li>
<li>如果对位置没要求，则从资源组里选择第一个可用的 SharedSlot，并将 Locality 设置为 <code>Locality.UNCONSTRAINED</code></li>
<li>如果没资源，返回 null</li>
<li>一旦一个 group 中的 SharedSlot 被分出去就会被从资源池中删除，也就是说一个 SharedSlot 不可能分配一个 JobVertex 的两个 sub tasks，这一点非常重要</li>
</ul>
<h6 id="无-CoLocationConstraint-限制的资源划分策略"><a href="#无-CoLocationConstraint-限制的资源划分策略" class="headerlink" title="无 CoLocationConstraint 限制的资源划分策略"></a>无 CoLocationConstraint 限制的资源划分策略</h6><p>主要是从走上面的逻辑，细节这里就不说了</p>
<h6 id="有-CoLocationConstraint-限制的资源划分策略"><a href="#有-CoLocationConstraint-限制的资源划分策略" class="headerlink" title="有 CoLocationConstraint 限制的资源划分策略"></a>有 CoLocationConstraint 限制的资源划分策略</h6><p>有 CoLocationConstraint 限制的时候，优先考虑 CoLocationConstraint 中的 SharedSlot【如果之前 CoLocationGroup 中的其它 task 分配过】，如果 CoLocationConstraint 中还没有分配 SharedSlot 则重新分配，并且再分配一个 SharedSlot 子节点，再这个节点上划出 SimpleSlot 供 task 使用</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//SlotSharingGroupAssignment line333</span></div><div class="line"><span class="function">SimpleSlot <span class="title">getSlotForTask</span><span class="params">(CoLocationConstraint constraint, Iterable&lt;Instance&gt; locationPreferences)</span> </span>&#123;</div><div class="line">		<span class="keyword">synchronized</span> (lock) &#123;</div><div class="line">			<span class="keyword">if</span> (constraint.isAssignedAndAlive()) &#123;</div><div class="line">				<span class="comment">// the shared slot of the co-location group is initialized and set we allocate a sub-slot</span></div><div class="line">				<span class="keyword">final</span> SharedSlot shared = constraint.getSharedSlot();</div><div class="line">				SimpleSlot subslot = shared.allocateSubSlot(<span class="keyword">null</span>);</div><div class="line">				subslot.setLocality(Locality.LOCAL);</div><div class="line">				<span class="keyword">return</span> subslot;</div><div class="line">			&#125;</div><div class="line">			<span class="keyword">else</span> <span class="keyword">if</span> (constraint.isAssigned()) &#123;</div><div class="line">				<span class="comment">// we had an assignment before.</span></div></pre></td></tr></table></figure>
<ul>
<li>如果之前 CoLocationGroup 中的其它 sub task 有分配过资源，直接复用这个资源，对应 Locality 属性为 <code>Locality.LOCAL</code></li>
<li>如果之前分配过，但是该 SharedSlot 但是却被标记死亡，那么依据之前的 SharedSlot 的所在节点重新分配一次 SharedSlot，再此基础上再分配一个 SharedSlot，后分配 SimpleSlot 返回，对应 Locality 属性为 <code>Locality.LOCAL</code></li>
<li>如果是第一次分配，依据节点的偏好位置为参考，并再此基础上再分配一个 SharedSlot，后分配 SimpleSlot 返回，对应 Locality 属性为 <code>Locality.LOCAL</code></li>
</ul>
<h2 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h2><p>flink 调度器的调度单位被抽象为一个 ScheduledUnit，一个 ScheduledUnit 封装了以下信息：Execution、SlotSharingGroup、CoLocationConstraint</p>
<p>flink 的关于调度的细节全部集成于 Scheduler</p>
<h3 id="调度细节"><a href="#调度细节" class="headerlink" title="调度细节"></a>调度细节</h3><p> 首先来明确下 Scheduler 的调度核心：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Scheduler line156</span></div><div class="line"><span class="function"><span class="keyword">private</span> Object <span class="title">scheduleTask</span><span class="params">(ScheduledUnit task, <span class="keyword">boolean</span> queueIfNoResource)</span> <span class="keyword">throws</span> NoResourceAvailableException </span>&#123;</div><div class="line">		<span class="keyword">if</span> (task == <span class="keyword">null</span>) &#123;</div><div class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</div><div class="line">			LOG.debug(<span class="string">"Scheduling task "</span> + task);</div><div class="line">		&#125;</div></pre></td></tr></table></figure>
<p>总结其逻辑：</p>
<ul>
<li>先判断该类型的 ExecutionVertex 是否已强制指定执行节点</li>
<li>如果有设置 SlotSharingGroup ，拿到对应的 SlotSharingGroupAssignment 和 ColocationConstraint，走上面描述的接口拿到分配的 SimpleSlot A，如果有 ColocationConstraint，会锁定位置，表示已经获取 SharedSlot 并分配完成，如果这个 SimpleSlot 的 Locality 是 LOCAL【预期的位置】，则立即返回，否则走下面的流程</li>
<li>如果上面的过程没有获取到 SimpleSlot，那么表示当前已经没有符合要求的 SharedSlot，这时候会重新分配一个新的 SharedSlot，方式是先遍历当前有资源的 Instance 依据偏好位置找到其中一个，并在 SlotSharingGroupAssignment 中注册返回一个新的 SimpleSlot B【细节上面介绍 SlotSharingGroupAssignment 时已说明】</li>
<li>如果已走到这一步，比较 A 与 B 的优劣，主要是调度位置是否符合预期的比较，选择更优的，释放掉另一个</li>
<li>如果没有 SlotSharingGroup 的约束，直接从 Instance 上申请一个根 SimpleSlot 来执行这个 task</li>
<li>以上调度器在分配 SharedSlot 的时候维护了一个队列：<code>instancesWithAvailableResources</code>，每次有 Slot 资源的 Instance 被加入对列尾部，消费过的 Slot 会被 remove，这样可以轮询机器，可以使机器的 SharedSlot 分配尽量均衡</li>
</ul>
<h4 id="约束信息的生成"><a href="#约束信息的生成" class="headerlink" title="约束信息的生成"></a>约束信息的生成</h4><p>影响上面调度预期位置有三个重要因素：SlotSharingGroup、ColocationConstraint、prefferedLocations，我们逐一分析它们的生成逻辑：</p>
<h5 id="SlotSharingGroup-1"><a href="#SlotSharingGroup-1" class="headerlink" title="SlotSharingGroup"></a>SlotSharingGroup</h5><p>向上追溯我们发现，Scheduler 的调度逻辑由 Execution 触发：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Execution line265</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">scheduleForExecution</span><span class="params">(Scheduler scheduler, <span class="keyword">boolean</span> queued)</span> <span class="keyword">throws</span> NoResourceAvailableException </span>&#123;</div><div class="line">   <span class="keyword">if</span> (scheduler == <span class="keyword">null</span>) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Cannot send null Scheduler when scheduling execution."</span>);</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">final</span> SlotSharingGroup sharingGroup = vertex.getJobVertex().getSlotSharingGroup();</div><div class="line">   <span class="keyword">final</span> CoLocationConstraint locationConstraint = vertex.getLocationConstraint();</div><div class="line"></div><div class="line">   <span class="comment">// sanity check</span></div><div class="line">   <span class="keyword">if</span> (locationConstraint != <span class="keyword">null</span> &amp;&amp; sharingGroup == <span class="keyword">null</span>) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Trying to schedule with co-location constraint but without slot sharing allowed."</span>);</div><div class="line">   &#125;</div><div class="line"></div><div class="line"><span class="comment">//StreamingJobGraphGenerator line416</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setSlotSharing</span><span class="params">()</span> </span>&#123;</div><div class="line"></div><div class="line">		Map&lt;String, SlotSharingGroup&gt; slotSharingGroups = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line"></div><div class="line">		<span class="keyword">for</span> (Entry&lt;Integer, JobVertex&gt; entry : jobVertices.entrySet()) &#123;</div><div class="line"></div><div class="line">			String slotSharingGroup = streamGraph.getStreamNode(entry.getKey()).getSlotSharingGroup();</div><div class="line"></div><div class="line">			SlotSharingGroup group = slotSharingGroups.get(slotSharingGroup);</div><div class="line">			<span class="keyword">if</span> (group == <span class="keyword">null</span>) &#123;</div><div class="line">				group = <span class="keyword">new</span> SlotSharingGroup();</div><div class="line">				slotSharingGroups.put(slotSharingGroup, group);</div><div class="line">			&#125;</div><div class="line">			entry.getValue().setSlotSharingGroup(group);</div><div class="line">		&#125;</div><div class="line"></div><div class="line"><span class="comment">//StreamGraphGenerator line577</span></div><div class="line"><span class="function"><span class="keyword">private</span> String <span class="title">determineSlotSharingGroup</span><span class="params">(String specifiedGroup, Collection&lt;Integer&gt; inputIds)</span> </span>&#123;</div><div class="line">		<span class="keyword">if</span> (specifiedGroup != <span class="keyword">null</span>) &#123;</div><div class="line">			<span class="keyword">return</span> specifiedGroup;</div><div class="line">		&#125; <span class="keyword">else</span> &#123;</div><div class="line">			String inputGroup = <span class="keyword">null</span>;</div><div class="line">			<span class="keyword">for</span> (<span class="keyword">int</span> id: inputIds) &#123;</div><div class="line">				String inputGroupCandidate = streamGraph.getSlotSharingGroup(id);</div><div class="line">				<span class="keyword">if</span> (inputGroup == <span class="keyword">null</span>) &#123;</div><div class="line">					inputGroup = inputGroupCandidate;</div><div class="line">				&#125; <span class="keyword">else</span> <span class="keyword">if</span> (!inputGroup.equals(inputGroupCandidate)) &#123;</div><div class="line">					<span class="keyword">return</span> <span class="string">"default"</span>;</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">			<span class="keyword">return</span> inputGroup == <span class="keyword">null</span> ? <span class="string">"default"</span> : inputGroup;</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>总结其逻辑：</p>
<ul>
<li>如果用户没有为 JobVertex 指定 SlotSharingGroup ，则生成名为 ‘default‘ 的 SlotSharingGroup，否则为每个用户指定的名字定义一个 SlotSharingGroup</li>
<li>同一个 SlotSharingGroup 中的节点的 sub task 会共享 SharedSlot 资源</li>
</ul>
<h5 id="ColocationConstraint"><a href="#ColocationConstraint" class="headerlink" title="ColocationConstraint"></a>ColocationConstraint</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//CoLocationGroup line81</span></div><div class="line"><span class="function"><span class="keyword">public</span> CoLocationConstraint <span class="title">getLocationConstraint</span><span class="params">(<span class="keyword">int</span> subtask)</span> </span>&#123;</div><div class="line">		ensureConstraints(subtask + <span class="number">1</span>);</div><div class="line">		<span class="keyword">return</span> constraints.get(subtask);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureConstraints</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</div><div class="line">		<span class="keyword">if</span> (constraints == <span class="keyword">null</span>) &#123;</div><div class="line">			constraints = <span class="keyword">new</span> ArrayList&lt;CoLocationConstraint&gt;(num);</div><div class="line">		&#125; <span class="keyword">else</span> &#123;</div><div class="line">			constraints.ensureCapacity(num);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (num &gt; constraints.size()) &#123;</div><div class="line">			constraints.ensureCapacity(num);</div><div class="line">			<span class="keyword">for</span> (<span class="keyword">int</span> i = constraints.size(); i &lt; num; i++) &#123;</div><div class="line">				constraints.add(<span class="keyword">new</span> CoLocationConstraint(<span class="keyword">this</span>));</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>总结其逻辑：</p>
<ul>
<li>CoLocationGroup 的生成逻辑和 SlotSharingGroup 类似，不过这个没有默认，需要用户手动指定，并且前提是需要有 SlotSharingGroup</li>
<li>从 CoLocationGroup 内 sub task 的 index 获取一个 ColocationConstraint，这样便实现了一一对应关系</li>
</ul>
<h5 id="prefferedLocations"><a href="#prefferedLocations" class="headerlink" title="prefferedLocations"></a>prefferedLocations</h5><p>这是调度位置信息的关键，不管是 CoLocationGroup 还是 SlotSharingGroup，都会优先参考节点偏好来申请资源，那么 flink 是依据什么信息来生成偏好位置的呢？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ExecutionVertex line373</span></div><div class="line"><span class="function"><span class="keyword">public</span> Iterable&lt;Instance&gt; <span class="title">getPreferredLocations</span><span class="params">()</span> </span>&#123;</div><div class="line">		<span class="comment">// if we have hard location constraints, use those</span></div><div class="line">		List&lt;Instance&gt; constraintInstances = <span class="keyword">this</span>.locationConstraintInstances;</div><div class="line">		<span class="keyword">if</span> (constraintInstances != <span class="keyword">null</span> &amp;&amp; !constraintInstances.isEmpty()) &#123;</div><div class="line">			<span class="keyword">return</span> constraintInstances;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="comment">// otherwise, base the preferred locations on the input connections</span></div><div class="line">		<span class="keyword">if</span> (inputEdges == <span class="keyword">null</span>) &#123;</div><div class="line">			<span class="keyword">return</span> Collections.emptySet();</div><div class="line">		&#125;</div></pre></td></tr></table></figure>
<p>总结其逻辑：</p>
<ul>
<li>flink 计算节点的上游所有生产者所在节点，并作为自己的偏好位置</li>
<li>清空之前旧的偏好位置</li>
</ul>
<p><em>这样就形成了最开始【资源组模型】一节中的调度模式，因为一开始的 source task 显然没有 prefferedLocations，由调度细节可以知道 flink 会轮询集群的不同 Instance，将 source task 分配在这些机器上，后面的 source task 的 consumer task 会优先调度到 source task 的节点上，这样便形成了一开始的调度模式！</em></p>
<h3 id="触发调度"><a href="#触发调度" class="headerlink" title="触发调度"></a>触发调度</h3><ul>
<li>第一次提交任务时</li>
<li>有新的 Instance 或者新的 Slot 获取时，会轮询排对的调度任务进行调度</li>
</ul>
<h3 id="调度流程"><a href="#调度流程" class="headerlink" title="调度流程"></a>调度流程</h3><p><em>这里只介绍 streaming 的流程，批处理类似，有兴趣的童鞋自行研究！</em></p>
<p>先梳理代码逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ExecutionGraph line 716</span></div><div class="line"><span class="comment">//schedule from source JobVertex first</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">scheduleForExecution</span><span class="params">(Scheduler scheduler)</span> <span class="keyword">throws</span> JobException </span>&#123;</div><div class="line">		<span class="keyword">if</span> (scheduler == <span class="keyword">null</span>) &#123;</div><div class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Scheduler must not be null."</span>);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (<span class="keyword">this</span>.scheduler != <span class="keyword">null</span> &amp;&amp; <span class="keyword">this</span>.scheduler != scheduler) &#123;</div><div class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Cannot use different schedulers for the same job"</span>);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (transitionState(JobStatus.CREATED, JobStatus.RUNNING)) &#123;</div><div class="line">			<span class="keyword">this</span>.scheduler = scheduler;</div><div class="line"></div><div class="line">			<span class="keyword">switch</span> (scheduleMode) &#123;</div><div class="line"></div><div class="line">				<span class="keyword">case</span> FROM_SOURCES:</div><div class="line">					<span class="comment">// simply take the vertices without inputs.</span></div><div class="line">					<span class="keyword">for</span> (ExecutionJobVertex ejv : <span class="keyword">this</span>.tasks.values()) &#123;</div><div class="line">						<span class="keyword">if</span> (ejv.getJobVertex().isInputVertex()) &#123;</div><div class="line">							ejv.scheduleAll(scheduler, allowQueuedScheduling);</div><div class="line">						&#125;</div><div class="line">					&#125;</div><div class="line">					<span class="keyword">break</span>;</div><div class="line"></div><div class="line"><span class="comment">//NetworkEnvironment line348</span></div><div class="line"><span class="comment">//For PIPELINED, eagerly notify is true, when source tasks is deployed and registered in NetworkEnvironment, will trigger consumer task to deploy</span></div><div class="line"><span class="keyword">for</span> (ResultPartition partition : producedPartitions) &#123;</div><div class="line">			<span class="comment">// Eagerly notify consumers if required.</span></div><div class="line">			<span class="keyword">if</span> (partition.getEagerlyDeployConsumers()) &#123;</div><div class="line">				jobManagerNotifier.notifyPartitionConsumable(</div><div class="line">						partition.getJobId(), partition.getPartitionId());</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line"></div><div class="line"><span class="comment">//NetworkEnvironment line467</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">notifyPartitionConsumable</span><span class="params">(JobID jobId, <span class="keyword">final</span> ResultPartitionID partitionId)</span> </span>&#123;</div><div class="line"></div><div class="line">			<span class="keyword">final</span> ScheduleOrUpdateConsumers msg = <span class="keyword">new</span> ScheduleOrUpdateConsumers(jobId, partitionId);</div><div class="line"></div><div class="line">			Future&lt;Object&gt; futureResponse = jobManager.ask(msg, jobManagerMessageTimeout);</div><div class="line"></div><div class="line">			...</div><div class="line"><span class="comment">//Explain why for PIPELINED, eagerlyDeployConsumers is always true here.</span></div><div class="line"><span class="comment">//JobVertex line371</span></div><div class="line"><span class="function"><span class="keyword">public</span> JobEdge <span class="title">connectNewDataSetAsInput</span><span class="params">(</span></span></div><div class="line">			JobVertex input,</div><div class="line">			DistributionPattern distPattern,</div><div class="line">			ResultPartitionType partitionType,</div><div class="line">			<span class="keyword">boolean</span> eagerlyDeployConsumers) &#123;</div><div class="line"></div><div class="line">		IntermediateDataSet dataSet = input.createAndAddResultDataSet(partitionType);</div><div class="line">		dataSet.setEagerlyDeployConsumers(eagerlyDeployConsumers);</div><div class="line"></div><div class="line">		JobEdge edge = <span class="keyword">new</span> JobEdge(dataSet, <span class="keyword">this</span>, distPattern);</div><div class="line">		<span class="keyword">this</span>.inputs.add(edge);</div><div class="line">		dataSet.addConsumer(edge);</div><div class="line">		<span class="keyword">return</span> edge;</div><div class="line">	&#125;</div><div class="line"></div><div class="line"><span class="comment">//StreamingJobGraphGenerator line371</span></div><div class="line"><span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardPartitioner) &#123;</div><div class="line">			downStreamVertex.connectNewDataSetAsInput(</div><div class="line">				headVertex,</div><div class="line">				DistributionPattern.POINTWISE,</div><div class="line">				ResultPartitionType.PIPELINED,</div><div class="line">				<span class="keyword">true</span>);</div><div class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> RescalePartitioner)&#123;</div><div class="line">			downStreamVertex.connectNewDataSetAsInput(</div><div class="line">				headVertex,</div><div class="line">				DistributionPattern.POINTWISE,</div><div class="line">				ResultPartitionType.PIPELINED,</div><div class="line">				<span class="keyword">true</span>);</div><div class="line">		&#125; <span class="keyword">else</span> &#123;</div><div class="line">			downStreamVertex.connectNewDataSetAsInput(</div><div class="line">					headVertex,</div><div class="line">					DistributionPattern.ALL_TO_ALL,</div><div class="line">					ResultPartitionType.PIPELINED,</div><div class="line">					<span class="keyword">true</span>);</div><div class="line">		&#125;</div><div class="line"></div><div class="line"><span class="comment">//ResultPartition line244</span></div><div class="line"><span class="comment">//Also for a ResultPartition, when is has first buffer produced, it will notify JobManager to deploy it's consumer tasks</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(Buffer buffer, <span class="keyword">int</span> subpartitionIndex)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="keyword">boolean</span> success = <span class="keyword">false</span>;</div><div class="line"></div><div class="line">		<span class="keyword">try</span> &#123;</div><div class="line">			checkInProduceState();</div><div class="line"></div><div class="line">			<span class="keyword">final</span> ResultSubpartition subpartition = subpartitions[subpartitionIndex];</div><div class="line"></div><div class="line">			<span class="keyword">synchronized</span> (subpartition) &#123;</div><div class="line">				success = subpartition.add(buffer);</div><div class="line"></div><div class="line">				<span class="comment">// Update statistics</span></div><div class="line">				totalNumberOfBuffers++;</div><div class="line">				totalNumberOfBytes += buffer.getSize();</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">finally</span> &#123;</div><div class="line">			<span class="keyword">if</span> (success) &#123;</div><div class="line">				notifyPipelinedConsumers();</div><div class="line">			&#125;</div><div class="line">			<span class="keyword">else</span> &#123;</div><div class="line">				buffer.recycle();</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line"><span class="comment">//ResultPartition line440</span></div><div class="line"><span class="comment">//Only PIPELINED will work here</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">notifyPipelinedConsumers</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="keyword">if</span> (partitionType.isPipelined() &amp;&amp; !hasNotifiedPipelinedConsumers) &#123;</div><div class="line">			partitionConsumableNotifier.notifyPartitionConsumable(jobId, partitionId);</div><div class="line"></div><div class="line">			hasNotifiedPipelinedConsumers = <span class="keyword">true</span>;</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>简单总结：</p>
<ul>
<li>对于 PIPELINED，在 source tasks 部署完成后立马会触发一次下游 consumer tasks 的部署</li>
<li>在生产者 task 产生第一个 buffer 数据的时候也会触发一次 consumer tasks 的部署</li>
</ul>
<p>附一张图解释该流程：</p>
<p><img src="flink-streaming-deploy-flow.png" alt="flink-streaming-deploy-flow.png"></p>
<p>再附上官方的一张经典调度图：</p>
<p><img src="slots_parallelism.svg" alt="slots_parallelism.svg"></p>
<h3 id="Slot的资源隔离"><a href="#Slot的资源隔离" class="headerlink" title="Slot的资源隔离"></a>Slot的资源隔离</h3><p>待更新</p>
]]></content>
    
    <summary type="html">
    
      前面已经介绍了一系列的 flink 任务抽象、网络传输、可靠性机制等细节，有了这些铺垫，终于可以开心的介绍 flink 的任务调度机制了，也是不易
    
    </summary>
    
      <category term="Flink源码" scheme="https://danny0405.github.io/categories/Flink%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="Flink调度" scheme="https://danny0405.github.io/tags/Flink%E8%B0%83%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>flink watermark and checkpoint</title>
    <link href="https://danny0405.github.io/2017/02/09/flink-watermark-checkpoint/"/>
    <id>https://danny0405.github.io/2017/02/09/flink-watermark-checkpoint/</id>
    <published>2017-02-09T05:44:06.000Z</published>
    <updated>2018-01-23T06:12:25.433Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在前面一章 flink 网络栈的讲解中，我们介绍了 Barrier 的改变以及 Barrier 在 InputGate 消费数据的过程中扮演的时间对齐作用，同时，我们介绍了 InputProcessor 负责数据读取，同时会追踪 watermark 时间并分发到下游。这里我们从 InputProcessor 开始讲起</p>
<p>为什么将 Watermark 和 Checkpoint 放在一起介绍，是因为它们在原理上有相似之处：上游节点逐级广播消息给下游节点来完成一次行为</p>
<h2 id="WaterMark"><a href="#WaterMark" class="headerlink" title="WaterMark"></a>WaterMark</h2><h3 id="WaterMark是什么"><a href="#WaterMark是什么" class="headerlink" title="WaterMark是什么"></a>WaterMark是什么</h3><p>Watermark 是协调窗口计算的一种方式，它告诉了算子时间不大于 WaterMark 的消息不应该再被接收【如果出现意味着延迟到达】。WaterMark 从源算子开始 emit，并逐级向下游算子传递，算子需要依据自己的缓存策略在适当的时机将 WaterMark 传递到下游。当源算子关闭时，会发射一个携带 <code>Long.MAX_VALUE</code> 值时间戳的 WaterMark，下游算子接收到之后便知道不会再有消息到达。</p>
<p>Flink 提供三种消息时间特性：EventTime【消息产生的时间】、ProcessingTime【消息处理时间】 和 IngestionTime【消息流入 flink 框架的时间】，WaterMark 只在时间特性 EventTime 和 IngestionTime 起作用，并且 IngestionTime 的时间等同于消息的 ingestion 时间。</p>
<h3 id="WaterMark的协调与分发"><a href="#WaterMark的协调与分发" class="headerlink" title="WaterMark的协调与分发"></a>WaterMark的协调与分发</h3><p>对于 watermark 的协调与分发集中在 InputProcessor 的 processInput 方法中，下面我们来详细分析其逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//StreamInputProcessor line134</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">processInput</span><span class="params">(OneInputStreamOperator&lt;IN, ?&gt; streamOperator, <span class="keyword">final</span> Object lock)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		<span class="keyword">if</span> (isFinished) &#123;</div><div class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">if</span> (numRecordsIn == <span class="keyword">null</span>) &#123;</div><div class="line">			numRecordsIn = streamOperator.getMetricGroup().counter(<span class="string">"numRecordsIn"</span>);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">			<span class="keyword">if</span> (currentRecordDeserializer != <span class="keyword">null</span>) &#123;</div><div class="line">				DeserializationResult result = currentRecordDeserializer.getNextRecord(deserializationDelegate);</div><div class="line"></div><div class="line">				<span class="keyword">if</span> (result.isBufferConsumed()) &#123;</div><div class="line">					currentRecordDeserializer.getCurrentBuffer().recycle();</div><div class="line">					currentRecordDeserializer = <span class="keyword">null</span>;</div><div class="line">				&#125;</div></pre></td></tr></table></figure>
<p>总结其逻辑：</p>
<ul>
<li>如果消费到的消息是一个 WaterMark，获得其对应的 source channel id 并将时间更新进去，同时记录下当前所有 channel 的最小 WaterMark 时间</li>
<li>如果当前最小 WaterMark 时间【所有的 channel 都至少消费到该时间】大于上次发射给下游的 WaterMark 时间，则更新 WaterMark 时间并将其交给算子处理</li>
<li>通常算子在处理【尤其是涉及了窗口计算或者需要时间缓存策略的算子】后会将 WaterMark 继续往下游广播发送</li>
</ul>
<h3 id="WaterMark-的来源"><a href="#WaterMark-的来源" class="headerlink" title="WaterMark 的来源"></a>WaterMark 的来源</h3><p>上面我们提到 WaterMark 最初由源算子负责发射到下游，那么它的生成规则是什么呢？又是如何协调的？</p>
<p><em>我们来看一个源算子的实现便知</em></p>
<p>在第一章 flink 逻辑计划生成，我们了解了 flink 所有的源算子都继承自 SourceFunction 接口，SourceFuntion 定义了管理消息发射环境的接口 SourceContext，SourceContext 的具体实现在 StreamSource 中，一共有三种：NonTimestampContext、AutomaticWatermarkContext、ManualWatermarkContext，我们来逐一分析。</p>
<h4 id="NonTimestampContext"><a href="#NonTimestampContext" class="headerlink" title="NonTimestampContext"></a>NonTimestampContext</h4><p>适用于时间特性：TimeCharacteristic#ProcessingTime，顾名思义，不会 emit 任何 WaterMark</p>
<h4 id="AutomaticWatermarkContext"><a href="#AutomaticWatermarkContext" class="headerlink" title="AutomaticWatermarkContext"></a>AutomaticWatermarkContext</h4><p>自动发射 WaterMark，适用于 TimeCharacteristic#IngestionTime ，也就是源算子的处理时间。flink 起了一个timer task 专门以一定的 interval 间隔发射 WaterMark，一个 Interval 内所有 Record 的发射时间处于上次 emit 的 WaterMark 和下次将要 emit 的 WaterMark 之间，Interval 边界到达后会提升下一个 WaterMark 时间，计算本次的 WaterMark 时间并 emit 出去。</p>
<p>自动 emit watermark 的 interval 默认是 200ms ，这是写死不可配置的，具体见：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//StreamExecutionEnvironment line598</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setStreamTimeCharacteristic</span><span class="params">(TimeCharacteristic characteristic)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.timeCharacteristic = Preconditions.checkNotNull(characteristic);</div><div class="line">		<span class="keyword">if</span> (characteristic == TimeCharacteristic.ProcessingTime) &#123;</div><div class="line">			getConfig().setAutoWatermarkInterval(<span class="number">0</span>);</div><div class="line">		&#125; <span class="keyword">else</span> &#123;</div><div class="line">			getConfig().setAutoWatermarkInterval(<span class="number">200</span>);</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<h4 id="ManualWatermarkContext"><a href="#ManualWatermarkContext" class="headerlink" title="ManualWatermarkContext"></a>ManualWatermarkContext</h4><p>用户自己指定 WaterMark 时间，使用于 TimeCharacteristic#EventTime，用户需要提供依据源消息提取 Watermark 时间的工具 function</p>
<p><em>那么用户有哪些指定时间的逻辑呢？</em></p>
<h5 id="TimestampAssigner"><a href="#TimestampAssigner" class="headerlink" title="TimestampAssigner"></a>TimestampAssigner</h5><p>flink 通过接口 TimestampAssigner 来让用户依据消息的格式自己抽取可能被用于 WaterMark 的 timestamp，它只定义了一个接口：<code>long extractTimestamp(T element, long previousElementTimestamp);</code></p>
<p>而 TimestampAssigner 的两个继承接口 AssignerWithPunctuatedWatermarks 以及 AssignerWithPeriodicWatermarks 定义了另种典型的时间戳生成规则：</p>
<ul>
<li>AssignerWithPunctuatedWatermarks：依据消息中事件元素及自带 timestamp 来驱动 watermark 的递增</li>
<li>AssignerWithPeriodicWatermarks：依据消息中的 timestamp 周期性地驱动 watermark 的递增</li>
</ul>
<p>各个场景可以依据业务的需求去继承和实现</p>
<h2 id="Checkpoint"><a href="#Checkpoint" class="headerlink" title="Checkpoint"></a>Checkpoint</h2><p>CheckPoint 是 flink 保证消息不丢的机制，通过 Barrier 的方式来协调时机，那么什么是 Barrier 呢？</p>
<p>其实前一章介绍 flink 网络栈 的时候已经有介绍在消费端 flink 对于不同的 Barrier 处理，实际上，Barrier 是用来校准 checkpint 的方式。由于对于一个拓扑结构，只有上游算子 checkpoint 完，下游算子的 cehckpoint 才能开始并有意义，同时下游算子的消费速率并不统一【有的 channel 快，有的 channel 慢】，而 Barrier 就是这样一种协调上下游算子的机制。</p>
<p>JobManager 统一通知源算子发射 Barrier 事件，并向下游广播，当下游算子收到这样的事件后，它就知道自己处于两次 checkpoint 之间【一次新的 checkpoint 将被发起】</p>
<p>当下游算子收到了它所有的 InputChannel 的 Barrier 事件后，它便知道上游算子的一次 checkpoint 已完成，自己也可以做 checkpoint 了，完成之后继续将 checkpoint 事件广播到下游算子</p>
<p>在 Exact-once 语义下，消费端会延迟消费并校准不同 channel 的消费速率，这在 flink 网络栈一章有详细介绍！</p>
<h3 id="Checkpoint-的协调与发起"><a href="#Checkpoint-的协调与发起" class="headerlink" title="Checkpoint 的协调与发起"></a>Checkpoint 的协调与发起</h3><p>前面提到过 checkpoint 统一由 JobManager 发起，我们来看相关逻辑：</p>
<h4 id="CheckpointCoordinator"><a href="#CheckpointCoordinator" class="headerlink" title="CheckpointCoordinator"></a>CheckpointCoordinator</h4><p>flink 的 checkpoint 统一由 CheckpointCoordinator 来协调，通过将 checkpoint 命令事件发送给相关的 tasks 【源 tasks】，它发起 checkpoint 并且收集 checkpoint 的 ack 消息。</p>
<h5 id="构造参数"><a href="#构造参数" class="headerlink" title="构造参数"></a>构造参数</h5><p>这里有必要了解一些关键参数，以便我们更加了解 Checkpoint 的细节策略</p>
<ul>
<li>baseInternal：快照的间隔</li>
<li>checkpointTimeout：一次 checkpoint 的超时时间，超时的 checkpoint 会被取消</li>
<li>maxConcurrentCheckpointAttempts：最多可同时存在的 checkpoint 任务，是对于整个 flink job</li>
<li>tasksToTrigger：触发分布式 Checkpoint 的起始 tasks，也就是 source tasks</li>
</ul>
<h5 id="Checkpoint的发起"><a href="#Checkpoint的发起" class="headerlink" title="Checkpoint的发起"></a>Checkpoint的发起</h5><p>前面的章节我们介绍过 ExecutionGraph【flink物理计划抽象】，它有一个核心接口 <code>enableSnapshotCheckpointing</code> ，这个接口在 JobManager 提交作业的时候被执行，具体见<code>JobManager line1238 from method submitJob</code>。这个接口的逻辑总结如下：</p>
<ul>
<li>获取 checkpoint 的发起节点【源节点】，需要 ack 和 commit 的节点【所有节点】</li>
<li>先关闭已有 checkpoint</li>
<li>实例化 CheckpointCoordinator 和它的监听 Akka 系统 CheckpointCoordinatorDeActivator，并将 CheckpointCoordinatorDeActivator 注册为 EecutionGraph 的 listener，当作业的执行状态变为 RUNNING 时，会通知 CheckpointCoordinatorDeActivator 启动 CheckpointCoordinator 的 checkpoint 线程</li>
</ul>
<p><em>那么 CheckpointCoordinator 在收到这样的消息后会怎么处理呢？</em></p>
<p>它会发起一个 timer task，定时执行，并且传入的时间为当前的系统时间，由于 CheckpointCoordinator 全局只有一个，这个时间也是全局递增并且唯一的：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//CheckpointCoordinator line 1020</span></div><div class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">ScheduledTrigger</span> <span class="keyword">extends</span> <span class="title">TimerTask</span> </span>&#123;</div><div class="line"></div><div class="line">		<span class="meta">@Override</span></div><div class="line">		<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">			<span class="keyword">try</span> &#123;</div><div class="line">				triggerCheckpoint(System.currentTimeMillis());</div><div class="line">			&#125;</div><div class="line">			<span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">				LOG.error(<span class="string">"Exception while triggering checkpoint"</span>, e);</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p><em>下面我们来具体分析 checkpoint 的一些核心动作</em></p>
<h6 id="checkpoint-的触发"><a href="#checkpoint-的触发" class="headerlink" title="checkpoint 的触发"></a>checkpoint 的触发</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//CheckpointCoordinator line389</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">triggerCheckpoint</span><span class="params">(<span class="keyword">long</span> timestamp, <span class="keyword">long</span> nextCheckpointId)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		<span class="comment">// make some eager pre-checks</span></div><div class="line">		<span class="keyword">synchronized</span> (lock) &#123;</div><div class="line">			<span class="comment">// abort if the coordinator has been shutdown in the meantime</span></div><div class="line">			<span class="keyword">if</span> (shutdown) &#123;</div><div class="line">				<span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">			&#125;</div><div class="line"><span class="comment">//...</span></div></pre></td></tr></table></figure>
<p>总结逻辑：</p>
<ul>
<li>如果已关闭或优先处理排队请求会总额并发任务超过限制，都会取消此次 checkpoint 的发起</li>
<li>如果最小间隔时间未达到，也会取消此次 checkpoint</li>
<li>check 所有的发起节点【源节点】与其他节点都为 RUNNING 状态后才会发起 checkpoint</li>
<li>发起 checkpoint 并生成一个 PendingCheckpoint 【已经发起但尚未 ack 的 checkpoint】</li>
<li>每个源节点都会发一条消息给自己的 TaskManager 进行 checkpoint</li>
</ul>
<h6 id="取消-CheckPoint-消息的处理"><a href="#取消-CheckPoint-消息的处理" class="headerlink" title="取消 CheckPoint 消息的处理"></a>取消 CheckPoint 消息的处理</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//CheckpointCoordinator line568</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">receiveDeclineMessage</span><span class="params">(DeclineCheckpoint message)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">   <span class="keyword">if</span> (shutdown || message == <span class="keyword">null</span>) &#123;</div><div class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">   &#125;</div><div class="line">   <span class="keyword">if</span> (!job.equals(message.getJob())) &#123;</div><div class="line">      LOG.error(<span class="string">"Received DeclineCheckpoint message for wrong job: &#123;&#125;"</span>, message);</div><div class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">   &#125;</div><div class="line"><span class="comment">//...</span></div></pre></td></tr></table></figure>
<p>总结其逻辑：</p>
<ul>
<li>如果有对应的 PendingCheckpoint ，取消掉并且如果在其之后还有其它 checkpoint 的话，重新发起它们的 checkpoint 任务</li>
</ul>
<h6 id="Ack-Checkpoint-消息的处理"><a href="#Ack-Checkpoint-消息的处理" class="headerlink" title="Ack Checkpoint 消息的处理"></a>Ack Checkpoint 消息的处理</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//CheckpointCoordinator line651</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">receiveAcknowledgeMessage</span><span class="params">(AcknowledgeCheckpoint message)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		<span class="keyword">if</span> (shutdown || message == <span class="keyword">null</span>) &#123;</div><div class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">if</span> (!job.equals(message.getJob())) &#123;</div><div class="line">			LOG.error(<span class="string">"Received AcknowledgeCheckpoint message for wrong job: &#123;&#125;"</span>, message);</div><div class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">		&#125;</div></pre></td></tr></table></figure>
<p>总结其逻辑：</p>
<ul>
<li>通过消息里的 checkpoint id 找到对应的  PendingCheckpoint，记录下 对应的 JobVertex 下某个 ExecutionVertex 的 ack 状态</li>
<li>PendingCheckpoint 里维护了该次 checkpoint 需要 ack 的全部 ExecutionVertex</li>
<li>如果全部 ack 完成，则清除 PendingCheckpoint 里维护的状态数据并将句柄转化给 CompletedCheckpoint 来维护</li>
<li>丢弃过时的 checkpoint 任务，并重新出发新的 checkpoint</li>
<li>如果全部 ack 完成，通知对应的 TaskManager checkpoint 已完成【checkpoint commit 阶段】，然后通过 CompletedCheckpointStore 将 CompletedCheckpoint 序列化并存储，高可用模式下为 ZK 的方式，具体细节见章节：【flink job manager 基本组件】，将来恢复时，将每个节点需要的句柄注入到状态中，之后算子启动时将状态数据附属于 TaskDeploymentDescriptor 之中分发给 TaskManager 去执行</li>
</ul>
<h3 id="Checkpoint-的消息流"><a href="#Checkpoint-的消息流" class="headerlink" title="Checkpoint 的消息流"></a>Checkpoint 的消息流</h3><p>上面我们说到 TaskManager 收到 AbstractCheckpointMessage 消息，并处理，我们来看核心逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//TaskManager line520</span></div><div class="line"><span class="function"><span class="keyword">private</span> def <span class="title">handleCheckpointingMessage</span><span class="params">(actorMessage: AbstractCheckpointMessage)</span>: Unit </span>= &#123;</div><div class="line"></div><div class="line">    actorMessage match &#123;</div><div class="line">      <span class="keyword">case</span> message: TriggerCheckpoint =&gt;</div><div class="line">        val taskExecutionId = message.getTaskExecutionId</div><div class="line">        val checkpointId = message.getCheckpointId</div><div class="line">        val timestamp = message.getTimestamp</div><div class="line"></div><div class="line">        log.debug(s<span class="string">"Receiver TriggerCheckpoint $checkpointId@$timestamp for $taskExecutionId."</span>)</div><div class="line"></div><div class="line"><span class="comment">//...</span></div><div class="line"><span class="comment">//Task.java line927</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">triggerCheckpointBarrier</span><span class="params">(<span class="keyword">final</span> <span class="keyword">long</span> checkpointID, <span class="keyword">final</span> <span class="keyword">long</span> checkpointTimestamp)</span> </span>&#123;</div><div class="line">		AbstractInvokable invokable = <span class="keyword">this</span>.invokable;</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (executionState == ExecutionState.RUNNING &amp;&amp; invokable != <span class="keyword">null</span>) &#123;</div><div class="line">			<span class="keyword">if</span> (invokable <span class="keyword">instanceof</span> StatefulTask) &#123;</div><div class="line"></div><div class="line">				<span class="comment">// build a local closure</span></div><div class="line">				<span class="keyword">final</span> StatefulTask&lt;?&gt; statefulTask = (StatefulTask&lt;?&gt;) invokable;</div><div class="line">				<span class="keyword">final</span> String taskName = taskNameWithSubtask;       </div><div class="line"></div><div class="line"><span class="comment">//...</span></div><div class="line"><span class="comment">//StreamTask line577</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">performCheckpoint</span><span class="params">(<span class="keyword">final</span> <span class="keyword">long</span> checkpointId, <span class="keyword">final</span> <span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		LOG.debug(<span class="string">"Starting checkpoint &#123;&#125; on task &#123;&#125;"</span>, checkpointId, getName());</div><div class="line"></div><div class="line">		<span class="keyword">synchronized</span> (lock) &#123;</div><div class="line">			<span class="keyword">if</span> (isRunning) &#123;</div></pre></td></tr></table></figure>
<p>总结其逻辑：</p>
<ul>
<li>先是通过 TaskManager 进行消息路由，对于 TriggerCheckpoint 消息，会路由给相应的 Task 做处理</li>
<li>Task 会起一个异步 task 进行 checkpoint，内部是调用 StreamTask 的 performCheckpoint 方法</li>
<li>performCheckpoint 内部首先先将此次 checkpoint 的 barrier 广播到下游，以便让下游快速 checkpoint</li>
<li>后执行具体的 checkpoint，将状态持久化，目前支持的持久化方式有：FileSystem、Memory、RocksDB，成功后通知 JobManager 进行 ack，否则取消此次 checkpoint</li>
<li>如果是 ack 消息，依据具体情况通知对应的 KVState</li>
</ul>
<p>附一张图描述交互过程：</p>
<p><img src="checkpoint-event-flow.png" alt="checkpoint-event-flow.png"></p>
<h3 id="Checkpoint-的存储和恢复"><a href="#Checkpoint-的存储和恢复" class="headerlink" title="Checkpoint 的存储和恢复"></a>Checkpoint 的存储和恢复</h3><p>Checkpoint 的存储和恢复均是通过 AbstractStateBackend 来完成，AbstractStateBackend 有三个实现类，FsStateBackend 是通过 HDFS 来存储 checkpoint 状态，继承关系如下：</p>
<p><img src="state-backend-extend.png" alt="state-backend-extend.png"></p>
<p><em>我们来看最常见的一种 FsStateBackend</em>，AbstractStateBackend 内部通过 State 来管理状态数据，依据状态数据的不同特性，状态分为三种：</p>
<ul>
<li>ValueState ：最简单的状态，一个 key 一个单值 value，可以跟更新和删除</li>
<li>ListState：一个 key 对应一个 value list</li>
<li>ReducingState：一个 key 对应的 value 可以进行 reduce 操作</li>
<li>FoldingState：一个key，后续添加的值都会通过 folding 函数附加到第一个值上</li>
</ul>
<p>AbstractStateBackend 内部通过 KvState 接口来管理用户自定义的 kv 数据，我们来看 FsValueState 的继承关系：</p>
<p><img src="value-state-extend.png" alt="value-state-extend.png"></p>
<p>那么如何获取这些 State 呢？flink 抽象了另一套接口：StateDescriptor 来获取 State，通过绑定特定的 StateBackend 来获取。这样一层抽象，解耦了 State 的类型和底层的具体的存储实现。我们来看 StateDescriptor 的继承关系：</p>
<p><img src="state-describtor.png" alt="state-describtor.png"></p>
<p>那么这些抽象是如何协调工作的呢？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//KvState 的初始化和获取</span></div><div class="line"><span class="comment">//AbstractKeyedCEPPatternOperator line93</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		<span class="keyword">if</span> (keys == <span class="keyword">null</span>) &#123;</div><div class="line">			keys = <span class="keyword">new</span> HashSet&lt;&gt;();</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (nfaOperatorState == <span class="keyword">null</span>) &#123;</div><div class="line">			nfaOperatorState = getPartitionedState(</div><div class="line">					<span class="keyword">new</span> ValueStateDescriptor&lt;NFA&lt;IN&gt;&gt;(</div><div class="line">						NFA_OPERATOR_STATE_NAME,</div><div class="line">						<span class="keyword">new</span> NFA.Serializer&lt;IN&gt;(),</div><div class="line">						<span class="keyword">null</span>));</div><div class="line">		&#125;</div><div class="line"></div><div class="line"><span class="comment">//AbstractStreamOperator line273</span></div><div class="line"><span class="keyword">protected</span> &lt;S extends State&gt; <span class="function">S <span class="title">getPartitionedState</span><span class="params">(StateDescriptor&lt;S, ?&gt; stateDescriptor)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		<span class="keyword">return</span> getStateBackend().getPartitionedState(<span class="keyword">null</span>, VoidSerializer.INSTANCE, stateDescriptor);</div><div class="line">	&#125;</div><div class="line"></div><div class="line"><span class="comment">//AbstractStateBackend line205</span></div><div class="line"><span class="comment">//具体的 kvState 由子类具体实现来决定</span></div><div class="line"><span class="keyword">public</span> &lt;N, S extends State&gt; <span class="function">S <span class="title">getPartitionedState</span><span class="params">(<span class="keyword">final</span> N namespace, <span class="keyword">final</span> TypeSerializer&lt;N&gt; namespaceSerializer, <span class="keyword">final</span> StateDescriptor&lt;S, ?&gt; stateDescriptor)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (keySerializer == <span class="keyword">null</span>) &#123;</div><div class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"State key serializer has not been configured in the config. "</span> +</div><div class="line">					<span class="string">"This operation cannot use partitioned state."</span>);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (!stateDescriptor.isSerializerInitialized()) &#123;</div><div class="line">			stateDescriptor.initializeSerializerUnlessSet(<span class="keyword">new</span> ExecutionConfig());</div><div class="line">		&#125;</div><div class="line"><span class="comment">//获取 KvState 后，用户经过一番更新...下面是快照的过程</span></div><div class="line"><span class="comment">// StateBackend 的创建</span></div><div class="line"><span class="comment">//AbstractStreamOperator line114</span></div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">			TypeSerializer&lt;Object&gt; keySerializer = config.getStateKeySerializer(getUserCodeClassloader());</div><div class="line">			<span class="comment">// if the keySerializer is null we still need to create the state backend</span></div><div class="line">			<span class="comment">// for the non-partitioned state features it provides, such as the state output streams</span></div><div class="line">			String operatorIdentifier = getClass().getSimpleName() + <span class="string">"_"</span> + config.getVertexID() + <span class="string">"_"</span> + runtimeContext.getIndexOfThisSubtask();</div><div class="line">			stateBackend = container.createStateBackend(operatorIdentifier, keySerializer);</div><div class="line">		&#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Could not initialize state backend. "</span>, e);</div><div class="line">		&#125;</div><div class="line"><span class="comment">//StreamTask line102</span></div><div class="line"><span class="function"><span class="keyword">public</span> AbstractStateBackend <span class="title">createStateBackend</span><span class="params">(String operatorIdentifier, TypeSerializer&lt;?&gt; keySerializer)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		AbstractStateBackend stateBackend = configuration.getStateBackend(userClassLoader);</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (stateBackend != <span class="keyword">null</span>) &#123;</div><div class="line">			<span class="comment">// backend has been configured on the environment</span></div><div class="line">			LOG.info(<span class="string">"Using user-defined state backend: "</span> + stateBackend);</div><div class="line">		&#125;</div><div class="line"><span class="comment">//快照入口</span></div><div class="line"><span class="comment">//AbstractStreamOperator line179</span></div><div class="line"><span class="function"><span class="keyword">public</span> StreamTaskState <span class="title">snapshotOperatorState</span><span class="params">(<span class="keyword">long</span> checkpointId, <span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		<span class="comment">// here, we deal with key/value state snapshots</span></div><div class="line"></div><div class="line">		StreamTaskState state = <span class="keyword">new</span> StreamTaskState();</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (stateBackend != <span class="keyword">null</span>) &#123;</div><div class="line">			HashMap&lt;String, KvStateSnapshot&lt;?, ?, ?, ?, ?&gt;&gt; partitionedSnapshots =</div><div class="line">				stateBackend.snapshotPartitionedState(checkpointId, timestamp);</div><div class="line">			<span class="keyword">if</span> (partitionedSnapshots != <span class="keyword">null</span>) &#123;</div><div class="line">				state.setKvStates(partitionedSnapshots);</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">return</span> state;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>上面的快照之行结束后，用户会获取 KvStateSnapshot 抽象，对于 FsState 来说，起内部封装了文件句柄以及序列化元数据等信息，同时提供了恢复快照的接口，其抽象关系如下：</p>
<p><img src="fs-snapshort-extend.png" alt="fs-snapshort-extend.png"></p>
<p>flink  进一步将每个 task 的每个 operator 快照后获取的 KvStateSnapshot 封装成 StreamTaskState，并最终获取一个 StreamTaskState List【对应一个 task 的一组 operators】，分装成 StreamTaskStateList，随后通知 JobManager 的 CheckpointCoordinator：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//RuntimeEnvironment line260</span></div><div class="line">AcknowledgeCheckpoint message = <span class="keyword">new</span> AcknowledgeCheckpoint(</div><div class="line">				jobId,</div><div class="line">				executionId,</div><div class="line">				checkpointId,</div><div class="line">				serializedState,</div><div class="line">				stateSize);</div><div class="line"></div><div class="line">		jobManager.tell(message);</div></pre></td></tr></table></figure>
<p>JobManager  再将这些句柄的数据再快照到本地和zk，具体见 JobManager 基本组件。恢复的过程是逆向的，暂时就不分析了，有耐心的用户可以自行查看源码!</p>
]]></content>
    
    <summary type="html">
    
      在前面一章 flink 网络栈的讲解中，我们介绍了 Barrier 的改变以及 Barrier 在 InputGate 消费数据的过程中扮演的时间对齐作用，同时，我们介绍了 InputProcessor 负责数据读取，同时会追踪 watermark 时间并分发到下游。这里我们从 InputProcessor 开始讲起，接着会介绍 checkpoint
    
    </summary>
    
      <category term="Flink源码" scheme="https://danny0405.github.io/categories/Flink%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="Flink内部实现" scheme="https://danny0405.github.io/tags/Flink%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>flink 网络栈</title>
    <link href="https://danny0405.github.io/2017/02/09/flink%E7%BD%91%E7%BB%9C%E6%A0%88/"/>
    <id>https://danny0405.github.io/2017/02/09/flink网络栈/</id>
    <published>2017-02-09T05:38:48.000Z</published>
    <updated>2018-01-23T06:12:05.803Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本章节主要介绍 flink 的网络交互，包括每个 task 的输入输出管理，内存分配和释放等，因为涉及到内存申请，这里会介绍 flink 的内存管理</p>
<p><em>我们会从 flink 网络相关的核心抽象开始介绍</em></p>
<h2 id="IntermediateResult"><a href="#IntermediateResult" class="headerlink" title="IntermediateResult"></a>IntermediateResult</h2><p>代表一个 Job Vertex 的中间执行结果，由于同一个 Job vertex 可能有多个线程并发执行，这里的 IntermediateResult 对应一个 Job Edge 的输出下游结果集，一个 IntermediateResult 包含多个 IntermediateResultPartition，一个 IntermediateResultPartition 对应一个并行任务 ExecutionVertex 的输出结果，如下图所示：</p>
<p><img src="intermediate-result.png" alt="intermediate-result.png"> <em>对应关系如下</em>：</p>
<ul>
<li>IntermediateDataSet [StreamGraph] &lt;-&gt; IntermediateResult[JobGraph]  代表一个 dataset</li>
<li>IntermediateResultPartition [ExecutionGraph] 代表一个并行任务的输出 partition </li>
<li>ResultPartition 代表一个  IntermediateResultPartition 的生产者 ExecutionVertex 的输出 副本，可输出多个副本</li>
<li>一个 ResultPartition 可能包含多个 ResultSubPartition【依据下游消费 task 的数目】</li>
</ul>
<p>下面介绍数据的消费*</p>
<h2 id="InputGate"><a href="#InputGate" class="headerlink" title="InputGate"></a>InputGate</h2><p>InputGate 是 flink 关于 task 的一个输入源的抽象，一个 InputGate 代表一个上游数据源，对应<strong>一个</strong>上游中间结果的一个或多个 partition</p>
<p>flink 依据中间结果的 producer 【<em>生产task</em>】的并发度来生成相应个数的 partition，每个 producer task 生产一个 partition，为了优化并发读写，flink 依据中间结果的消费者 task 的并发度进一步将每个 partition 划分为多个 subPartition，假设有如下这样的一个简单地生产消费模型：</p>
<p><img src="sub-partition.png" alt="sub-partition.png"></p>
<p>上图的模型包含了一个生产者任务 map() 和消费者任务 reduce()，两者的并行度都是 2，由于有两个 producer task，这时候 flink 会将中间结果划分为两个 partition，同时由于有 2 个 consumer task，每个 partition被进一步划分为两个 sub partiton</p>
<p>对于每个 consumer task，flink 会维护一个消费的中间数据集和 task 本身的对应关系，这就是 InputGate</p>
<p>每个 InputGate 会维护消费一个中间结果的每个partition中的一个 subPartition，这样的一个 subPartition 的消费需要建立一个 InputChannel</p>
<p>ps: 目前 flink 的一个中间结果 partition 仅支持一个 consumer 节点【JobVertex】</p>
<h3 id="InputGate的创建"><a href="#InputGate的创建" class="headerlink" title="InputGate的创建"></a>InputGate的创建</h3><p>在上一节 Flink 算子的声明周期一节，我们介绍了 Flink 的 Instance 直接执行类 Task.java，在其构造器中有这样一段逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Consumed intermediate result partitions</span></div><div class="line">		<span class="keyword">this</span>.inputGates = <span class="keyword">new</span> SingleInputGate[consumedPartitions.size()];</div><div class="line">		<span class="keyword">this</span>.inputGatesById = <span class="keyword">new</span> HashMap&lt;IntermediateDataSetID, SingleInputGate&gt;();</div><div class="line"></div><div class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>.inputGates.length; i++) &#123;</div><div class="line">			SingleInputGate gate = SingleInputGate.create(</div><div class="line">					taskNameWithSubtaskAndId, jobId, executionId, consumedPartitions.get(i), networkEnvironment,</div><div class="line">					metricGroup.getIOMetricGroup());</div><div class="line"></div><div class="line">			<span class="keyword">this</span>.inputGates[i] = gate;</div><div class="line">			inputGatesById.put(gate.getConsumedResultId(), gate);</div><div class="line">		&#125;</div></pre></td></tr></table></figure>
<p>这里直接用的 SingleInputGate，而 SingleInputGate 有是如何去建立和上游 ResultSubPartition 的连接关系的呢？SingleInputGate 在初始化的时候利用了关键的的参数 InputGateDeploymentDescriptor，其中包含了 partition 的一些关键信息，具体见 <code>SingleInputGate line 502</code>，这里列一下初始化过程：</p>
<ul>
<li><p>从 InputGateDeploymentDescriptor 中获取 IntermediateDataSetID、consumedSubpartitionIndex</p>
<p>、InputChannelDeploymentDescriptor</p>
</li>
<li><p>IntermediateDataSetID 告知了消费的数据集，consumedSubpartitionIndex 告知了每个 ResultPartition 要消费的 ResultSubPartition 的索引，InputChannelDeploymentDescriptor 中告知了每个要消费的 ResultPartition 的 id 和 location 信息</p>
</li>
</ul>
<p>这样 InputGate 便知道从哪里消费 partition 以及 partition 的连接信息【本地 or 远程】</p>
<h3 id="注册-BufferPool"><a href="#注册-BufferPool" class="headerlink" title="注册 BufferPool"></a>注册 BufferPool</h3><p>在 NetworkEnvironment 注册 Task 的时候会为每个 task 的每个 InputGate 申请一个 BufferPool：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//NetworkEnvironment line323</span></div><div class="line"><span class="keyword">for</span> (SingleInputGate gate : inputGates) &#123;</div><div class="line">				BufferPool bufferPool = <span class="keyword">null</span>;</div><div class="line"></div><div class="line">				<span class="keyword">try</span> &#123;</div><div class="line">					bufferPool = networkBufferPool.createBufferPool(gate.getNumberOfInputChannels(), <span class="keyword">false</span>);</div><div class="line">					gate.setBufferPool(bufferPool);</div><div class="line">				&#125;</div><div class="line">				<span class="keyword">catch</span> (Throwable t) &#123;</div><div class="line">					<span class="keyword">if</span> (bufferPool != <span class="keyword">null</span>) &#123;</div><div class="line">						bufferPool.lazyDestroy();</div><div class="line">					&#125;</div></pre></td></tr></table></figure>
<p>BufferPool 的最小大小为 input channel 的个数！</p>
<h3 id="InputChannel"><a href="#InputChannel" class="headerlink" title="InputChannel"></a>InputChannel</h3><p>和上游的一个 ResultSubPartition 的连接叫做一个 InputChannel，InputChannel 一共分为三种: LocalInputChannel、RemoteInputChannel、UnknownInputChannel，分别表示一个本地连接，远程连接以及需要运行时确立的连接</p>
<h3 id="InputGateDeploymentDescriptor"><a href="#InputGateDeploymentDescriptor" class="headerlink" title="InputGateDeploymentDescriptor"></a>InputGateDeploymentDescriptor</h3><p>上面我们提到，SingleInputGate 在初始化的时候用到了关键信息主要由 InputGateDeploymentDescriptor 来提供，那么 InputGateDeploymentDescriptor 是如何生成的呢？规则是什么？</p>
<p>在 Task 实例化的时候有一个关键的描述对象参数：TaskDeploymentDescriptor，其中包含了 task 运行时需要的 jar 包路径，配置等重要信息，其中一个关键信息便是：InputGateDeploymentDescriptor，而 Task 是 TaskManager 负责启动的，TaskManager 接收了 JobManager 的 RPC 命令，提交作业，具体的生成首先会牵涉到 Flink 的调度，这里会简单介绍下，细节会另开一节专门介绍</p>
<p>前面章节中，我们介绍运行时，一个并发 Task 被抽象为 ExecutionVertex，而一次执行被抽象为 Execution，在 Execution 中有负责获取资源和调度的接口：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Execution.java line265</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">scheduleForExecution</span><span class="params">(Scheduler scheduler, <span class="keyword">boolean</span> queued)</span> <span class="keyword">throws</span> NoResourceAvailableException</span></div></pre></td></tr></table></figure>
<p>Execution 通过 Flink 的核心调度器 [ 目前也是唯一一个 ] 先获取一个 Slot [ SimpleSlot/ flink 对资源的抽象，在调度章节会详细介绍 ]，后将 Task 部署到该 Slot 上，而 TaskDeploymentDescriptor 就在部署前被创建：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Execution.java line 370</span></div><div class="line"><span class="keyword">final</span> TaskDeploymentDescriptor deployment = vertex.createDeploymentDescriptor(</div><div class="line">	attemptId,</div><div class="line">	slot,</div><div class="line">	operatorState,</div><div class="line">	operatorKvState,</div><div class="line">	attemptNumber);</div></pre></td></tr></table></figure>
<p>具体的规则见 ExecutionVertex <code>line 636: createDeploymentDescriptor</code>，这里简单分析下：</p>
<ul>
<li>依据要输出的 IntermediateResultPartition 生成对应的 ResultPartition 实例</li>
<li>对每个上游的 JobVertex 生成一个 InputChannelDeploymentDescriptor 数组用来实例化一个 InputGateDeploymentDescriptor</li>
<li>获取额外配置</li>
</ul>
<h4 id="InputGateDeploymentDescriptor的生成规则"><a href="#InputGateDeploymentDescriptor的生成规则" class="headerlink" title="InputGateDeploymentDescriptor的生成规则"></a>InputGateDeploymentDescriptor的生成规则</h4><ul>
<li>InputGateDeploymentDescriptor line 87</li>
<li>对于每个上游输入边 ExecutionEdge 获取其生产者 ExecutionVertex</li>
<li>获取 ExecutionVertex 的执行状态，如果是 RUNNING 或 FINISHED，判断生产者的 slot 和本 task 即将部署的 slot 是否在一个 Instance，生成相应的 Location 抽象： ResultPartitionLocation</li>
<li>如果不是  RUNNING 或 FINISHED 状态，生成未知 Location</li>
<li>一个 ExecutionEdge 实例化一个 InputGateDeploymentDescriptor ，一个 InputGateDeploymentDescripto 会同时消费上游每个生产者 ExecutionVertex 的固定 index 的 sub partition</li>
</ul>
<h2 id="InputProcessor"><a href="#InputProcessor" class="headerlink" title="InputProcessor"></a>InputProcessor</h2><p>对于每个 task，flink 都会生成一个 InputProcessor，具体就是 StreamIputProcessor。StreamIputProcessor 干了两件事情：</p>
<ul>
<li>消息解序列化并交由算子处理</li>
<li>追踪 Watermark Event 并分发时间对齐事件</li>
</ul>
<p><em>时间 对齐会专门起一章讲解，这里只介绍 消息序列化 ^_^</em></p>
<p>Flink 将消息的序列化抽象为两个模块：</p>
<ul>
<li>内存申请和流数据读取：由 SpillingAdaptiveSpanningRecordDeserializer 负责，从名字就可以大致了解其会动态吐磁盘【当内存不够的时候】</li>
<li>解序列化：由 NonReusingDeserializationDelegate 负责</li>
</ul>
<p>NonReusingDeserializationDelegate 包含一个 serializer，依据是否需要发射水印区分为 MultiplexingStreamRecordSerializer 和 StreamRecordSerializer，这两个 serializer 只是在 TypeSerializer 上做了一层封装，这里就不做介绍了【TypeSerializer 是类型系统的重要组成，如果有时间的话我会单独开一章节介绍 flink 的类型系统】</p>
<p>这里重点讲一下流数据的读取部分，也就是  SpillingAdaptiveSpanningRecordDeserializer</p>
<h3 id="SpillingAdaptiveSpanningRecordDeserializer"><a href="#SpillingAdaptiveSpanningRecordDeserializer" class="headerlink" title="SpillingAdaptiveSpanningRecordDeserializer"></a>SpillingAdaptiveSpanningRecordDeserializer</h3><p><em>每个 channel 都有一个 SpillingAdaptiveSpanningRecordDeserializer</em></p>
<p>前面提到，SpillingAdaptiveSpanningRecordDeserializer 主要负责流数据的读取，同时通过 NonReusingDeserializationDelegate 来解序列化，从而获得可以发送给算子处理的 StreamElement。SpillingAdaptiveSpanningRecordDeserializer 内部有两个实现类：NonSpanningWrapper 和 SpanningWrapper，前者将数据存储进内存，后者存储前者存不下的部分内存数据以及将超量数据吐到磁盘。我们来解释下其核心方法：</p>
<h4 id="添加-Buffer"><a href="#添加-Buffer" class="headerlink" title="添加 Buffer"></a>添加 Buffer</h4><p>添加 Buffer 会首先 check spanningWrapper 中是否已有数据，如果有，说明 nonSpanningWrapper 中的数据已满，会继续走 spanningWrapper 添加数据，否则走 nonSpanningWrapper 添加数据。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setNextMemorySegment</span><span class="params">(MemorySegment segment, <span class="keyword">int</span> numBytes)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="comment">// check if some spanning record deserialization is pending</span></div><div class="line">		<span class="keyword">if</span> (<span class="keyword">this</span>.spanningWrapper.getNumGatheredBytes() &gt; <span class="number">0</span>) &#123;</div><div class="line">			<span class="keyword">this</span>.spanningWrapper.addNextChunkFromMemorySegment(segment, numBytes);</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">else</span> &#123;</div><div class="line">			<span class="keyword">this</span>.nonSpanningWrapper.initializeFromMemorySegment(segment, <span class="number">0</span>, numBytes);</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<h4 id="读取-Record"><a href="#读取-Record" class="headerlink" title="读取 Record"></a>读取 Record</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//SpillingAdaptiveSpanningRecordDeserializer line98</span></div><div class="line"><span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> DeserializationResult <span class="title">getNextRecord</span><span class="params">(T target)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="comment">// always check the non-spanning wrapper first.</span></div><div class="line">		<span class="comment">// this should be the majority of the cases for small records</span></div><div class="line">		<span class="comment">// for large records, this portion of the work is very small in comparison anyways</span></div><div class="line"></div><div class="line">		<span class="keyword">int</span> nonSpanningRemaining = <span class="keyword">this</span>.nonSpanningWrapper.remaining();</div><div class="line"></div><div class="line">		<span class="comment">// check if we can get a full length;</span></div><div class="line">		<span class="keyword">if</span> (nonSpanningRemaining &gt;= <span class="number">4</span>) &#123;</div><div class="line">			<span class="keyword">int</span> len = <span class="keyword">this</span>.nonSpanningWrapper.readInt();</div></pre></td></tr></table></figure>
<p>简单总结：</p>
<ul>
<li>先判断 nonSpanningWrapper 中有没有记录，如果有优先读取并返回解序列化的类型，其中有两个特例，一是 Record 长度过大，超出了 nonSpanningWrapper 中的 Buffer 的剩余内存，这时候将 nonSpanningWrapper 的数据托管给 spanningWrapper，二是 record 的长度数据不全，这时候也是采取托管给 spanningWrapper 的策略</li>
<li>如果 nonSpanningWrapper  没有记录，判断 spanningWrapper 是否有完全记录，有的话读取，并将 spanningWrapper 中的不完全 buffer 托管给 nonSpanningWrapper</li>
<li>除了个别超大的 record ，对于流式计算来说，大部分走的都应该是 nonSpanningWrapper，之所以设计了 spanningWrapper，主要是为了适应超大的 Record</li>
</ul>
<h3 id="处理输入"><a href="#处理输入" class="headerlink" title="处理输入"></a>处理输入</h3><p>我们回来 InputProcessor</p>
<p>上面提到了 nonSpanningWrapper 或 spanningWrapper 都是从 buffer 中读取数据的，那么这个 buffer 是如何写入的呢？</p>
<p>答案是通过 BarrierHandler 写入</p>
<p>这里涉及到了 flink 的 checkpoint 机制，会在下一节详细介绍，这里只介绍 Buffer 内存的获取</p>
<p>处理输入的核心逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//StreamInputProcessor line134</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">processInput</span><span class="params">(OneInputStreamOperator&lt;IN, ?&gt; streamOperator, <span class="keyword">final</span> Object lock)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		<span class="keyword">if</span> (isFinished) &#123;</div><div class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">if</span> (numRecordsIn == <span class="keyword">null</span>) &#123;</div><div class="line">			numRecordsIn = streamOperator.getMetricGroup().counter(<span class="string">"numRecordsIn"</span>);</div><div class="line">		&#125;</div></pre></td></tr></table></figure>
<p>这里简要概括：</p>
<ul>
<li>第一次进方法的时候，currentRecordDeserializer 为空，通过 barrierHandler 来协调当前读取的 channel 索引和获取数据流 Buffer</li>
<li>以后 currentRecordDeserializer 的消费以 Buffer 为单位，Buffer 消费完会转换下一个 channel 继续消费</li>
</ul>
<p><em>这里的关键是 barrierHandler BufferOrEvent 事件的获取，我们向上追溯</em></p>
<p>依据消费语义的不同，barrierHandler 有两种：</p>
<ul>
<li>BarrierBuffer 对应 exactly_once 语义【有 checkpoint】</li>
<li>BarrierTracker 对应 at_least_once 语义</li>
</ul>
<p><em>我们先来看 BarrierBuffer</em></p>
<h4 id="BarrierBuffer"><a href="#BarrierBuffer" class="headerlink" title="BarrierBuffer"></a>BarrierBuffer</h4><h5 id="BufferSpiller"><a href="#BufferSpiller" class="headerlink" title="BufferSpiller"></a>BufferSpiller</h5><p>BufferSpiller 消费 buffer 和 event 事件流并吐到磁盘文件，在写一定数量的流数据后，可以执行 rollOver 操作：将磁盘数据以 sequence 的方式暴露出来，同时创建新的磁盘文件以备下次读写。由于读写操作间隔很短，文件本身也不是很大，大部分读写操作都会在 ms 级别完成。</p>
<h5 id="获取-BufferOrEvent"><a href="#获取-BufferOrEvent" class="headerlink" title="获取 BufferOrEvent"></a>获取 BufferOrEvent</h5><p><em>以下简称 BufferOrEvent 为 boe</em></p>
<p>核心逻辑在 getNextNonBlocked方法中:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//BarrierBuffer line103</span></div><div class="line"><span class="function"><span class="keyword">public</span> BufferOrEvent <span class="title">getNextNonBlocked</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">		<span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">			<span class="comment">// process buffered BufferOrEvents before grabbing new ones</span></div><div class="line">			BufferOrEvent next;</div><div class="line">			<span class="keyword">if</span> (currentBuffered == <span class="keyword">null</span>) &#123;</div><div class="line">				next = inputGate.getNextBufferOrEvent();</div><div class="line">			&#125;</div></pre></td></tr></table></figure>
<p>简单总结：</p>
<ul>
<li>如果当前没有堆积的 boe，直接从 InputGate 中获取，否则从缓存中获取【通过BufferSpiller缓存的数据】</li>
<li>如果是从堆积中获取 boe，并且来自一个被 block 的 channel，再次将 boe 通过 BufferSpiller 写入缓存</li>
<li>如果是该 boe 是消息 Buffer，返回；如果是 Checkpoint Event，处理 Barrier 事件</li>
<li>如果获取到某个 channel 的 Partition 消费结束事件或获取不到消息，取消 block 所有的 channel，并继续读取缓存中的数据，如果最后缓存中的数据也消费完了，返回 null</li>
</ul>
<p><em>那么处理 barrier 的逻辑是怎样的呢？</em></p>
<h5 id="处理-Barrier"><a href="#处理-Barrier" class="headerlink" title="处理 Barrier"></a>处理 Barrier</h5><p>处理 barrier 的核心逻辑在 processBarrier 方法中:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//BarrierBuffer line161</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">processBarrier</span><span class="params">(CheckpointBarrier receivedBarrier, <span class="keyword">int</span> channelIndex)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="keyword">final</span> <span class="keyword">long</span> barrierId = receivedBarrier.getId();</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (numBarriersReceived &gt; <span class="number">0</span>) &#123;</div><div class="line">			<span class="comment">// subsequent barrier of a checkpoint.</span></div><div class="line">			<span class="keyword">if</span> (barrierId == currentCheckpointId) &#123;</div><div class="line">				<span class="comment">// regular case</span></div><div class="line">				onBarrier(channelIndex);</div><div class="line">			&#125;</div></pre></td></tr></table></figure>
<p>简单总结：</p>
<ul>
<li>如果当前的 Barrier 事件的 id 与当前 checkpointId 相等，则 block 该 boe 的源 channel，并增加 numBarriersReceived 计数</li>
<li>如果 Barrier 事件的 id 大于 checkpointId，取消 block 所有的 channels，并更新当前 checkpointId 为该 id 并且阻断对应的 channel</li>
<li>如果 numBarriersReceived 加上 已经关闭的 channel 数等于 channel 的总数，进行 checkpoint，并取消 block 所有的 channel</li>
</ul>
<p>这里补张图说明数据的消费过程:</p>
<p><img src="flink-network-dataflow.png" alt="flink-network-dataflow.png"></p>
<p>在一轮 barrier 过程中，flink 接收每个 channel 的 barrier event，获取其 barrier id 与此轮之前最大的 id：checkpoint id 作比较：如果相等，则 block 对应的 channel，被 block 的 channel 在此轮中的数据会通过 BufferSpiller 吐到磁盘【大部分情况是 page cache】；如果大于 checkpoint id，会提升 checkpoint id 为此 id，并取消 block 所有的 channel，直接进入下一轮 barrier；如果小于，直接丢弃此事件；如果此轮所有的 channel 都发送了一致的 id，则以此 id 进行 checkpoint，并取消所有的 channel block</p>
<p>值得注意的是，每次取消所有的 channel block 都会将 BufferSpiller 中的数据暴露成 buffer sequence 并加入队列中，下次获取记录时会优先取 sequence 中的数据</p>
<p>列举几种典型的数据流向：</p>
<ul>
<li>input-gate -&gt; operator：消费顺畅，一轮无任何阻塞</li>
<li>input-gate -&gt; BufferSpiller -&gt; current-queue -&gt; operator：相对于其它 channel，消费过快，此轮被吐到磁盘，下一轮再消费</li>
<li>current-queue -&gt; operator：相对于其它 channel，消费过慢，被吐到磁盘，下一轮再消费</li>
</ul>
<p><em>下面介绍数据的生产</em></p>
<h2 id="RecordWriterOutput"><a href="#RecordWriterOutput" class="headerlink" title="RecordWriterOutput"></a>RecordWriterOutput</h2><p>在 flink 算子的声明周期一节我们介绍过：算子通过 RecordWriterOutput 将处理过的 record 写出去，下面我们来分析其行为</p>
<p>RecordWriterOutput 实现了接口 Output，具备以下功能接口：</p>
<ul>
<li>collect：发射 record</li>
<li>emitWatermark：广播 watermark 到每个输出 channel</li>
<li>broadcastEvent：广播 Barrier Event 到每个输出 channel</li>
</ul>
<p>它的内部维护了一个序列化器和真正的 RecordWriter【这里是 StreamRecordWriter】，我们来稍微分析下它们</p>
<h3 id="SpanningRecordSerializer"><a href="#SpanningRecordSerializer" class="headerlink" title="SpanningRecordSerializer"></a>SpanningRecordSerializer</h3><p>实现比较简单，将输入的 record 序列化并连同长度信息一同写入内存 Buffer</p>
<h3 id="RecordWriter"><a href="#RecordWriter" class="headerlink" title="RecordWriter"></a>RecordWriter</h3><p>recordWriter 维护了运行时的 ResultPartitionWriter【真正写 ResultSubPartition】，将 record 序列化进内存 Buffer，来看它的构造器：</p>
<ul>
<li>传入 ResultPartitionWriter</li>
<li>传入 channelSelector【不同的 partition 方式有不同的 selector，如 forward、shuffle 等】</li>
<li>为每一个写 channel 实例化一个 SpanningRecordSerializer</li>
</ul>
<p><em>下面介绍其核心接口</em></p>
<h4 id="发射-record"><a href="#发射-record" class="headerlink" title="发射 record"></a>发射 record</h4><p>对应方法 emit，核心逻辑概括：</p>
<ul>
<li>通过  channelSelector 选择要输出的 channel id</li>
<li>找到对应的 SpanningRecordSerializer 并通过其序列化</li>
<li>有三种情况：record 小于 Buffer 大小、record 等于 Buffer 大小、record 大于【准确的说应该加上长度信息的4个字节】，分别对应三种不同的序列化返回结果，这里有一个细节需要特别注意，写 record 的 Buffer 是写死的 128 bytes 长度，长度不够时会 resize 其大小，所以消息是不会丢失的</li>
<li>如果以上的前两种情况满足意味着消息被完整序列化并写入内存 Buffer，此时通过 ResultPartitionWriter 将包含序列化数据的 Buffer 写出去</li>
<li>同时为 SpanningRecordSerializer 向 NetworkeEnvironment 申请的 BufferPool 再次申请新 Buffer【见 TaskManager 基本组件】</li>
</ul>
<h6 id="生产端反压"><a href="#生产端反压" class="headerlink" title="生产端反压"></a>生产端反压</h6><p>细心的童鞋会发现，在每个发送的接口里，用来解析序列化消息的内存 Buffer 申请都走了这样一个逻辑 <code>writer.getBufferProvider().requestBufferBlocking()</code></p>
<p>这段逻辑是阻塞操作，对应每个 ResultPartition 来说，其 BufferPool 是有限大小的，当内存不足时会阻塞，并停止继续解析消息！</p>
<h4 id="广播-Barrier-事件"><a href="#广播-Barrier-事件" class="headerlink" title="广播 Barrier 事件"></a>广播 Barrier 事件</h4><p>逻辑与发射 Record 类似，稍有不同：</p>
<ul>
<li>对于每个要广播的 channel，如果有残留在内存 Buffer 的数据，先通过 ResultPartitionWriter 写出去，再写 Barrier 事件</li>
<li>申请内存逻辑类似</li>
</ul>
<h3 id="ResultPartitionWriter"><a href="#ResultPartitionWriter" class="headerlink" title="ResultPartitionWriter"></a>ResultPartitionWriter</h3><p>上面介绍过 ResultPartitionWriter 是执行了序列化后 Record 的写操作，其实它是消息的路由器，具体写是通过 ResultPartition 来完成的</p>
<h3 id="ResultPartition"><a href="#ResultPartition" class="headerlink" title="ResultPartition"></a>ResultPartition</h3><p><em>先翻译下官方的 java doc：</em></p>
<p>一个 ResultPartition 是单个 task 的输出结果，在运行时，它是逻辑概念 IntermediateResultPartition 的一部分。ResultPartition 是一个 Buffer 的集合，并且以 ResultSubpartition 的形式进行组织，而 ResultSubPartiton 的个数等同于下游 consumer task 的个数，具体写入到哪个 ResultSubpartition 由 DistributionPattern 【上面说的 channel selector】来决定。</p>
<p>关于生命周期：一个 ResultPartition 有三个阶段：</p>
<ul>
<li>Produce</li>
<li>Consume</li>
<li>Release</li>
</ul>
<p>关于 consumer tasks 的延迟部署和更新：</p>
<ul>
<li>依据 ResultPartition 的类型：PIPELINED 和 BLOCKING 对应两种部署模式，分别对应流式计算和批处理</li>
<li>对于 PIPELINED 类型，consumer tasks 在 ResultPartiton 产生第一个结果 Buffer 的时候会立即部署</li>
<li>对于 BLOCKING 类型，consumer tasks 在 ResultPartiton 结束后才会部署</li>
</ul>
<p>观察 <code>line162</code> 可知如果是 PIPELINED 类型，ResultSubPartition 的类型为：PipelinedSubpartition</p>
<p>我们来分析下它的核心接口：</p>
<h4 id="注册-BufferPool-1"><a href="#注册-BufferPool-1" class="headerlink" title="注册 BufferPool"></a>注册 BufferPool</h4><p>代码见 <code>line187 registerBufferPool</code> 方法，此 BufferPool 为 TaskManager 在运行 task 时 通过 NetworkEnvironment 注册进来的。如果是 BLOCKING 类型，这里会注册 BufferPool 的 owner 为此 ResultPartiton，NetworkEnvironment 如果出现内存不足的情况，会通过 ResultPartiton 的引用，以 Sub Partiton 为单位将内存释放掉【见 TaskManager 基本组件 NetworkEnvironment】。</p>
<p>既然注册 BufferPool 是以 task 为单位的，那么每个 task 会拥有几个 BufferPool， BufferPool 的大小又是多少呢？在 NetworkEnvironment 中有这样一段逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//NetworkEnvironment line297</span></div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">					bufferPool = networkBufferPool.createBufferPool(partition.getNumberOfSubpartitions(), <span class="keyword">false</span>);</div><div class="line">					partition.registerBufferPool(bufferPool);</div><div class="line"></div><div class="line">					partitionManager.registerResultPartition(partition);</div><div class="line">				&#125;</div></pre></td></tr></table></figure>
<p>其中的 <code>partition.getNumberOfSubpartitions()</code> 是 ResultPartition 的 sub partition 的个数，我们可以看到，一个 task 拥有的 BufferPool 个数等于它产生的 ResultPartition 的个数，一个 ResultPartition 对应一个 BufferPool，而每个 BufferPool 的最小大小等于它的 sub partition 的个数【consumer task 的个数】。为什么说是最小大小，见 TaskManager 基本组件 NetworkEnvironment。</p>
<h4 id="写入-Buffer"><a href="#写入-Buffer" class="headerlink" title="写入 Buffer"></a>写入 Buffer</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// ResultPartition line244</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(Buffer buffer, <span class="keyword">int</span> subpartitionIndex)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="keyword">boolean</span> success = <span class="keyword">false</span>;</div><div class="line"></div><div class="line">		<span class="keyword">try</span> &#123;</div><div class="line">			checkInProduceState();</div></pre></td></tr></table></figure>
<p>简单总结：</p>
<ul>
<li>找到对应的 ResultSubPartition 并写入 Buffer</li>
<li>如果是第一次写入，并且是 PIPELINED 类型，通知 JobManager 可以部署下游 consumer，具体细节将会在 调度一章专门讲解</li>
<li>如果写入失败，回收内存 Buffer</li>
</ul>
<h4 id="ResultSubpatition"><a href="#ResultSubpatition" class="headerlink" title="ResultSubpatition"></a>ResultSubpatition</h4><p>这里我们只介绍 PipelinedSubpartition，实现比较简单，就不具体介绍了，有一点需要注意，就是 NotificationListener</p>
<p>每次 ResultSubpatition 的状态有变动【被写入，被释放，写入完成等】都会通知 NotificationListener，通知完之后就会销毁 NotificationListener 的注册</p>
<p><em>典型的我们详细分析下 add Buffer 后通过 NotificationListener 触发的一些列行为</em></p>
<p>PipelinedSubPartition 在添加一个 Buffer 后，如果 NotificationListener 不为 null，那么会通知 NotificationListener，NotificationListener 共有两个实现类 LocalInputChannel 和 SequenceNumberingSubpartitionView，分别对应本地传输和网络传输。</p>
<h4 id="与-InputChannel-的交互"><a href="#与-InputChannel-的交互" class="headerlink" title="与 InputChannel 的交互"></a>与 InputChannel 的交互</h4><p>在 consumer task 向 SingleInputGate 请求 Buffer 时，会先触发 requestPartitions 操作，不同的 channel 会触发不同的行为:</p>
<ul>
<li>对于 LocalInputChannel 来说就是获取 ResultSubPartitonView 并将自己注册到对应的 ResultSubPartition 中</li>
<li>对于 RemoteInputChannel 会触发 netty 连接并开始请求队列数据</li>
</ul>
<p><em>我们先来看 LocalInputChannel</em></p>
<h5 id="LocalInputChannel"><a href="#LocalInputChannel" class="headerlink" title="LocalInputChannel"></a>LocalInputChannel</h5><p>LocalInputChannel 在收到通知后，发生以下行为：</p>
<ul>
<li>check ResultSubPartitonView 中有无数据，如果有的话通知 SingleInputGate，并将自己【LocalInputChannel】加入到 SingleInputGate 的有数据 channel 队列中</li>
<li>如果没有数据，将自己重新注册到对应的 ResultSubPartition 中</li>
</ul>
<p><em>再来看SequenceNumberingSubpartitionView，不过在这之前先介绍 RemoteInputChannel</em></p>
<h5 id="RemoteInputChannel"><a href="#RemoteInputChannel" class="headerlink" title="RemoteInputChannel"></a>RemoteInputChannel</h5><p>上面提到 RemoteInputChannel 在 SingleInputGate 第一次执行获取 Buffer 的逻辑时，会触发 RemoteInputChannel 的网络连接，具体逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//RemoteInputChannel line114</span></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">requestSubpartition</span><span class="params">(<span class="keyword">int</span> subpartitionIndex)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">		<span class="keyword">if</span> (partitionRequestClient == <span class="keyword">null</span>) &#123;</div><div class="line">			<span class="comment">// Create a client and request the partition</span></div><div class="line">			partitionRequestClient = connectionManager</div><div class="line">					.createPartitionRequestClient(connectionId);</div><div class="line"></div><div class="line">			partitionRequestClient.requestSubpartition(partitionId, subpartitionIndex, <span class="keyword">this</span>, <span class="number">0</span>);</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>这一段先会创建 netty 连接，然后会请求 netty 数据，下面我们详细分析 flink 关于 netty 连接的抽象</p>
<h2 id="Netty-连接"><a href="#Netty-连接" class="headerlink" title="Netty 连接"></a>Netty 连接</h2><p>flink 采用 netty 作为底层的网络传输框架，各个 Instance 之间会依照消费依赖建立一个 Netty 连接，然后通过 NetworkEnvironment 管理数据的消费，本人对 Netty 研究的不是很精，这里只站在 flink 的角度梳理一些重点的组件</p>
<h3 id="ConnectionManager"><a href="#ConnectionManager" class="headerlink" title="ConnectionManager"></a>ConnectionManager</h3><p>flink 通过 ConnectionManager 来创建和管理一个 Instance 上的 netty 连接【client】，它有两个实现：LocalConnectionManager 和 NettyConnectionManager，这里只介绍后者</p>
<h4 id="NettyConnectionManager"><a href="#NettyConnectionManager" class="headerlink" title="NettyConnectionManager"></a>NettyConnectionManager</h4><h5 id="PartitionRequestQueue"><a href="#PartitionRequestQueue" class="headerlink" title="PartitionRequestQueue"></a>PartitionRequestQueue</h5><p>对于 netty 来说，这是 server 端 netty handler 的 adapter，除了基本的事件处理如注册 channel 外还定义了用户可以手动触发的两个事件：</p>
<ul>
<li>取消 channel 事件：会取消对应的 channel 的队列【用来读取对应 subPartition 的临时队列】，并标志该 channel 已被取消</li>
<li>入队事件：当开始消费某个 sub partition 时会触发此事件，会将该队列 queue 加入到已有的 队列中</li>
</ul>
<h6 id="SequenceNumberingSubpartitionView"><a href="#SequenceNumberingSubpartitionView" class="headerlink" title="SequenceNumberingSubpartitionView"></a>SequenceNumberingSubpartitionView</h6><p>上面说的对应每个 sub partition 的读取 queue 被抽象为 SequenceNumberingSubpartitionView，它维护了两个成员 ResultSubpartitionView 和 一个不断递增的 buffer 序列编号，之前在介绍 ResultPartition 时介绍过，当往 ResultSubPartiton 中写数据时会通知对应的 NotificationListener 【如果有的话】，其中对应 RemoteInputChannel 的便是 SequenceNumberingSubpartitionView，它在收到通知后会触发 入队 用户事件，将自己加入到已有队列中，表明自己是一个有数据队列</p>
<h6 id="数据发送"><a href="#数据发送" class="headerlink" title="数据发送"></a>数据发送</h6><p>每当 channel 可写，就会触发 netty 的 channelWritabilityChanged 接口调用，触发数据发送，我们来看下数据发送的核心逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//PartitionRequestQueue line134</span></div><div class="line"><span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">channelWritabilityChanged</span><span class="params">(ChannelHandlerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		writeAndFlushNextMessageIfPossible(ctx.channel());</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">writeAndFlushNextMessageIfPossible</span><span class="params">(<span class="keyword">final</span> Channel channel)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="keyword">if</span> (fatalError) &#123;</div><div class="line">			<span class="keyword">return</span>;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		Buffer buffer = <span class="keyword">null</span>;</div><div class="line"></div><div class="line">		<span class="keyword">try</span> &#123;</div><div class="line">			<span class="keyword">if</span> (channel.isWritable()) &#123;</div><div class="line">				<span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">					<span class="keyword">if</span> (currentPartitionQueue == <span class="keyword">null</span> &amp;&amp; (currentPartitionQueue = queue.poll()) == <span class="keyword">null</span>) &#123;</div><div class="line">						<span class="keyword">return</span>;</div><div class="line">					&#125;</div></pre></td></tr></table></figure>
<p>总结其逻辑：</p>
<ul>
<li>如果当前没有可写数据的队列，直接返回</li>
<li>如果某个 queue 有数据，会一直读取该队列的数据发送到下游，知道把数据读完后将该队列注册到 对应的 ResultSubPartition 中，等待下次有数据时再次入队</li>
<li>如果某个 ResultSubPartition 被 release 【批处理内存不足时】，标识其被 release</li>
<li>如果收到某个 ResultSubPartition 处理完成的事件，释放对应 ResultSubPartition 所有的资源【将对应的 ResultSubPartition 资源释放】并给出相应通知</li>
</ul>
<h5 id="PartitionRequetServerHandler"><a href="#PartitionRequetServerHandler" class="headerlink" title="PartitionRequetServerHandler"></a>PartitionRequetServerHandler</h5><p>Netty Server 端的 handler，负责做一些初始化和派发事件的功能</p>
<h6 id="消息路由"><a href="#消息路由" class="headerlink" title="消息路由"></a>消息路由</h6><p>该 handler 负责路由以下消息 NettyMessage：</p>
<ul>
<li>PartitionRequest：将对应的 ResultSubPartition 封装成 SequenceNumberingSubpartitionView 入队</li>
<li>TaskEventRequest：像对应的 ResultPartitionWriter 派发 task event</li>
<li>CancelPartition</li>
<li>CloseRequest</li>
</ul>
<h5 id="PartitionRequestProtocol"><a href="#PartitionRequestProtocol" class="headerlink" title="PartitionRequestProtocol"></a>PartitionRequestProtocol</h5><p>flink 的 netty 消息 协议，主要是客户端和 Server 端 handlers 的获取</p>
<h5 id="NettyBufferPool"><a href="#NettyBufferPool" class="headerlink" title="NettyBufferPool"></a>NettyBufferPool</h5><p>配置每个 netty chunk 为 16M</p>
<h3 id="PartitionRequestClient"><a href="#PartitionRequestClient" class="headerlink" title="PartitionRequestClient"></a>PartitionRequestClient</h3><p>通过 NettyConnectionManager 可以创建 PartitionRequestClient，它封装了原生的 NettyClient，通过 PartitionRequestClientFactory 来创建</p>
<h4 id="PartitionRequestClientFactory"><a href="#PartitionRequestClientFactory" class="headerlink" title="PartitionRequestClientFactory"></a>PartitionRequestClientFactory</h4><p>主要用来缓存已获得的 NettyChannel，这里就不具体介绍了，有兴趣的童鞋可以自行研究</p>
<h4 id="PartitionRequestClientHandler"><a href="#PartitionRequestClientHandler" class="headerlink" title="PartitionRequestClientHandler"></a>PartitionRequestClientHandler</h4><p>Netty Client 端的 Client Handler，负责将消息 decode 到 RemoteInputChannel，在 PartitionRequestClient 发起 PartitionRequest 请求消息的时候会将对应的 RemoteInputChannel 加入到 PartitionRequestClientHandler 备份起来。</p>
<h5 id="Channel-消息读"><a href="#Channel-消息读" class="headerlink" title="Channel 消息读"></a>Channel 消息读</h5><h6 id="消费端反压"><a href="#消费端反压" class="headerlink" title="消费端反压"></a>消费端反压</h6><p>为了防止读取消息时内存不够，读速度快于生产，这里有两个有意思的 runnable，BufferListenerTask 和 StagedMessagesHandlerTask，它们均由 netty io 线程负责执行，当消息顺畅读取，内存充足时，当然是 decode 消息，并将消息加入到对应的 RemoteInputChannel buffer 队列中，但是内存不足时，上面的两个 task 就会扮演重要角色！</p>
<ul>
<li>如果之前有消息堆积，将消息直接写入堆积队列</li>
<li>BufferListenerTask：当第一次出现读内存不足时，会将对应的 Buffer 托管给 BufferListenerTask，BufferListenerTask 将自己注册到 SingleInputGate 的 BufferPool 中，同时停止 channel 的自动读取，当 BufferPool 有内存，会通知 BufferListenerTask 并解析消息</li>
<li>这时候有一个意外，就是收到通知后仍然没有获取到内存 Buffer，这表明 BufferPool 已经被销毁【批处理内存不够时】，这时候启动 StagedMessagesHandlerTask，一直循环解析堆积队列里的数据</li>
</ul>
<p>这里涉及到了 backpressure，流式计算里的反压技术，flink 采取的是逐级反压，并且这两个 task 扮演了消费端的 反压角色</p>
<p>这里补一张图，说明 flink 的消费与生产之间的关系：</p>
<p><img src="netty-client-server.png" alt="netty-client-server.png"></p>
<p>这里链接一个 flink 官方的 wiki介绍了 flink task 的数据流交换细节，比较经典，这里偷个懒，就不翻译了，有时间一定补上^_^</p>
<blockquote>
<p> <a href="https://cwiki.apache.org/confluence/display/FLINK/Data+exchange+between+tasks" target="_blank" rel="external">flink data exchange betweek tasks </a></p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      本章节主要介绍 flink 的网络交互，包括每个 task 的输入输出管理，内存分配和释放等，因为涉及到内存申请，这里会介绍 flink 的内存管理
    
    </summary>
    
      <category term="Flink源码" scheme="https://danny0405.github.io/categories/Flink%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="Flink内部实现" scheme="https://danny0405.github.io/tags/Flink%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>taskmanager 基本组件</title>
    <link href="https://danny0405.github.io/2017/02/09/taskmanager%E5%9F%BA%E6%9C%AC%E7%BB%84%E4%BB%B6/"/>
    <id>https://danny0405.github.io/2017/02/09/taskmanager基本组件/</id>
    <published>2017-02-09T04:41:53.000Z</published>
    <updated>2018-01-23T06:12:09.765Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>TaskManager 在 Flink 中也被叫做一个 Instance，统一管理该物理节点上的所有 Flink job 的 tasks 运行，它的功能包括了 task 的启动销毁、内存管理、磁盘IO、网络传输管理等，本章将一一介绍这些功能，方面后续章节的开展</p>
<h2 id="MemoryManager"><a href="#MemoryManager" class="headerlink" title="MemoryManager"></a>MemoryManager</h2><p><em>先来翻译一下类的 JavaDoc ^_^</em></p>
<p>MemoryManager 统一管理了 flink 的内存使用，内存被划分为相同大小的 segment，通过申请不同数量的 segment 来分配不同大小的内存</p>
<p>这里支持两种内存：on-heap 内存和 off-heap 内存，通过参数可以控制分配内存的种类</p>
<p>MemoryManager 管理内存也分两种模式：预分配和按需分配。预分配模式下，内存在启动时就会分好，这就会意味着不会发生 OOM 异常，释放的内存会重新归还 MemoryManager 的内存池；按需模式下，MemoryManager 仅仅追踪内存的使用【做记录】，释放内存不会归还 MemoryManager 的内存池，而是通过托管给 JVM 的垃圾回收来最终释放，这样便可能会发生 OOM</p>
<p><em>下面我们就来分析下 MemoryManager 的实现细节</em></p>
<h3 id="MemorySegment"><a href="#MemorySegment" class="headerlink" title="MemorySegment"></a>MemorySegment</h3><p>上面已经提到，MemoryManager 以 segment 为单位来实现内存的分配和管理，在 flink 中一个 segment 被抽象为 MemorySegment，MemorySegment 为抽象类，定义了基本的 put/get 方法，以及 swap、compare 等工具方法，同时维护了一个偏移量：BYTE_ARRAY_BASE_OFFSET，这个偏移量为 byte[] 对象在内存中的基本偏移量，后续通过 <code>sun.misc.Unsafe</code> 直接操纵内存就是基于这个偏移量来完成，这个类定义的实现方法屏蔽了内存的种类【堆和非堆】，当其成员变量 heapMemory 不为空时就是堆内存，此时的 address 就是 BYTE_ARRAY_BASE_OFFSET；而 heapMemory 为 null 时代表非堆内存，此时的 address 是内存中的绝对地址。</p>
<p>MemorySegment 有两个实现类：HeapMemorySegment 和 HibridMemorySegment，分别代表堆内存 segment 和 非堆内存 segment，具体的继承关系如下：</p>
<p><img src="memory-segment-extend.png" alt="memory-segment-extend.png"></p>
<p>HeapMemorySegment 和 HibridMemorySegment 中都分别定义了工厂类来实例化对象实例。</p>
<h3 id="MemoryPool"><a href="#MemoryPool" class="headerlink" title="MemoryPool"></a>MemoryPool</h3><p>MemoryPool 是 MemoryManager 用来统一管理资源的组件，具体又分为 HeapMemoryPool 和 HybridOffHeapMemoryPool，前者管理堆内存，后者管理非堆内存。</p>
<p><em>先来看HeapMemoryPool</em></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//MemoryManager.java line 616</span></div><div class="line"><span class="meta">@Override</span></div><div class="line">		<span class="function">HeapMemorySegment <span class="title">allocateNewSegment</span><span class="params">(Object owner)</span> </span>&#123;</div><div class="line">			<span class="keyword">return</span> HeapMemorySegment.FACTORY.allocateUnpooledSegment(segmentSize, owner);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="meta">@Override</span></div><div class="line">		<span class="function">HeapMemorySegment <span class="title">requestSegmentFromPool</span><span class="params">(Object owner)</span> </span>&#123;</div><div class="line">			<span class="keyword">byte</span>[] buf = availableMemory.remove();</div><div class="line">			<span class="keyword">return</span>  HeapMemorySegment.FACTORY.wrapPooledHeapMemory(buf, owner);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="meta">@Override</span></div><div class="line">		<span class="function"><span class="keyword">void</span> <span class="title">returnSegmentToPool</span><span class="params">(MemorySegment segment)</span> </span>&#123;</div><div class="line">			<span class="keyword">if</span> (segment.getClass() == HeapMemorySegment.class) &#123;</div><div class="line">				HeapMemorySegment heapSegment = (HeapMemorySegment) segment;</div><div class="line">				availableMemory.add(heapSegment.getArray());</div><div class="line">				heapSegment.free();</div><div class="line">			&#125;</div><div class="line">			<span class="keyword">else</span> &#123;</div><div class="line">				<span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Memory segment is not a "</span> + HeapMemorySegment.class.getSimpleName());</div><div class="line">			&#125;</div><div class="line">		&#125;</div></pre></td></tr></table></figure>
<p>简单总结：</p>
<ul>
<li>allocateNewSegment 走的是 on demand 模式，通过 new byte[] 从堆上分配内存</li>
<li>requestSegmentFromPool 走的是 pre allocate 模式，通过复用已有的堆对象</li>
</ul>
<p>HybridOffHeapMemoryPool 的接口与其类似，不过分配内存走的是 <code>ByteBuffer.allocateDirect(segmentSize);</code> 直接分配了物理内存，也就是非堆内存</p>
<h2 id="IOManager"><a href="#IOManager" class="headerlink" title="IOManager"></a>IOManager</h2><p>flink 通过 IOManager 来控制磁盘 IO 的过程，提供同步和异步两种写模式【其实只有异步】，具体的读写方式又分为 block、buffer、bulk 三种方式；用户可以指定 IO 的文件目录集合，IOManager 会以 round-robin 的方式写不同目录的不同文件。</p>
<p>IOManager 提供两种方式枚举新的 IO 文件：</p>
<ul>
<li><p>直接 round-robin 文件夹并生成文件，每个新文件的命名 pattern 为 random_hex_string.channel，最终对应的目录结构是：</p>
<p>path1/random_hex_string1.channel</p>
<p>path2/random_hex_string2.channel</p>
<p>path3/random_hex_string3.channel</p>
</li>
<li><p>采取 Enumerator 的模式，每个 Enumerator 也是类似如上一种方式进行 round-robin，不过 Enumerator 会维护一个固定的本地命名前缀、一个本地计数器、一个全局计数器，命名前缀用于区分不同的 Enumerator 写的文件，本地计数器用于 Enumerator 自身的文件命名递增，全局计数器用于 round-robin 文件夹，最终的目录结构是：</p>
<p>path1/prefix.local_counter1.channel</p>
<p>path2/prefix.local_counter2.channel</p>
<p>path3/prefix.local_counter3.channel</p>
</li>
</ul>
<p>flink 又进一步将一个文件的 IO 抽象成了 FileIOChannel，通过 FileIOChannel 封装了底层的文件读写，具体的继承关系如下：</p>
<p><img src="file-channel-entend.png" alt="file-channel-entend.png"></p>
<p>IOManager 的唯一实现类：IOManagerAsync 为每个人临时文件加【用户初始化的时候指定】维护了一个读线程和写线程，并且每个读写线程内部会维护一个请求队列: RequestQueue，上面的 FileIOChannel 通过将 读写请求加入到对应的 RequestQueue 中来实现文件读写，具体的线程模型如下：</p>
<p><img src="io-manager-async.png" alt="io-manager-async.png"></p>
<p>ps: 默认的临时文件夹目录是 java.io.tmpDir</p>
<h2 id="NetworkEnvironment"><a href="#NetworkEnvironment" class="headerlink" title="NetworkEnvironment"></a>NetworkEnvironment</h2><p>NetworkEnvironment 是每个 Instance 的网络 IO 组件，包含了追踪中间结果和数据交换的数据结构。它的构造器会统一将配置的内存先分配出来，抽象成 NetworkBufferPool 统一管理内存的申请和释放。</p>
<h3 id="BufferPool"><a href="#BufferPool" class="headerlink" title="BufferPool"></a>BufferPool</h3><p>从 MemoryManager 的介绍中我们讲到 flink 是以 MemorySegment 为单位来管理内存的，而一个 MemorySegment 又被叫做一个 Buffer。BufferPool 是管理 Buffer 的工具。Buffer 的申请统一交给 NetworkBufferPool，具体的管理交给 LocalBufferPool。</p>
<h4 id="LocalBufferPool"><a href="#LocalBufferPool" class="headerlink" title="LocalBufferPool"></a>LocalBufferPool</h4><p>我们来看 LocalBufferPool 的关键接口，以了解具体都有哪些方式来管理 Buffer 😄。</p>
<h5 id="申请-Buffer"><a href="#申请-Buffer" class="headerlink" title="申请 Buffer"></a>申请 Buffer</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//LocalBufferPool line136</span></div><div class="line"><span class="function"><span class="keyword">private</span> Buffer <span class="title">requestBuffer</span><span class="params">(<span class="keyword">boolean</span> isBlocking)</span> <span class="keyword">throws</span> InterruptedException, IOException </span>&#123;</div><div class="line">		<span class="keyword">synchronized</span> (availableMemorySegments) &#123;</div><div class="line">			returnExcessMemorySegments();</div><div class="line"></div><div class="line">			<span class="keyword">boolean</span> askToRecycle = owner != <span class="keyword">null</span>;</div><div class="line"><span class="comment">//...</span></div></pre></td></tr></table></figure>
<p>总结其逻辑：</p>
<ul>
<li>申请 Buffer</li>
<li>释放超量申请的 Buffer</li>
<li>像 NetworkBufferPool 申请 Buffer</li>
<li>如果此 LocalBufferPool 有 owner【ResultPartition】，像 ResultPartition 释放内存，这里又会下发到 ResultPartition 的 subPartition，释放是以 subPartition 的全部内存为单位，会将内存中的数据吐到磁盘上或者不释放【依据配置的不同】</li>
</ul>
<h5 id="回收-Buffer"><a href="#回收-Buffer" class="headerlink" title="回收 Buffer"></a>回收 Buffer</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//LocalBufferPool line175</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">recycle</span><span class="params">(MemorySegment segment)</span> </span>&#123;</div><div class="line">		<span class="keyword">synchronized</span> (availableMemorySegments) &#123;</div><div class="line">			<span class="keyword">if</span> (isDestroyed || numberOfRequestedMemorySegments &gt; currentPoolSize) &#123;</div><div class="line">				returnMemorySegment(segment);</div><div class="line">			&#125;</div></pre></td></tr></table></figure>
<p>简单的总结：</p>
<ul>
<li>如果此 LocalBuffer 已销毁或超量使用，将 Buffer 归还给 NetworkBufferPool</li>
<li>否则如果注册了 EventListener ，通知每个 listener 这个 Buffer 被回收</li>
<li>如果没有注册，将这个 Buffer 重新标记为可使用【加入到待申请队列】</li>
</ul>
<h5 id="调整-Buffer-大小"><a href="#调整-Buffer-大小" class="headerlink" title="调整 Buffer 大小"></a>调整 Buffer 大小</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//LocalBufferPool line237</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setNumBuffers</span><span class="params">(<span class="keyword">int</span> numBuffers)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="keyword">synchronized</span> (availableMemorySegments) &#123;</div><div class="line">			checkArgument(numBuffers &gt;= numberOfRequiredMemorySegments, <span class="string">"Buffer pool needs at least "</span> + numberOfRequiredMemorySegments + <span class="string">" buffers, but tried to set to "</span> + numBuffers + <span class="string">"."</span>);</div><div class="line"></div><div class="line">			currentPoolSize = numBuffers;</div></pre></td></tr></table></figure>
<p>简单总结：</p>
<ul>
<li>归还超量使用的内存给 NetworkBufferPool</li>
<li>如果还是超量使用，调用 owner 的释放接口【以 ResultSubPartiton 为单位释放】</li>
</ul>
<h4 id="NetworkBufferPool"><a href="#NetworkBufferPool" class="headerlink" title="NetworkBufferPool"></a>NetworkBufferPool</h4><p>上面已经提到，NetworkbufferPool 统一管理了网络栈的内存，LocalBufferPool 只是管理 Buffer 的方式，具体的申请和释放还是要走 NetworkBufferPool 的接口。值得注意的是，NetworkBufferPool 在实例化的时候就将初始的固定大小的内存分配出来了【不管是堆还是非堆】。我们来看它的关键接口：</p>
<h5 id="创建-LocalBufferPool"><a href="#创建-LocalBufferPool" class="headerlink" title="创建 LocalBufferPool"></a>创建 LocalBufferPool</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//NetworkBufferPool line184</span></div><div class="line"><span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> BufferPool <span class="title">createBufferPool</span><span class="params">(<span class="keyword">int</span> numRequiredBuffers, <span class="keyword">boolean</span> isFixedSize)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="comment">// It is necessary to use a separate lock from the one used for buffer</span></div><div class="line">		<span class="comment">// requests to ensure deadlock freedom for failure cases.</span></div><div class="line">		<span class="keyword">synchronized</span> (factoryLock) &#123;</div><div class="line">			<span class="keyword">if</span> (isDestroyed) &#123;</div><div class="line">				<span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Network buffer pool has already been destroyed."</span>);</div><div class="line">			&#125;</div></pre></td></tr></table></figure>
<p>简单总结：</p>
<ul>
<li>做一些状态备份，包括整体使用的 Buffer 数、可动态调整大小的 BufferPool 等</li>
<li>对于可动态调整的 BufferPool，重新调整可用内存，调整方式为 round-robin</li>
</ul>
<h5 id="销毁-LocalBufferPool"><a href="#销毁-LocalBufferPool" class="headerlink" title="销毁 LocalBufferPool"></a>销毁 LocalBufferPool</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//NetworkBufferPool line227</span></div><div class="line"><span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroyBufferPool</span><span class="params">(BufferPool bufferPool)</span> </span>&#123;</div><div class="line">		<span class="keyword">if</span> (!(bufferPool <span class="keyword">instanceof</span> LocalBufferPool)) &#123;</div><div class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"bufferPool is no LocalBufferPool"</span>);</div><div class="line">		&#125;</div></pre></td></tr></table></figure>
<p>简单总结：</p>
<ul>
<li>消除状态记录</li>
<li>对于可动态调整的 BufferPool，重新调整可用内存，调整方式为 round-robin</li>
</ul>
]]></content>
    
    <summary type="html">
    
      TaskManager 在 Flink 中也被叫做一个 Instance，统一管理该物理节点上的所有 Flink job 的 tasks 运行，它的功能包括了 task 的启动销毁、内存管理、磁盘IO、网络传输管理等，本章将一一介绍这些功能，方面后续章节的开展
    
    </summary>
    
      <category term="Flink源码" scheme="https://danny0405.github.io/categories/Flink%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="Flink组件" scheme="https://danny0405.github.io/tags/Flink%E7%BB%84%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>flink 算子的生命周期</title>
    <link href="https://danny0405.github.io/2017/02/08/flink%E7%AE%97%E5%AD%90%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/"/>
    <id>https://danny0405.github.io/2017/02/08/flink算子的生命周期/</id>
    <published>2017-02-08T14:35:36.000Z</published>
    <updated>2018-01-23T06:10:34.183Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前面已经介绍了 flink 的逻辑计划、物理计划等相关信息，本文将重点介绍 flink 的 operator 以及运行时的 task，后续会介绍 flink task 的调度算法</p>
<h2 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h2><h3 id="什么是一个算子"><a href="#什么是一个算子" class="headerlink" title="什么是一个算子"></a>什么是一个算子</h3><p>flink 中的一个 operator 代表一个最顶级的 api 接口，拿 streaming 来说就是，在 DataStream 上做诸如 map/reduce/keyBy 等操作均会生成一个算子</p>
<h3 id="算子的生成"><a href="#算子的生成" class="headerlink" title="算子的生成"></a>算子的生成</h3><p>先来看 operator 的继承关系:</p>
<p><img src="flink-operator-extend.png" alt="flink-operator-extend.png">对于 Streaming 来说所有的算子都继承自 StreamOperator，StreamOperator 中定义了一系列的生命周期方法，同时也定义了 snapshort 的接口，AbstractStreamOperator 定义了基本的设置和声明周期方法，AbstractUdfStreamOperator 定义了用户自定义函数的生命周期和快照策略，这些接口的调用时机会在下面一一阐述😄。</p>
<p>算子的生成触发于对 DataStream 的操作上，比如 map addSink等。</p>
<h3 id="算子-chain"><a href="#算子-chain" class="headerlink" title="算子 chain"></a>算子 chain</h3><p>在 <strong>flink 基本组件和逻辑计划生成一节</strong> 我们介绍了 JobGraph 的生成过程，其中 JobGraph 的生成最大的意义在于做了一些算子的 chain 优化，那么什么样的节点可以被 chain 呢？如下图：</p>
<p><img src="op-chian-chianable.png" alt="op-chian-chianable.png"></p>
<p>一些必须要经过 shuffle 的节点是 chain 或者 节点可达 的边界，非常类似于 Spark Streaming 中对于 Stage 的划分，上图中 keyBy 这样的 groupBy 操作就是划分是否可被 chain 的边界</p>
<p>在 StreamingJobGraphGenerator 的 createChain 方法中为每个 StreamNode 生成了一个 StreamConfig，并且对于可以生成 JobVertex 的节点[ <em>chain 的起始节点</em> ]设置了如下属性：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//StreamingJobGraphGenerator line212</span></div><div class="line"><span class="keyword">if</span> (currentNodeId.equals(startNodeId)) &#123;</div><div class="line"></div><div class="line">   config.setChainStart();</div><div class="line">   config.setChainIndex(<span class="number">0</span>);</div><div class="line">   config.setOutEdgesInOrder(transitiveOutEdges);</div><div class="line">   config.setOutEdges(streamGraph.getStreamNode(currentNodeId).getOutEdges());</div><div class="line"></div><div class="line">   <span class="keyword">for</span> (StreamEdge edge : transitiveOutEdges) &#123;</div><div class="line">      connect(startNodeId, edge);</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   config.setTransitiveChainedTaskConfigs(chainedConfigs.get(startNodeId));</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面的逻辑概括如下：</p>
<ul>
<li>标志本节点为 chain 的起始位置</li>
<li>设置 chain 的索引</li>
<li>设置可达输出边，就是与下游 JobVertex 直接连接的 StreamEdge</li>
<li>设置自身的直接输出边 StreamEdge</li>
<li>将本 JobVertex 与下游的 JobVertex 连接起来</li>
<li>将被 chained 的可达的下游 StreamNode 的配置一同设置进本 JobVertex 的配置中，后面 task 运行时会用到</li>
</ul>
<p>连接的逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//StreamingJobGraphGenerator line357</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">connect</span><span class="params">(Integer headOfChain, StreamEdge edge)</span> </span>&#123;</div><div class="line"></div><div class="line">   physicalEdgesInOrder.add(edge);</div><div class="line"></div><div class="line">   Integer downStreamvertexID = edge.getTargetId();</div><div class="line"></div><div class="line">   JobVertex headVertex = jobVertices.get(headOfChain);</div><div class="line">   JobVertex downStreamVertex = jobVertices.get(downStreamvertexID);</div><div class="line"></div><div class="line">   StreamConfig downStreamConfig = <span class="keyword">new</span> StreamConfig(downStreamVertex.getConfiguration());</div><div class="line"></div><div class="line">   downStreamConfig.setNumberOfInputs(downStreamConfig.getNumberOfInputs() + <span class="number">1</span>);</div><div class="line"></div><div class="line">   StreamPartitioner&lt;?&gt; partitioner = edge.getPartitioner();</div><div class="line">   <span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardPartitioner) &#123;</div><div class="line">      downStreamVertex.connectNewDataSetAsInput(</div><div class="line">         headVertex,</div><div class="line">         DistributionPattern.POINTWISE,</div><div class="line">         ResultPartitionType.PIPELINED,</div><div class="line">         <span class="keyword">true</span>);</div><div class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> RescalePartitioner)&#123;</div><div class="line">      downStreamVertex.connectNewDataSetAsInput(</div><div class="line">         headVertex,</div><div class="line">         DistributionPattern.POINTWISE,</div><div class="line">         ResultPartitionType.PIPELINED,</div><div class="line">         <span class="keyword">true</span>);</div><div class="line">   &#125; <span class="keyword">else</span> &#123;</div><div class="line">      downStreamVertex.connectNewDataSetAsInput(</div><div class="line">            headVertex,</div><div class="line">            DistributionPattern.ALL_TO_ALL,</div><div class="line">            ResultPartitionType.PIPELINED,</div><div class="line">            <span class="keyword">true</span>);</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</div><div class="line">      LOG.debug(<span class="string">"CONNECTED: &#123;&#125; - &#123;&#125; -&gt; &#123;&#125;"</span>, partitioner.getClass().getSimpleName(),</div><div class="line">            headOfChain, downStreamvertexID);</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>概括下逻辑：</p>
<ul>
<li>获取要连接的两个 JobVertex 对象</li>
<li>设置下游 JobVertex 的输入 partition 算法，如果是 forward 或 rescale 的话为 POINTWISE，否则为全连接，也就是 shuffle，POINTWISE 的连接算法在 <strong>flink 物理计划生成</strong> 一节已经介绍，这里不再赘述</li>
</ul>
<p><em>以上只是客户端生成逻辑计划时的算子 chain，在运行时算子的的 chain 被封装成了一个单独的对象 OperatorChain，里面在原有的基础上将 operators 的操作封装起来并且确定了下游的的输出入口</em></p>
<p>来看 OperatorChain 的核心实现</p>
<p>首先总结下构造器的功能:</p>
<ul>
<li>获取可达的 chain 的 StreamNode 配置</li>
<li>为直接可达的输出 StreamEdge 分别创建一个 Output，这里为 RecordWriterOutput</li>
<li>创建chain的入口</li>
<li>如果创建有任何失败，释放掉 RecordWriterOutput 占用的资源，主要是内存 buffer，后面章节会介绍</li>
</ul>
<p>这里的关键是算子 chain 的创建过程，见下图创建过程：</p>
<p><img src="op-chain-internal.png" alt="op-chain-internal.png"></p>
<p>上图中 S 节点的下游 A/B/C 是可以与 S Chain 在一起的，D/E 是必须经过网络传输的节点，一个 OperatorChain 封装了图中的节点 S/A/B/C，也就是说上图可以被看做如下所示：</p>
<p><img src="operator-chain-simple.png" alt="operator-chain-simple.png"></p>
<p>OperatorChain 中有两个关键的方法：<code>createOutputCollector</code> 和 <code>createChainedOperator</code>，前者负责获取一个 StreamNode 的输出Output，后者负责创建 StreamNode 对应的 chain 算子，两者相互调用形成递归，如上面的创建过程图，具体的流程如下：</p>
<ul>
<li>创建 S 的所有网络输出 RecordWriterOutput，这里会为 D 和 E 分别创建一个</li>
<li>由于从 A 开始对于 S 是可被 chain 的，会递归创建从 C 开始</li>
<li>先获取 C 的输出，这里为对应 D 的 RecordWriterOutput</li>
<li>拿到 C 对应的 StreamOperator 并将 运行时的 StreamTask 和 Output 设置进去</li>
<li>将 StreamOperator 封装成 ChainingOutput 并作为 Output 传给 B</li>
<li>B 将重复 C 的过程，直到 S/A/B/C 全部被创建</li>
</ul>
<p><em>那么 S 发射一条消息后的处理流程是如何呢？</em></p>
<p>S 在调用 <code>processElement</code> 方法时会调用 <code>output.collect</code>，这里的 output 为 A 对应的 ChainingOutput，ChainingOutput 的 collect 调用了对应的算子 <code>StreamOperator A</code> 的 <code>processElement</code> 方法，这里又会调用 B 的 ChainingOutput 的 collect 方法，以此类推。这样便实现了可 chain 算子的本地处理，最终经由网络输出 RecordWriterOutput 发送到下游节点</p>
<h3 id="算子的运行"><a href="#算子的运行" class="headerlink" title="算子的运行"></a>算子的运行</h3><p>flink 算子的运行牵涉到两个关键类 <code>Task.java</code> 和 <code>StreamTask.java</code>，Task 是直接受 TaskManager 管理和调度的，而 Task 又会调用 StreamTask，StreamTask 中封装了算子的处理逻辑</p>
<p><strong>我们先来看 StreamTask</strong></p>
<p>StreamTask 的 JavaDoc 上描述了其生命周期:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">*  -- restoreState() -&gt; restores state of all operators in the chain</div><div class="line">*  </div><div class="line">*  -- invoke()</div><div class="line">*        |</div><div class="line">*        +----&gt; <span class="function">Create basic <span class="title">utils</span> <span class="params">(config, etc)</span> and load the chain of operators</span></div><div class="line">*        +----&gt; operators.<span class="title">setup</span><span class="params">()</span></div><div class="line">*        +----&gt; task specific <span class="title">init</span><span class="params">()</span></div><div class="line">*        +----&gt; open-<span class="title">operators</span><span class="params">()</span></div><div class="line">*        +----&gt; <span class="title">run</span><span class="params">()</span></div><div class="line">*        +----&gt; close-<span class="title">operators</span><span class="params">()</span></div><div class="line">*        +----&gt; dispose-<span class="title">operators</span><span class="params">()</span></div><div class="line">*        +----&gt; common cleanup</div><div class="line">*        +----&gt; task specific <span class="title">cleanup</span><span class="params">()</span></div></pre></td></tr></table></figure>
<p>StreamTask 运行之初会尝试恢复算子的 State 快照，然后由 Task 调用其 invoke 方法</p>
<p>下面重点分析一下其 invoke 方法的实现</p>
<ul>
<li>获取 headOperator，这里的 headOperator 在 StreamingJobGraphGenerator line 210 <code>setVertexConfig(currentNodeId, config, chainableOutputs, nonChainableOutputs);</code>设置，对应上面算子 chain 中的 S 节点</li>
<li>创建 operatorChain 并设置为 headOperator 的 Output</li>
<li><code>init()</code></li>
<li><code>restoreState</code></li>
<li>执行 operatorChain 中所有 operator 的 open 方法</li>
<li><code>run()</code></li>
<li>执行 operatorChain 中所有 operator 的 close 方法</li>
<li>执行资源回收及 <code>cleanup()</code>，最主要的目的是回收内存 buffer</li>
</ul>
<p>StreamTask 中还有关于 Checkpoint 和 StateBackup 的核心逻辑，这里先不介绍，会另开一篇😄</p>
<p>我们来看 StreamTask 的实现类之一 OneInputStreamTask ，便可以知道 <code>init()</code> 和 <code>run()</code> 分别都做了什么：</p>
<p><strong>init方法</strong>：</p>
<ul>
<li>获取算子对应的输入序列化器 TypeSerializer</li>
<li>获取输入数据 InputGate[]，InputGate 是 flink 网络传输的核心抽象之一，其在内部封装了消息的接收和内存的管理，后面介绍 flink 网络栈的时候会详细介绍，这里只要了解从 InputGate 可以拿到上游传送过来的数据就可以了</li>
<li>初始化 StreamInputProcessor</li>
<li>设置一些 metrics 及 累加器</li>
</ul>
<p>StreamInputProcessor 是 StreamTask 内部用来处理 Record 的组件，里面封装了外部 IO 逻辑【<em>内存不够时将 buffer 吐到磁盘上</em>】以及 时间对齐逻辑【<em>Watermark</em>】，这两个将会合并一节在下一章介绍^_^</p>
<p><strong>run方法</strong>:</p>
<ul>
<li>从 StreamInputProcessor 中处理一条记录</li>
<li>check 是否有异常</li>
</ul>
<p><strong>真正的运行时类 Task</strong></p>
<p> <em>这里我们会详细的介绍下 Task 的核心逻辑</em></p>
<p>Task 代表一个 TaskManager 中所起的并行 子任务，执行封装的 flink 算子并运行，提供以下服务：消费输入data、生产 IntermediateResultPartition [ <em>flink关于中间结果的抽象</em> ]、与 JobManager 交互</p>
<p>JobManager 分发 Task 时最初是抽象成了一个描述类 TaskDeploymentDescriptor，TaskManager 在抽到对应的 RPC 请求后会将 Task 初始化后将 线程 拉起，TaskDeploymentDescriptor 是提供 task 信息的核心抽象：</p>
<ul>
<li>ResultPartitions：task 输出的 partition 数[ <em>通常和 JobVertex 的下游节点数对应</em>  ]</li>
<li>InputGates：task 的输入中间结果 partition</li>
<li>operator-state：算子的状态句柄，由 TaskManager 上报给 JobManager，并统一维护</li>
<li>jar-files</li>
<li>class-paths</li>
</ul>
<p>构造器的一些组件我们会在介绍 TaskManager 的时候再详述</p>
<p>其核心的运行方法 run()逻辑总结如下：</p>
<p>line408: run</p>
<ul>
<li>核心的运行逻辑</li>
<li>line429: 遇到错误后通知 TaskManager</li>
<li>line469: 从 NetworkEnvironment 中申请 BufferPool，包括 InputGate 的接收 pool 以及 task 的每个 ResultPartition 的输出 pool，申请的资源数[ <em>num of Buffer</em> ] 由 input channels 和 ResultSubPartition 数决定</li>
</ul>
<p>关于网络管理[ 输入和输出 ] NetworkEnvironment，内存管理 MemoryManager 会分别开章节介绍</p>
<p>那么 StreamTask 是如何在 Task 中被实例化，又是如何被调用的呢？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//line 418</span></div><div class="line">invokable = loadAndInstantiateInvokable(userCodeClassLoader, nameOfInvokableClass);</div><div class="line"><span class="comment">//一系列初始化操作 ...</span></div><div class="line"><span class="comment">//line 584</span></div><div class="line">invokable.invoke();</div></pre></td></tr></table></figure>
<p>上面的 invokable 就是 StreamTask，StreamTask  的继承关系:</p>
<p><img src="stream-task-extend.png" alt="stream-task-extend.png"></p>
<p>那么具体是什么时候被 set 进去作为属性的呢？</p>
<p>在 StreamNode 生成的时候有这样一段逻辑:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> &lt;IN, OUT&gt; <span class="function"><span class="keyword">void</span> <span class="title">addOperator</span><span class="params">(</span></span></div><div class="line">      Integer vertexID,</div><div class="line">      String slotSharingGroup,</div><div class="line">      StreamOperator&lt;OUT&gt; operatorObject,</div><div class="line">      TypeInformation&lt;IN&gt; inTypeInfo,</div><div class="line">      TypeInformation&lt;OUT&gt; outTypeInfo,</div><div class="line">      String operatorName) &#123;</div><div class="line"></div><div class="line">   <span class="keyword">if</span> (operatorObject <span class="keyword">instanceof</span> StoppableStreamSource) &#123;</div><div class="line">      addNode(vertexID, slotSharingGroup, StoppableSourceStreamTask.class, operatorObject, operatorName);</div><div class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operatorObject <span class="keyword">instanceof</span> StreamSource) &#123;</div><div class="line">      addNode(vertexID, slotSharingGroup, SourceStreamTask.class, operatorObject, operatorName);</div><div class="line">   &#125; <span class="keyword">else</span> &#123;</div><div class="line">      addNode(vertexID, slotSharingGroup, OneInputStreamTask.class, operatorObject, operatorName);</div><div class="line">   &#125;</div></pre></td></tr></table></figure>
<p>将 OneInputStreamTask 等 StreamTask 设置到 StreamNode 的节点属性中，同时在 JobVertex 的节点构造时也会做一次初始化:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">jobVertex.setInvokableClass(streamNode.getJobVertexClass());</div></pre></td></tr></table></figure>
<p>在 TaskDeploymentDescriptor 实例化的时候会获取 jobVertex 中的属性，见<code>ExecutionVertex line673</code></p>
<h4 id="算子初始化"><a href="#算子初始化" class="headerlink" title="算子初始化"></a>算子初始化</h4><p>那么算子是什么时候被初始化的呢？这就需要梳理下 StreamTask 的 <code>init()</code> 方法的处理时机，上面已经分析过 <code>init()</code> 方法会在 StreamTask 的 <code>invoke()</code> 方法中被调用，那么 <code>invoke()</code> 方法又是何时被调用的呢？这就涉及到另外一个重要的类 Task.java，Task 才是运行时真正直接被 TaskManager 实例化和调用的类，上面已经分析过 Task 的 run 方法，是 TaskManager 收到 rpc 命令后起起来的 具体的细节会另起一章 flink 任务分发</p>
<h4 id="算子销毁"><a href="#算子销毁" class="headerlink" title="算子销毁"></a>算子销毁</h4><p>StreamTask 下执行完 invoke 方法之后[<em>意味着流程正常结束或者有异常打断</em>]，会执行下面这段逻辑:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Execute the operator-specific &#123;<span class="doctag">@link</span> StreamOperator#dispose()&#125; method in each</div><div class="line"> * of the operators in the chain of this &#123;<span class="doctag">@link</span> StreamTask&#125;. &lt;/b&gt; Disposing happens</div><div class="line"> * from &lt;b&gt;tail to head&lt;/b&gt; operator in the chain.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">tryDisposeAllOperators</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">   <span class="keyword">for</span> (StreamOperator&lt;?&gt; operator : operatorChain.getAllOperators()) &#123;</div><div class="line">      <span class="keyword">if</span> (operator != <span class="keyword">null</span>) &#123;</div><div class="line">         operator.dispose();</div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>所以，算子中有任何 hook 函数或者必须执行的销毁工作可以写在 dispose 方法里，这段逻辑是 flink 保证一定可以执行到的</p>
]]></content>
    
    <summary type="html">
    
      前面已经介绍了 flink 的逻辑计划、物理计划等相关信息，本文将重点介绍 flink 的 operator 以及运行时的 task，后续会介绍 flink task 的调度算法
    
    </summary>
    
      <category term="Flink源码" scheme="https://danny0405.github.io/categories/Flink%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="Flink内部实现" scheme="https://danny0405.github.io/tags/Flink%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>jobmanager 基本组件</title>
    <link href="https://danny0405.github.io/2017/02/08/jobmanager%E5%9F%BA%E6%9C%AC%E7%BB%84%E4%BB%B6/"/>
    <id>https://danny0405.github.io/2017/02/08/jobmanager基本组件/</id>
    <published>2017-02-08T14:22:33.000Z</published>
    <updated>2018-01-23T06:12:37.549Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>JobManager 是 flink 集群的中控节点，类似于 Apache Storm 的 Nimbus 以及 Apache Spark 的 Driver 的角色，它负责作业的调度、jar 包管理、checkpoint 的协调和发起等，为了后续章节的开展，本文将介绍 flink JobManager 中所部署的一些服务。</p>
<h2 id="BolbServer"><a href="#BolbServer" class="headerlink" title="BolbServer"></a>BolbServer</h2><p>flink 用来管理二进制大文件的服务，flink JobManager 中启动的 BLOB Server 负责监听请求并派发线程去处理。更进一步，它将负责创建对应的目录结构去存储这些 BLOBs 或者只是临时性地缓存。背后支持的文件系统：本底磁盘</p>
<p>来看它的构造器：</p>
<ul>
<li>第一步获取 RecoveryMode，一共两种 STANDALONE 和 ZOOKEEPER，后者是有 JobManager leader 选举的高可用模式</li>
<li>获取文件系统存储的根目录，可配置，默认是从系统环境变量 <code>System.getProperty(&quot;java.io.tmpdir&quot;)</code> 中获取，其实就是本次磁盘存储</li>
<li>初始化 <em>恢复存储</em> 模块 BolbStore，STANDALONE 模式下为 VoidBlobStore，VoidBlobStore 是一个空实现；不会有任何持久化操作；ZOOKEEPER 模式下为 FileSystemBlobStore，FileSystemBlobStore 内部封装了磁盘文件的管理，包括添加、删除、拷贝等，BlogStore 会备份 BlobServer 的本地存储，主要用于恢复模式下的作业磁盘状态恢复用</li>
<li>启动 ServerSocket</li>
<li>启动 BlobServer 服务线程</li>
</ul>
<h3 id="BlogServer-和-BlobStore"><a href="#BlogServer-和-BlobStore" class="headerlink" title="BlogServer 和 BlobStore"></a>BlogServer 和 BlobStore</h3><p>BlobStore 是 BlobServer 的组件之一，BolbStore 主要负责 BlobServer 本地存储的恢复【JobManager 重启】，这里只介绍 FileSystemBlobStore，FileSystemBlobStore 依据配置的不同支持两种文件系统存储：HDFS 和 本地文件系统</p>
<p>BlobServer 和  FileSystemBlobStore 的存储目录结构如下图所示：</p>
<p><img src="blob-server-store-dirctory-tree.png" alt="blob-server-store-dirctory-tree.png"></p>
<p><em>下面以一次客户端连接请求的发起介绍两者的协同</em></p>
<p>来看 BolbServer 的核心 <code>run</code> 方法:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//BlobServer line230</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">   <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">while</span> (!<span class="keyword">this</span>.shutdownRequested.get()) &#123;</div><div class="line">         BlobServerConnection conn = <span class="keyword">new</span> BlobServerConnection(serverSocket.accept(), <span class="keyword">this</span>);</div><div class="line">         <span class="keyword">try</span> &#123;</div><div class="line">            <span class="keyword">synchronized</span> (activeConnections) &#123;</div><div class="line">               <span class="keyword">while</span> (activeConnections.size() &gt;= maxConnections) &#123;</div><div class="line">                  activeConnections.wait(<span class="number">2000</span>);</div><div class="line">               &#125;</div><div class="line">               activeConnections.add(conn);</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            conn.start();</div><div class="line">            conn = <span class="keyword">null</span>;</div><div class="line">         &#125;</div><div class="line">         <span class="keyword">finally</span> &#123;</div><div class="line">            <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</div><div class="line">               conn.close();</div><div class="line">               <span class="keyword">synchronized</span> (activeConnections) &#123;</div><div class="line">                  activeConnections.remove(conn);</div><div class="line">               &#125;</div><div class="line">            &#125;</div><div class="line">         &#125;</div><div class="line">      &#125;</div><div class="line">   &#125;</div></pre></td></tr></table></figure>
<p>简要概括下逻辑：</p>
<ul>
<li>当服务端收到一次存储的 request 时，会首先封装成对象 BlobServerConnection，并执行其 <code>start()</code> 方法</li>
<li>BlobServerConnection 本身也是一个 Thread，封装了具体的存储逻辑</li>
<li>会接收 3 种客户端请求：PUT/GET/DELETE，具体见：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//BlobServerConnection line111</span></div><div class="line"><span class="keyword">switch</span> (operation) &#123;</div><div class="line"><span class="keyword">case</span> PUT_OPERATION:</div><div class="line">   put(inputStream, outputStream, buffer);</div><div class="line">   <span class="keyword">break</span>;</div><div class="line"><span class="keyword">case</span> GET_OPERATION:</div><div class="line">   get(inputStream, outputStream, buffer);</div><div class="line">   <span class="keyword">break</span>;</div><div class="line"><span class="keyword">case</span> DELETE_OPERATION:</div><div class="line">   delete(inputStream, outputStream, buffer);</div><div class="line">   <span class="keyword">break</span>;</div><div class="line"><span class="keyword">default</span>:</div><div class="line">   <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Unknown operation "</span> + operation);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><em>这里重点介绍下 PUT 操作</em></p>
<ul>
<li>获取本次存储操作是否带 JobID</li>
<li>在 BlobServer 的本地 incoming 文件夹中生成临时文件：temp-[auto increment integer]</li>
<li>读取将要存储的字节长度</li>
<li>读取该长度字节存储到临时文件 temp-[auto increment integer]</li>
<li>如果带 JobID，会将临时文件移动到 JobID 对应的存储目录，并将该存储文件在 BlobStore 的对应 JobID恢复目录中备份，写 OK 消息到 Socket Client 端，最终生成的路径和文件： job-id/blob_[base64 encode key]</li>
<li>如果不带 JobID，则依据传递的消息字节数组生成一个 key：BlogKey，并存储在 cache 文件夹下，同时在 BlobStore 的 cache 文件夹下做备份，将 OK 消息和 BlobKey 写回 Socket Client，最终生成的路径和文件：cache/blob_[unique hex string]</li>
</ul>
<h3 id="BlobServer-交互协议"><a href="#BlobServer-交互协议" class="headerlink" title="BlobServer 交互协议"></a>BlobServer 交互协议</h3><p>与 BlobServer 通信的消息协议包括四段：操作类型【PUT/GET/DELETE】、存储类型【是否带 JobID】、内容长度、内容，如下图所示：</p>
<p><img src="blob-server-contact.png" alt="blob-server-contact.png"></p>
<p><em>到这里 BlobServer 就介绍完了</em></p>
<h2 id="InstanceManager"><a href="#InstanceManager" class="headerlink" title="InstanceManager"></a>InstanceManager</h2><p>flink 用来追踪当前存活的 TaskManager 的管理组件，实现比较简单，这里只简单罗列下其功能：</p>
<ul>
<li>book 下载 JobManager 中注册的所有 TaskManager</li>
<li>负责更新从 TaskManager 中上报的心跳及 metrics 信息</li>
<li>通知 InstanceListener TaskManager 的增加与死亡</li>
</ul>
<h2 id="BlobLibraryCacheManager"><a href="#BlobLibraryCacheManager" class="headerlink" title="BlobLibraryCacheManager"></a>BlobLibraryCacheManager</h2><p>flink job 的 jar 包存储服务，使用上面的 BlobServer 完成，一个 JVM 里只会存在一个 BlobLibraryCacheManager，BlobLibraryCacheManager 负责管理 BlobService【这里为BlobServer】 中存储的 jars，并存储运行时 task 对 BlobService 中 jars 的引用计数，会清理不被使用任何 task 使用的 jars。</p>
<p><em>BlobCache 负责 jars 的下载，介绍 TaskManager 的时候会详细介绍</em></p>
<p>BlobLibraryCacheManager 与 BlobService 交互，而 BlobService 负责具体的文件管理，其具体实现有两个：BlobServer 和 BlobCache，具体见下图：</p>
<p><img src="blob-service-extends-arch.png" alt="blob-service-extends-arch.png"></p>
<p>BlobServer 前面已经介绍过了，那么 BlobCache 的功能是什么呢？</p>
<p>来看 BlobCache 的构造器：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//BlobCache line60</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">BlobCache</span><span class="params">(InetSocketAddress serverAddress, Configuration configuration)</span> </span>&#123;</div><div class="line">   <span class="keyword">if</span> (serverAddress == <span class="keyword">null</span> || configuration == <span class="keyword">null</span>) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.serverAddress = serverAddress;</div><div class="line"></div><div class="line">   <span class="comment">// configure and create the storage directory</span></div><div class="line">   String storageDirectory = configuration.getString(ConfigConstants.BLOB_STORAGE_DIRECTORY_KEY, <span class="keyword">null</span>);</div><div class="line">   <span class="keyword">this</span>.storageDir = BlobUtils.initStorageDirectory(storageDirectory);</div><div class="line">   LOG.info(<span class="string">"Created BLOB cache storage directory "</span> + storageDir);</div></pre></td></tr></table></figure>
<p>这里传入的 serverAddress 其实是 BlobServer 的服务端口，在 TaskManager 中可以看到：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// start a blob service, if a blob server is specified TaskManager line940</span></div><div class="line"><span class="keyword">if</span> (blobPort &gt; <span class="number">0</span>) &#123;</div><div class="line">  val jmHost = jobManager.path.address.host.getOrElse(<span class="string">"localhost"</span>)</div><div class="line">  val address = <span class="keyword">new</span> InetSocketAddress(jmHost, blobPort)</div><div class="line"></div><div class="line">  log.info(s<span class="string">"Determined BLOB server address to be $address. Starting BLOB cache."</span>)</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    val blobcache = <span class="keyword">new</span> BlobCache(address, config.configuration)</div><div class="line">    blobService = Option(blobcache)</div><div class="line">    libraryCacheManager = Some(<span class="keyword">new</span> BlobLibraryCacheManager(blobcache, config.cleanupInterval))</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>来看 BlobCache 的核心服务方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//BlobCache line97</span></div><div class="line"><span class="function"><span class="keyword">public</span> URL <span class="title">getURL</span><span class="params">(<span class="keyword">final</span> BlobKey requiredBlob)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">   <span class="keyword">if</span> (requiredBlob == <span class="keyword">null</span>) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"BLOB key cannot be null."</span>);</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">final</span> File localJarFile = BlobUtils.getStorageLocation(storageDir, requiredBlob);</div><div class="line"></div><div class="line">   <span class="keyword">if</span> (!localJarFile.exists()) &#123;</div><div class="line"></div><div class="line">      <span class="keyword">final</span> <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[BlobServerProtocol.BUFFER_SIZE];</div><div class="line"></div><div class="line">      <span class="comment">// loop over retries</span></div><div class="line">      <span class="keyword">int</span> attempt = <span class="number">0</span>;</div><div class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line"></div><div class="line">         <span class="keyword">if</span> (attempt == <span class="number">0</span>) &#123;</div><div class="line">            LOG.info(<span class="string">"Downloading &#123;&#125; from &#123;&#125;"</span>, requiredBlob, serverAddress);</div><div class="line">         &#125; <span class="keyword">else</span> &#123;</div><div class="line">            LOG.info(<span class="string">"Downloading &#123;&#125; from &#123;&#125; (retry &#123;&#125;)"</span>, requiredBlob, serverAddress, attempt);</div><div class="line">         &#125;</div><div class="line"></div><div class="line">         <span class="keyword">try</span> &#123;</div><div class="line">            BlobClient bc = <span class="keyword">null</span>;</div><div class="line">            InputStream is = <span class="keyword">null</span>;</div><div class="line">            OutputStream os = <span class="keyword">null</span>;</div><div class="line"></div><div class="line">            <span class="keyword">try</span> &#123;</div><div class="line">               bc = <span class="keyword">new</span> BlobClient(serverAddress);</div><div class="line">               is = bc.get(requiredBlob);</div><div class="line">               os = <span class="keyword">new</span> FileOutputStream(localJarFile);</div><div class="line"></div><div class="line">               <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">                  <span class="keyword">final</span> <span class="keyword">int</span> read = is.read(buf);</div><div class="line">                  <span class="keyword">if</span> (read &lt; <span class="number">0</span>) &#123;</div><div class="line">                     <span class="keyword">break</span>;</div><div class="line">                  &#125;</div><div class="line">                  os.write(buf, <span class="number">0</span>, read);</div><div class="line">               &#125;</div><div class="line"></div><div class="line">               <span class="comment">// we do explicitly not use a finally block, because we want the closing</span></div><div class="line">               <span class="comment">// in the regular case to throw exceptions and cause the writing to fail.</span></div><div class="line">               <span class="comment">// But, the closing on exception should not throw further exceptions and</span></div><div class="line">               <span class="comment">// let us keep the root exception</span></div><div class="line">               os.close();</div><div class="line">               os = <span class="keyword">null</span>;</div><div class="line">               is.close();</div><div class="line">               is = <span class="keyword">null</span>;</div><div class="line">               bc.close();</div><div class="line">               bc = <span class="keyword">null</span>;</div><div class="line"></div><div class="line">               <span class="comment">// success, we finished</span></div><div class="line">               <span class="keyword">break</span>;</div></pre></td></tr></table></figure>
<p>简要概括下其逻辑：</p>
<ul>
<li>先从本地磁盘中获取，如果存在，直接返回</li>
<li>如果没有，生成 BlobClient 与 BlobServer 交互，并拉取文件到本地缓存，后返回本地缓存的文件句柄</li>
</ul>
<p>从这里我们可以看到 BlobCache 是 TaskManager 操作本地文件的工具，它负责从 JobManager 中的 BlobServer 同步所需的文件【jar包等】，而 BlobServer 和 BlobCache 的文件管理的入口，统一由对应 JVM 中的 BlobLibraryCacheManager 来控制【没有任务使用的 jar 定期清除等】。</p>
<p>task 拉取 jar包文件的过程如下：</p>
<p><img src="blob-server-cache-store.png" alt="blob-server-cache-store.png"></p>
<h2 id="ZooKeeperCompletedCheckpointStore"><a href="#ZooKeeperCompletedCheckpointStore" class="headerlink" title="ZooKeeperCompletedCheckpointStore"></a>ZooKeeperCompletedCheckpointStore</h2><p>flink 做 checkpoint 【有关 checkpoint 会另起一节介绍】存储的组件，负责存储已完成的 Checkpoint ，实现了接口 CompletedCheckpointStore，StandaloneCompletedCheckpointStore 和 ZooKeeperCompletedCheckpointStore 都实现了 CompletedCheckpointStore 接口，前者只在内存里存储 checkpoint，这里只介绍 ZooKeeperCompletedCheckpointStore 的实现。</p>
<p>ZooKeeperCompletedCheckpointStore 存储 checkpoint 的基本思路：</p>
<ul>
<li>先在本地磁盘持久化指定数量的 checkpoint</li>
<li>将文件句柄更新到 ZK 的特定节点下</li>
<li>滑动更新 zk 的节点存储</li>
<li>在恢复的时候只取最近一次的更新值</li>
</ul>
<p>先来看下  ZooKeeperCompletedCheckpointStore 用来和 ZK 存储交互的组件：ZooKeeperStateHandleStore，来看它的核心添加 state 的方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ZooKeeperStateHandleStore line117</span></div><div class="line"><span class="function"><span class="keyword">public</span> StateHandle&lt;T&gt; <span class="title">add</span><span class="params">(</span></span></div><div class="line">      String pathInZooKeeper,</div><div class="line">      T state,</div><div class="line">      CreateMode createMode) <span class="keyword">throws</span> Exception &#123;</div><div class="line">   checkNotNull(pathInZooKeeper, <span class="string">"Path in ZooKeeper"</span>);</div><div class="line">   checkNotNull(state, <span class="string">"State"</span>);</div><div class="line"></div><div class="line">   StateHandle&lt;T&gt; stateHandle = storage.store(state);</div><div class="line"></div><div class="line">   <span class="keyword">boolean</span> success = <span class="keyword">false</span>;</div><div class="line"></div><div class="line">   <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">// Serialize the state handle. This writes the state to the backend.</span></div><div class="line">      <span class="keyword">byte</span>[] serializedStateHandle = InstantiationUtil.serializeObject(stateHandle);</div><div class="line"></div><div class="line">      <span class="comment">// Write state handle (not the actual state) to ZooKeeper. This is expected to be</span></div><div class="line">      <span class="comment">// smaller than the state itself. This level of indirection makes sure that data in</span></div><div class="line">      <span class="comment">// ZooKeeper is small, because ZooKeeper is designed for data in the KB range, but</span></div><div class="line">      <span class="comment">// the state can be larger.</span></div><div class="line">      client.create().withMode(createMode).forPath(pathInZooKeeper, serializedStateHandle);</div><div class="line"></div><div class="line">      success = <span class="keyword">true</span>;</div><div class="line"></div><div class="line">      <span class="keyword">return</span> stateHandle;</div><div class="line">   &#125;</div><div class="line">   <span class="keyword">finally</span> &#123;</div><div class="line">      <span class="keyword">if</span> (!success) &#123;</div><div class="line">         <span class="comment">// Cleanup the state handle if it was not written to ZooKeeper.</span></div><div class="line">         <span class="keyword">if</span> (stateHandle != <span class="keyword">null</span>) &#123;</div><div class="line">            stateHandle.discardState();</div><div class="line">         &#125;</div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简要概括其逻辑：</p>
<ul>
<li>使用 StateStorageHelper 存储 state，ZK 模式下为 FileSystemStateStorageHelper，方式为直接存储到本地磁盘</li>
<li>将 state 的句柄对象 StateHandle 序列化并持久化到 ZK 的节点</li>
</ul>
<p>其在 zk 上的存储路径如下图所示：</p>
<p><img src="zk-state-handle-storage.png" width="300" height="300" alt="zk-state-handle-storage.png" align="center"></p>
<p>现在来看 ZooKeeperCompletedCheckpointStore 的核心功能：添加 checkpoint 和 从 checkpoint 做 recovery</p>
<h3 id="添加-checkpoint"><a href="#添加-checkpoint" class="headerlink" title="添加 checkpoint"></a>添加 checkpoint</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ZooKeeperCompletedCheckpointStore line190</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addCheckpoint</span><span class="params">(CompletedCheckpoint checkpoint)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">   checkNotNull(checkpoint, <span class="string">"Checkpoint"</span>);</div><div class="line"></div><div class="line">   <span class="comment">// First add the new one. If it fails, we don't want to loose existing data.</span></div><div class="line">   String path = String.format(<span class="string">"/%s"</span>, checkpoint.getCheckpointID());</div><div class="line"></div><div class="line">   <span class="keyword">final</span> StateHandle&lt;CompletedCheckpoint&gt; stateHandle = checkpointsInZooKeeper.add(path, checkpoint);</div><div class="line"></div><div class="line">   checkpointStateHandles.addLast(<span class="keyword">new</span> Tuple2&lt;&gt;(stateHandle, path));</div><div class="line"></div><div class="line">   <span class="comment">// Everything worked, let's remove a previous checkpoint if necessary.</span></div><div class="line">   <span class="keyword">if</span> (checkpointStateHandles.size() &gt; maxNumberOfCheckpointsToRetain) &#123;</div><div class="line">      removeFromZooKeeperAndDiscardCheckpoint(checkpointStateHandles.removeFirst());</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   LOG.debug(<span class="string">"Added &#123;&#125; to &#123;&#125;."</span>, checkpoint, path);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简要概括其逻辑：</p>
<ul>
<li>在本地磁盘存储该 checkpoint 的内容并返回句柄对象：StateHandle</li>
<li>以 checkpoint id 在 zk 上新建一个 node，并存储对应的序列化后的 StateHandle</li>
<li>检查存储的 checkpoint 个数是否超过限制，如果超过，删除本地磁盘及zk上最旧的数据</li>
<li>如果添加失败，已有的 checkpoint 数据不会受影响，这里 flink 想最大化保留作业的 checkpoint</li>
</ul>
<h3 id="从-checkpoint-中恢复"><a href="#从-checkpoint-中恢复" class="headerlink" title="从 checkpoint 中恢复"></a>从 checkpoint 中恢复</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ZooKeeperCompletedCheckpointStore line137</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">recover</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">   LOG.info(<span class="string">"Recovering checkpoints from ZooKeeper."</span>);</div><div class="line"></div><div class="line">   <span class="comment">// Clear local handles in order to prevent duplicates on</span></div><div class="line">   <span class="comment">// recovery. The local handles should reflect the state</span></div><div class="line">   <span class="comment">// of ZooKeeper.</span></div><div class="line">   checkpointStateHandles.clear();</div><div class="line"></div><div class="line">   <span class="comment">// Get all there is first</span></div><div class="line">   List&lt;Tuple2&lt;StateHandle&lt;CompletedCheckpoint&gt;, String&gt;&gt; initialCheckpoints;</div><div class="line">   <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">         initialCheckpoints = checkpointsInZooKeeper.getAllSortedByName();</div><div class="line">         <span class="keyword">break</span>;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">catch</span> (ConcurrentModificationException e) &#123;</div><div class="line">         LOG.warn(<span class="string">"Concurrent modification while reading from ZooKeeper. Retrying."</span>);</div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">int</span> numberOfInitialCheckpoints = initialCheckpoints.size();</div><div class="line"></div><div class="line">   LOG.info(<span class="string">"Found &#123;&#125; checkpoints in ZooKeeper."</span>, numberOfInitialCheckpoints);</div><div class="line"></div><div class="line">   <span class="keyword">if</span> (numberOfInitialCheckpoints &gt; <span class="number">0</span>) &#123;</div><div class="line">      <span class="comment">// Take the last one. This is the latest checkpoints, because path names are strictly</span></div><div class="line">      <span class="comment">// increasing (checkpoint ID).</span></div><div class="line">      Tuple2&lt;StateHandle&lt;CompletedCheckpoint&gt;, String&gt; latest = initialCheckpoints</div><div class="line">            .get(numberOfInitialCheckpoints - <span class="number">1</span>);</div><div class="line"></div><div class="line">      CompletedCheckpoint latestCheckpoint = latest.f0.getState(userClassLoader);</div><div class="line"></div><div class="line">      checkpointStateHandles.add(latest);</div><div class="line"></div><div class="line">      LOG.info(<span class="string">"Initialized with &#123;&#125;. Removing all older checkpoints."</span>, latestCheckpoint);</div><div class="line"></div><div class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numberOfInitialCheckpoints - <span class="number">1</span>; i++) &#123;</div><div class="line">         <span class="keyword">try</span> &#123;</div><div class="line">            removeFromZooKeeperAndDiscardCheckpoint(initialCheckpoints.get(i));</div><div class="line">         &#125;</div><div class="line">         <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            LOG.error(<span class="string">"Failed to discard checkpoint"</span>, e);</div><div class="line">         &#125;</div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简要概括其逻辑：</p>
<ul>
<li>清除内存中维护的句柄对象 StateHandle s</li>
<li>从 ZK 上拉取作业对应的所有的 checkpoint StateHandle 节点，并排序【从小到大】</li>
<li>获取最新的一次快照并从本地磁盘恢复 checkpoint</li>
<li>删除其余所有的 checkpoint 信息【ZK 和本地磁盘】</li>
</ul>
<p>ZooKeeperCompletedCheckpointStore 由 ZooKeeperCheckpointRecoveryFactory 负责实例化，一个 Job 会实例化一个 ZooKeeperCompletedCheckpointStore 负责快照。这里存储的只是个节点快照的句柄，并不是真正的状态数据。</p>
<p>具体的启动流程见 JobManager</p>
<p><code>line1208 val completedCheckpoints = checkpointRecoveryFactory.createCheckpointStore(jobId, userCodeLoader)</code></p>
<p><code>line1238 executionGraph.enableSnapshotCheckpointing</code></p>
<p>到这里 JobManager 的核心组件基本就介绍结束了😄</p>
]]></content>
    
    <summary type="html">
    
      JobManager 是 flink 集群的中控节点，类似于 Apache Storm 的 Nimbus 以及 Apache Spark 的 Driver 的角色，它负责作业的调度、jar 包管理、checkpoint 的协调和发起等，为了后续章节的开展，本文将介绍 flink JobManager 中所部署的一些服务
    
    </summary>
    
      <category term="Flink源码" scheme="https://danny0405.github.io/categories/Flink%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="Flink组件" scheme="https://danny0405.github.io/tags/Flink%E7%BB%84%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>flink 物理计划生成</title>
    <link href="https://danny0405.github.io/2017/02/06/flink%E7%89%A9%E7%90%86%E8%AE%A1%E5%88%92%E7%94%9F%E6%88%90/"/>
    <id>https://danny0405.github.io/2017/02/06/flink物理计划生成/</id>
    <published>2017-02-06T09:12:21.000Z</published>
    <updated>2018-01-23T06:10:27.488Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>上一节讲到业务代码<code>StreamExecutionEnvironment.execute()</code>会触发job的客户端逻辑计划<code>JobGraph</code> 的生成，之后是客户端与<code>JobManager</code>的交互过程</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ClusterClient line388</span></div><div class="line"><span class="function"><span class="keyword">public</span> JobExecutionResult <span class="title">run</span><span class="params">(JobGraph jobGraph, ClassLoader classLoader)</span> <span class="keyword">throws</span> ProgramInvocationException </span>&#123;</div><div class="line"></div><div class="line">   waitForClusterToBeReady();</div><div class="line"></div><div class="line">   <span class="keyword">final</span> LeaderRetrievalService leaderRetrievalService;</div><div class="line">   <span class="keyword">try</span> &#123;</div><div class="line">      leaderRetrievalService = LeaderRetrievalUtils.createLeaderRetrievalService(flinkConfig);</div><div class="line">   &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> ProgramInvocationException(<span class="string">"Could not create the leader retrieval service"</span>, e);</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">try</span> &#123;</div><div class="line">      logAndSysout(<span class="string">"Submitting job with JobID: "</span> + jobGraph.getJobID() + <span class="string">". Waiting for job completion."</span>);</div><div class="line">      <span class="keyword">this</span>.lastJobExecutionResult = JobClient.submitJobAndWait(actorSystemLoader.get(),</div><div class="line">         leaderRetrievalService, jobGraph, timeout, printStatusDuringExecution, classLoader);</div><div class="line">      <span class="keyword">return</span> <span class="keyword">this</span>.lastJobExecutionResult;</div><div class="line">   &#125; <span class="keyword">catch</span> (JobExecutionException e) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> ProgramInvocationException(<span class="string">"The program execution failed: "</span> + e.getMessage(), e);</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中<code>leaderRetrievalService = LeaderRetrievalUtils.createLeaderRetrievalService(flinkConfig);</code>是启动获取 leader JobManager 的服务，flink 支持 JobManager HA，需要通过 leader JobManager 获取当前的 leader JobManager，稍微介绍下这个服务：</p>
<h2 id="JobManager-Leader-选举"><a href="#JobManager-Leader-选举" class="headerlink" title="JobManager Leader 选举"></a>JobManager Leader 选举</h2><p>先来看获取 <code>LeaderRetrievalService</code>的逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//LeaderRetrievalUtils line61</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LeaderRetrievalService <span class="title">createLeaderRetrievalService</span><span class="params">(Configuration configuration)</span></span></div><div class="line">   <span class="keyword">throws</span> Exception &#123;</div><div class="line"></div><div class="line">   RecoveryMode recoveryMode = getRecoveryMode(configuration);</div><div class="line"></div><div class="line">   <span class="keyword">switch</span> (recoveryMode) &#123;</div><div class="line">      <span class="keyword">case</span> STANDALONE:</div><div class="line">         <span class="keyword">return</span> StandaloneUtils.createLeaderRetrievalService(configuration);</div><div class="line">      <span class="keyword">case</span> ZOOKEEPER:</div><div class="line">         <span class="keyword">return</span> ZooKeeperUtils.createLeaderRetrievalService(configuration);</div><div class="line">      <span class="keyword">default</span>:</div><div class="line">         <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">"Recovery mode "</span> + recoveryMode + <span class="string">" is not supported."</span>);</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>首先 flink 会依据配置获取 <code>RecoveryMode</code>，<code>RecoveryMode</code>一共两种：<em>STANDALONE</em>和<em>ZOOKEEPER</em>。如果用户配置的是<em>STANDALONE</em>，会直接去配置中获取<code>JobManager</code>的地址，这里主要介绍<code>ZOOKEEPER</code>模式下的<code>JobManager</code>leader的发现过程：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ZooKeeperUtils line141</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ZooKeeperLeaderRetrievalService <span class="title">createLeaderRetrievalService</span><span class="params">(</span></span></div><div class="line">      Configuration configuration) <span class="keyword">throws</span> Exception &#123;</div><div class="line">   CuratorFramework client = startCuratorFramework(configuration);</div><div class="line">   String leaderPath = configuration.getString(ConfigConstants.ZOOKEEPER_LEADER_PATH,</div><div class="line">         ConfigConstants.DEFAULT_ZOOKEEPER_LEADER_PATH);</div><div class="line"></div><div class="line">   <span class="keyword">return</span> <span class="keyword">new</span> ZooKeeperLeaderRetrievalService(client, leaderPath);</div><div class="line">&#125;</div><div class="line"></div><div class="line">...</div><div class="line"><span class="comment">//ZooKeeperLeaderRetrievalService line103</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">nodeChanged</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		<span class="keyword">try</span> &#123;</div><div class="line">			LOG.debug(<span class="string">"Leader node has changed."</span>);</div><div class="line"></div><div class="line">			ChildData childData = cache.getCurrentData();</div><div class="line"></div><div class="line">			String leaderAddress;</div><div class="line">			UUID leaderSessionID;</div><div class="line"></div><div class="line">			<span class="keyword">if</span> (childData == <span class="keyword">null</span>) &#123;</div><div class="line">				leaderAddress = <span class="keyword">null</span>;</div><div class="line">				leaderSessionID = <span class="keyword">null</span>;</div><div class="line">			&#125; <span class="keyword">else</span> &#123;</div><div class="line">				<span class="keyword">byte</span>[] data = childData.getData();</div><div class="line"></div><div class="line">				<span class="keyword">if</span> (data == <span class="keyword">null</span> || data.length == <span class="number">0</span>) &#123;</div><div class="line">					leaderAddress = <span class="keyword">null</span>;</div><div class="line">					leaderSessionID = <span class="keyword">null</span>;</div><div class="line">				&#125; <span class="keyword">else</span> &#123;</div><div class="line">					ByteArrayInputStream bais = <span class="keyword">new</span> ByteArrayInputStream(data);</div><div class="line">					ObjectInputStream ois = <span class="keyword">new</span> ObjectInputStream(bais);</div><div class="line"></div><div class="line">					leaderAddress = ois.readUTF();</div><div class="line">					leaderSessionID = (UUID) ois.readObject();</div></pre></td></tr></table></figure>
<p>这里 flink 会首先尝试连接 zookeeper，利用 zookeeper的leader选举服务发现leader节点的地址和当前的 sessionid，session id的作用介绍<code>JobManager</code>的时候会详细说明</p>
<h2 id="客户端JobGraph的提交"><a href="#客户端JobGraph的提交" class="headerlink" title="客户端JobGraph的提交"></a>客户端JobGraph的提交</h2><p>客户端的<code>JobGraph</code>生成之后，通过上面的<code>LeaderRetrivalService</code>获取<code>JobManager</code>的地址，接下来就是将<code>JobGraph</code>提交给<code>JobManager</code>去执行。flink 的核心进程通信是通过 Akka 来完成的，<code>JobManager</code>、<code>TaskManager</code>都是一个 Akka system，所以这里的提交首先需要生成一个客户端actor与<code>JobManager</code>交互，然后执行rpc命令，具体见：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//JobClient line98</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> JobExecutionResult <span class="title">submitJobAndWait</span><span class="params">(</span></span></div><div class="line">      ActorSystem actorSystem,</div><div class="line">      LeaderRetrievalService leaderRetrievalService,</div><div class="line">      JobGraph jobGraph,</div><div class="line">      FiniteDuration timeout,</div><div class="line">      <span class="keyword">boolean</span> sysoutLogUpdates,</div><div class="line">      ClassLoader classLoader) <span class="keyword">throws</span> JobExecutionException &#123;</div><div class="line"></div><div class="line">   ...</div><div class="line"></div><div class="line">   <span class="comment">// for this job, we create a proxy JobClientActor that deals with all communication with</span></div><div class="line">   <span class="comment">// the JobManager. It forwards the job submission, checks the success/failure responses, logs</span></div><div class="line">   <span class="comment">// update messages, watches for disconnect between client and JobManager, ...</span></div><div class="line"></div><div class="line">   Props jobClientActorProps = JobClientActor.createJobClientActorProps(</div><div class="line">      leaderRetrievalService,</div><div class="line">      timeout,</div><div class="line">      sysoutLogUpdates);</div><div class="line"></div><div class="line">   ActorRef jobClientActor = actorSystem.actorOf(jobClientActorProps);</div><div class="line"></div><div class="line">   <span class="comment">// first block handles errors while waiting for the result</span></div><div class="line">   Object answer;</div><div class="line">   <span class="keyword">try</span> &#123;</div><div class="line">      Future&lt;Object&gt; future = Patterns.ask(jobClientActor,</div><div class="line">            <span class="keyword">new</span> JobClientMessages.SubmitJobAndWait(jobGraph),</div><div class="line">            <span class="keyword">new</span> Timeout(AkkaUtils.INF_TIMEOUT()));</div><div class="line"></div><div class="line">      answer = Await.result(future, AkkaUtils.INF_TIMEOUT());</div><div class="line">   &#125;</div><div class="line">   ...</div></pre></td></tr></table></figure>
<p>在<code>JobClientActor</code>启动之前会启动<code>LeaderRetrivalService</code>，<code>LeaderRetrivalService</code>启动之后会通知它的 Listener <code>JobClientActor</code>获取<code>JobManager</code>的地址和当前 session id。之后经过消息路由跳转到提交的核心逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//JobClientActor line354</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">tryToSubmitJob</span><span class="params">(<span class="keyword">final</span> JobGraph jobGraph)</span> </span>&#123;</div><div class="line">   <span class="keyword">this</span>.jobGraph = jobGraph;</div><div class="line"></div><div class="line">   <span class="keyword">if</span> (isConnected()) &#123;</div><div class="line">      LOG.info(<span class="string">"Sending message to JobManager &#123;&#125; to submit job &#123;&#125; (&#123;&#125;) and wait for progress"</span>,</div><div class="line">         jobManager.path().toString(), jobGraph.getName(), jobGraph.getJobID());</div><div class="line"></div><div class="line">      Futures.future(<span class="keyword">new</span> Callable&lt;Object&gt;() &#123;</div><div class="line">         <span class="meta">@Override</span></div><div class="line">         <span class="function"><span class="keyword">public</span> Object <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">            ActorGateway jobManagerGateway = <span class="keyword">new</span> AkkaActorGateway(jobManager, leaderSessionID);</div><div class="line"></div><div class="line">            LOG.info(<span class="string">"Upload jar files to job manager &#123;&#125;."</span>, jobManager.path());</div><div class="line"></div><div class="line">            <span class="keyword">try</span> &#123;</div><div class="line">               jobGraph.uploadUserJars(jobManagerGateway, timeout);</div></pre></td></tr></table></figure>
<p>上面的代码有所省略😝。</p>
<p>总结下上面的过程：</p>
<img src="/2017/02/06/flink物理计划生成/jobclient-to-jobmanager.png" alt="jobclient-to-jobmanager" title="jobclient-to-jobmanager">
<ul>
<li>启动<code>JobClientActor</code>用来和<code>JobManager</code>交互</li>
<li>启动<code>LeaderRetrievalService</code>获取<code>JobManager</code>的地址</li>
<li>上传用户 jar 包</li>
<li>提交 SubmitJob 命令</li>
</ul>
<h2 id="JobManager执行计划生成"><a href="#JobManager执行计划生成" class="headerlink" title="JobManager执行计划生成"></a>JobManager执行计划生成</h2><p><code>JobManager</code>负责接收 flink 的作业，调度 task，收集 job 的状态、管理 TaskManagers。被实现为一个 akka actor。</p>
<p>客户端上传完 jar 包和<code>JobGraph</code>，flink 会进一步解析封装成运行时的执行计划<code>ExecutionGraph</code>，<code>JobManager</code>的构造器在初始化的时候传入了很多组件，这里简单列举下功能方便后面的逻辑展开，具体的细节将会在下一节讲解。</p>
<ul>
<li><code>BlobServer</code>：实现了 BOLB server，其会监听收到的 requests，并会创建 目录结构存储 BLOBS 【持久化】或者临时性的缓存他们</li>
<li><code>InstanceManager</code>：TaskManager在<code>flink</code>框架内部被叫做<code>Instance</code>，flink通过<code>InstanceManager</code>管理 flink 集群中当前所有活跃的 TaskManager，包括接收心跳，通知 InstanceListener Instance 的生成与死亡，一个典型的 <code>InstanceListener</code> 为 flink 的 Scheduler</li>
<li><code>BlobLibraryCacheManager</code>：flink job 的 jar 包存储服务，使用上面的 BlobServer 完成。</li>
<li><code>MemoryArchivist</code>备案已提交的flink作业，包括<code>JobGraph</code>、<code>ExecutionGraph</code>等</li>
<li>​</li>
<li><code>ZooKeeperCompletedCheckpointStore</code>：负责持久化 job 的 checkpoint 信息，一个 job 可以持久化多个 checkpoint，但只有最新的会被使用，具体方式为先在文件系统中持久化一份，再将文件句柄更新到 zk，并在 zk上依次递增节点路径号，zk 上保存了最近的 10 次 checkpoint</li>
<li>SavepointStore：flink 的状态存储，负责存储算子内部定义的状态，与 checkpoint 稍有区别，后者由 flink 框架来维护</li>
</ul>
<p><em>为了对<code>JobManager</code>中所起的 actors 服务有所了解，这里简单介绍下<code>JobManager</code>的启动过程</em></p>
<p>简单分析得知<code>line2049: runJobManager</code>是JobManager启动的入口，在获取<code>JobManager</code>启动的主机和端口后，变开始启动 actor system，web ui以及其他 actors：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//JobManager line2008</span></div><div class="line"><span class="function">def <span class="title">runJobManager</span><span class="params">(</span></span></div><div class="line">    configuration: Configuration,</div><div class="line">    executionMode: JobManagerMode,</div><div class="line">    listeningAddress: String,</div><div class="line">    listeningPort: Int)</div><div class="line">  : Unit = &#123;</div><div class="line"></div><div class="line">  val (jobManagerSystem, _, _, webMonitorOption, _) = startActorSystemAndJobManagerActors(</div><div class="line">    configuration,</div><div class="line">    executionMode,</div><div class="line">    listeningAddress,</div><div class="line">    listeningPort,</div><div class="line">    classOf[JobManager],</div><div class="line">    classOf[MemoryArchivist],</div><div class="line">    Option(classOf[StandaloneResourceManager])</div><div class="line">  )</div><div class="line"></div><div class="line">  <span class="comment">// block until everything is shut down</span></div><div class="line">  jobManagerSystem.awaitTermination()</div></pre></td></tr></table></figure>
<p>具体的启动逻辑在<code>startActorSystemAndJobManagerActors</code>方法中：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//JobManager line2150</span></div><div class="line"><span class="function">def <span class="title">startActorSystemAndJobManagerActors</span><span class="params">(</span></span></div><div class="line">    configuration: Configuration,</div><div class="line">    executionMode: JobManagerMode,</div><div class="line">    listeningAddress: String,</div><div class="line">    listeningPort: Int,</div><div class="line">    jobManagerClass: Class[_ &lt;: JobManager],</div><div class="line">    archiveClass: Class[_ &lt;: MemoryArchivist],</div><div class="line">    resourceManagerClass: Option[Class[_ &lt;: FlinkResourceManager[_]]])</div><div class="line">  : <span class="params">(ActorSystem, ActorRef, ActorRef, Option[WebMonitor], Option[ActorRef])</span> = &#123;</div><div class="line">  ...</div></pre></td></tr></table></figure>
<p>简单列举下逻辑：</p>
<ul>
<li>JobManager 程序的主入口，由 ApplicationMasterBase 发起</li>
<li>line 2174 使用 Json 配置 Akka 并生成 ActorSystem</li>
<li>line 2197 初始化 ZooKeeperLeaderRetrievalService，JobManager在启动的时候会以 LeaderRetrievalListener 的身份将自己注册进来，该 service 负责监听最新的 leader 信息，当发生改变时 通知所有 listener【所有的 JobManager】</li>
<li>line 2220 启动 YarnJobManager 和 MemoryArchivist actors【这里并没有启动】</li>
<li>line2268 启动【flink基本组件和JobGraph的生成一节中提到的】FlinkResourceManager</li>
<li>line 2620 createJobManagerComponents 获取以上两个组件必要的配置，并初始化相关服务 具体见【 flink JobManager 中所起的服务】这里在初始化相关组件后会初始化 JobManager，akka actorOf 方法传入的属性为构造器中参数，重载 preStart 和 postStop 方法会在 actor 启动和关闭后 相继执行，JobManager 会在这两个方法中启动和停止这些服务</li>
</ul>
<p>到这里一个完整的<code>JobManager</code> actor 便启动起来了😜</p>
<p>既然是 actor ，那么他的核心逻辑一定是各种消息的路由和处理：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//JobManager line304</span></div><div class="line">override def handleMessage: Receive = &#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">case</span> <span class="title">GrantLeadership</span><span class="params">(newLeaderSessionID)</span> </span>=&gt;</div><div class="line">    log.info(s<span class="string">"JobManager $getAddress was granted leadership with leader session ID "</span> +</div><div class="line">      s<span class="string">"$newLeaderSessionID."</span>)</div><div class="line"></div><div class="line">    leaderSessionID = newLeaderSessionID</div></pre></td></tr></table></figure>
<p>介绍下这里比较重要的几种消息：</p>
<ul>
<li>处理消息的核心方法</li>
<li>GrantLeadership 获得leader授权，将自身被分发到的 session id 写到 zookeeper，并恢复所有的 jobs.</li>
<li>RevokeLeadership 剥夺leader授权，打断清空所有的 job 信息，但是保留作业缓存，注销所有的 TaskManagers. </li>
<li>RegisterTaskManagers 注册 TaskManager，如果之前已经注册过，则只给对应的 Instance 发送消息，否则启动注册逻辑：在 InstanceManager 中注册该 Instance 的信息，并停止 Instance BlobLibraryCacheManager 的端口【供下载 lib 包用】，同时使用 watch 监听 task manager 的存活</li>
<li>SubmitJob 提交 jobGraph</li>
</ul>
<h3 id="执行计划-ExecutionGraph-的生成"><a href="#执行计划-ExecutionGraph-的生成" class="headerlink" title="执行计划 ExecutionGraph 的生成"></a>执行计划 ExecutionGraph 的生成</h3><p>flink 的运行时执行计划为 ExecutionGraph，ExecutionGraph 对应之前的 JobGraph，一个 ExecutionGraph 包含多个 ExecutionJobVertex 节点，JobGraph 的 JobVertex，每个 ExecutionJobVertex 节点的并发子 task 对应一个 ExecutionVertex，每个 ExecutionVertex 的一次 attempt 执行被抽象为一次 Execution，具体如下图所示：</p>
<img src="/2017/02/06/flink物理计划生成/flink-job-vertex-to-execution.png" alt="flink-job-vertex-to-execution.png" title="flink-job-vertex-to-execution.png">
<p><em>下面会对每个抽象做详细的介绍</em></p>
<p>ExecutionGraph 的创建是在 JobManager 接收 SubmitJob 命令后开始的，这条消息会被路由到方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//JobManager line1048</span></div><div class="line"><span class="function"><span class="keyword">private</span> def <span class="title">submitJob</span><span class="params">(jobGraph: JobGraph, jobInfo: JobInfo, isRecovery: Boolean = <span class="keyword">false</span>)</span>: Unit </span>= &#123;</div><div class="line">  <span class="keyword">if</span> (jobGraph == <span class="keyword">null</span>) &#123;</div><div class="line">    jobInfo.client ! decorateMessage(JobResultFailure(</div><div class="line">      <span class="keyword">new</span> SerializedThrowable(</div><div class="line">        <span class="keyword">new</span> JobSubmissionException(<span class="keyword">null</span>, <span class="string">"JobGraph must not be null."</span>)</div><div class="line">      )</div><div class="line">    ))</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>其逻辑总结如下：</p>
<ul>
<li>提交作业</li>
<li>具体的组件交互过程 Client.java line169 runBlocking -&gt; JobClient.java line102 submitJobAndWait -&gt; JobClientActor.java line 337 tryToSubmitJob  这里会先上传 jars 到 JobManager 的 BlobServer，然后发起提交命令</li>
<li>line1068: 设置用户lib包，使用  LibraryCacheManager book job 的jar包，由于之前包已上传，这会创建jobId 和 jars 以及class paths 的对应关系</li>
<li>line1114: 将 JobGraph 转换为 ExecutionGraph 逻辑计划转化为物理计划【后者维护 data flow 的协调执行、连接、计算中间结果】具体见章节： flink runtime</li>
<li>line 1178 ExecutionJobVertex 在此处生成，通过 JobGraph 依照数据源顺序获取下游 JobVertex，具体算法如下：</li>
</ul>
<img src="/2017/02/06/flink物理计划生成/job-graph-node-sort.png" alt="job-graph-node-sort.png" title="job-graph-node-sort.png">
<p>flink排序节点的顺序：</p>
<ul>
<li>数据源节点</li>
<li>只有一个上游的节点</li>
<li>sink节点</li>
</ul>
<p><em>例如上图的两个拓扑结构，左边节点排序完的顺序为： 1 2 3 4 5 右边的节点排序完的顺序为：1 2 3 5 4 6</em></p>
<p>那么 flink 为什么要将 JobGraph 转换为 ExecutionGraph ，并且排序这些节点呢？ExecutionGraph 代表了运行时的执行计划，包括 task 的并发、连接、中间结果的维护等，排序的目的是给 task 的部署设置先后顺序，想来也是很自然的。我们来看一下 ExecutionGraph 的构造器就能了解个大概：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">ExecutionGraph</span><span class="params">(</span></span></div><div class="line">      ExecutionContext executionContext,</div><div class="line">      JobID jobId,</div><div class="line">      String jobName,</div><div class="line">      Configuration jobConfig,</div><div class="line">      SerializedValue&lt;ExecutionConfig&gt; serializedConfig,</div><div class="line">      FiniteDuration timeout,</div><div class="line">      RestartStrategy restartStrategy,</div><div class="line">      List&lt;BlobKey&gt; requiredJarFiles,</div><div class="line">      List&lt;URL&gt; requiredClasspaths,</div><div class="line">      ClassLoader userClassLoader,</div><div class="line">      MetricGroup metricGroup) &#123;</div><div class="line"></div><div class="line">   ...</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.executionContext = executionContext;</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.jobID = jobId;</div><div class="line">   <span class="keyword">this</span>.jobName = jobName;</div><div class="line">   <span class="keyword">this</span>.jobConfiguration = jobConfig;</div><div class="line">   <span class="keyword">this</span>.userClassLoader = userClassLoader;</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.tasks = <span class="keyword">new</span> ConcurrentHashMap&lt;JobVertexID, ExecutionJobVertex&gt;();</div><div class="line">   <span class="keyword">this</span>.intermediateResults = <span class="keyword">new</span> ConcurrentHashMap&lt;IntermediateDataSetID, IntermediateResult&gt;();</div><div class="line">   <span class="keyword">this</span>.verticesInCreationOrder = <span class="keyword">new</span> ArrayList&lt;ExecutionJobVertex&gt;();</div><div class="line">   <span class="keyword">this</span>.currentExecutions = <span class="keyword">new</span> ConcurrentHashMap&lt;ExecutionAttemptID, Execution&gt;();</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.jobStatusListenerActors  = <span class="keyword">new</span> CopyOnWriteArrayList&lt;ActorGateway&gt;();</div><div class="line">   <span class="keyword">this</span>.executionListenerActors = <span class="keyword">new</span> CopyOnWriteArrayList&lt;ActorGateway&gt;();</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.stateTimestamps = <span class="keyword">new</span> <span class="keyword">long</span>[JobStatus.values().length];</div><div class="line">   <span class="keyword">this</span>.stateTimestamps[JobStatus.CREATED.ordinal()] = System.currentTimeMillis();</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.requiredJarFiles = requiredJarFiles;</div><div class="line">   <span class="keyword">this</span>.requiredClasspaths = requiredClasspaths;</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.serializedExecutionConfig = checkNotNull(serializedConfig);</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.timeout = timeout;</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.restartStrategy = restartStrategy;</div><div class="line"></div><div class="line">   metricGroup.gauge(RESTARTING_TIME_METRIC_NAME, <span class="keyword">new</span> RestartTimeGauge());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从构造器可以看出，ExecutionGraph 会维护当前的逻辑计划信息【就是有哪些task要执行】、中间结果生成信息，当前正在运行的 task，负责 job 和 task 状态切换的通知等。</p>
<h4 id="执行计划节点-ExecutionJobVertex-的生成"><a href="#执行计划节点-ExecutionJobVertex-的生成" class="headerlink" title="执行计划节点 ExecutionJobVertex 的生成"></a>执行计划节点 ExecutionJobVertex 的生成</h4><p>attachJobGraph 是 ExecutionGraph 构造图结构的核心方法，而其中最关键的逻辑是 执行节点 ExecutionJobGraph 的创建，下面详细分析下其创建过程和核心功能：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ExecutionJobVertex line95</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">ExecutionJobVertex</span><span class="params">(ExecutionGraph graph, JobVertex jobVertex,</span></span></div><div class="line">                  <span class="keyword">int</span> defaultParallelism, FiniteDuration timeout, <span class="keyword">long</span> createTimestamp)</div><div class="line">      <span class="keyword">throws</span> JobException</div><div class="line">&#123;</div><div class="line">   ...</div><div class="line">   <span class="keyword">this</span>.graph = graph;</div><div class="line">   <span class="keyword">this</span>.jobVertex = jobVertex;</div><div class="line"></div><div class="line">   <span class="keyword">int</span> vertexParallelism = jobVertex.getParallelism();</div><div class="line">   <span class="keyword">int</span> numTaskVertices = vertexParallelism &gt; <span class="number">0</span> ? vertexParallelism : defaultParallelism;</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.parallelism = numTaskVertices;</div><div class="line">   <span class="keyword">this</span>.taskVertices = <span class="keyword">new</span> ExecutionVertex[numTaskVertices];</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.inputs = <span class="keyword">new</span> ArrayList&lt;IntermediateResult&gt;(jobVertex.getInputs().size());</div><div class="line"></div><div class="line">   <span class="comment">// take the sharing group</span></div><div class="line">   <span class="keyword">this</span>.slotSharingGroup = jobVertex.getSlotSharingGroup();</div><div class="line">   <span class="keyword">this</span>.coLocationGroup = jobVertex.getCoLocationGroup();</div><div class="line">   ...</div><div class="line"></div><div class="line">   <span class="comment">// create the intermediate results</span></div><div class="line">   <span class="keyword">this</span>.producedDataSets = <span class="keyword">new</span> IntermediateResult[jobVertex.getNumberOfProducedIntermediateDataSets()];</div><div class="line"></div><div class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; jobVertex.getProducedDataSets().size(); i++) &#123;</div><div class="line">      <span class="keyword">final</span> IntermediateDataSet result = jobVertex.getProducedDataSets().get(i);</div><div class="line"></div><div class="line">      <span class="keyword">this</span>.producedDataSets[i] = <span class="keyword">new</span> IntermediateResult(</div><div class="line">            result.getId(),</div><div class="line">            <span class="keyword">this</span>,</div><div class="line">            numTaskVertices,</div><div class="line">            result.getResultType(),</div><div class="line">            result.getEagerlyDeployConsumers());</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="comment">// create all task vertices</span></div><div class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numTaskVertices; i++) &#123;</div><div class="line">      ExecutionVertex vertex = <span class="keyword">new</span> ExecutionVertex(<span class="keyword">this</span>, i, <span class="keyword">this</span>.producedDataSets, timeout, createTimestamp);</div><div class="line">      <span class="keyword">this</span>.taskVertices[i] = vertex;</div><div class="line">   &#125;</div><div class="line">   ...</div><div class="line"></div><div class="line">   <span class="comment">// set up the input splits, if the vertex has any</span></div><div class="line">   <span class="keyword">try</span> &#123;</div><div class="line">      <span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</div><div class="line">      InputSplitSource&lt;InputSplit&gt; splitSource = (InputSplitSource&lt;InputSplit&gt;) jobVertex.getInputSplitSource();</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (splitSource != <span class="keyword">null</span>) &#123;</div><div class="line">         inputSplits = splitSource.createInputSplits(numTaskVertices);</div><div class="line"></div><div class="line">         <span class="keyword">if</span> (inputSplits != <span class="keyword">null</span>) &#123;</div><div class="line">            <span class="keyword">if</span> (splitSource <span class="keyword">instanceof</span> StrictlyLocalAssignment) &#123;</div><div class="line">               inputSplitsPerSubtask = computeLocalInputSplitsPerTask(inputSplits);</div><div class="line">               splitAssigner = <span class="keyword">new</span> PredeterminedInputSplitAssigner(inputSplitsPerSubtask);</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">               splitAssigner = splitSource.getInputSplitAssigner(inputSplits);</div><div class="line">            &#125;</div><div class="line">         &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">else</span> &#123;</div><div class="line">         inputSplits = <span class="keyword">null</span>;</div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line">   <span class="keyword">catch</span> (Throwable t) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> JobException(<span class="string">"Creating the input splits caused an error: "</span> + t.getMessage(), t);</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   finishedSubtasks = <span class="keyword">new</span> <span class="keyword">boolean</span>[parallelism];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简要介绍下其构建逻辑：</p>
<ul>
<li>依据对应的 JobVetex 的并发生成对应个数的 ExecutionVertex，一个 ExecutionVertex 代表一个 ExecutionJobVertex 的并发子 task</li>
<li>设置 SlotSharingGroup 和 CoLocationGroup，这两个组件是 flink 运行时任务调度的核心抽象，会约束 flink 调度 task 的策略，在 flink 任务调度算法 一节会详细介绍</li>
<li>将原来 JobVertex 的中间结果 IntermediateDataSet 转化为 IntermediateResult，后者在前者的基础上加入了 当前正在运行的 producer 信息，是真正关于运行时中间数据的抽象</li>
<li>如果对应的 job 节点是数据源节点，会获取其 InputSplitSource，InputSplitSource 控制了数据源并发子 task 和生产的 InputSplit 的对应关系，一个 InputSplit 代表一个数据源分片，对于 flink streaming 来说，InputSplitSource 就是一个 InputFormat，对应一个输入源 task</li>
<li>这里的 InputSplitSource 是在什么时候设置进去的呢？见<code>JobManager line1163 vertex.initializeOnMaster(userCodeLoader)</code>以及<code>StreamingJobGraphGenerator.java line 278 createDataSourceVertex</code></li>
</ul>
<h4 id="执行计划节点-ExecutionJobVertex-的连接"><a href="#执行计划节点-ExecutionJobVertex-的连接" class="headerlink" title="执行计划节点 ExecutionJobVertex 的连接"></a>执行计划节点 ExecutionJobVertex 的连接</h4><p>构建完节点后通过连接生成执行计划 DAG【见ExecutionGraph attachJobGraph 方法】，connectToPredecessors 是连接执行节点的核心逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ExecutionJobGraph line237</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">connectToPredecessors</span><span class="params">(Map&lt;IntermediateDataSetID, IntermediateResult&gt; intermediateDataSets)</span> <span class="keyword">throws</span> JobException </span>&#123;</div><div class="line"></div><div class="line">   List&lt;JobEdge&gt; inputs = jobVertex.getInputs();</div><div class="line"></div><div class="line">   ...</div><div class="line"></div><div class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> num = <span class="number">0</span>; num &lt; inputs.size(); num++) &#123;</div><div class="line">      JobEdge edge = inputs.get(num);</div><div class="line"></div><div class="line">      ...</div><div class="line"></div><div class="line">      <span class="comment">// fetch the intermediate result via ID. if it does not exist, then it either has not been created, or the order</span></div><div class="line">      <span class="comment">// in which this method is called for the job vertices is not a topological order</span></div><div class="line">      IntermediateResult ires = intermediateDataSets.get(edge.getSourceId());</div><div class="line">      <span class="keyword">if</span> (ires == <span class="keyword">null</span>) &#123;</div><div class="line">         <span class="keyword">throw</span> <span class="keyword">new</span> JobException(<span class="string">"Cannot connect this job graph to the previous graph. No previous intermediate result found for ID "</span></div><div class="line">               + edge.getSourceId());</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">this</span>.inputs.add(ires);</div><div class="line"></div><div class="line">      <span class="keyword">int</span> consumerIndex = ires.registerConsumer();</div><div class="line"></div><div class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; parallelism; i++) &#123;</div><div class="line">         ExecutionVertex ev = taskVertices[i];</div><div class="line">         ev.connectSource(num, ires, edge, consumerIndex);</div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简要概括逻辑如下：</p>
<ul>
<li>设置输入 IntermediateResult</li>
<li>将自己注册到  IntermediateResult，目前一个 IntermediateResult 只支持一个 消费 ExecutionJobVertex 节点</li>
<li>设置并发子 task ExecutionVertex 和中间结果 IntermediateResult 的连接关系，通过 ExecutionVertex 的 connectSource  方法设置 ExecutionVertex 的连接策略，策略一共两种： POINT_WISE ALL_TO_ALL 前者上游 partition 与下游 consumers 之间是一对多关系，后者是 all to all 关系，这里会将 ExecutionEdge 创建出来并添加 consumer 为此 edge【partition在 new ExecutionVertex时创建出来，由 ExecutionVertex 构造器可知一个 ExecutionVertex 生产一个 partition，partition number 就是 sub task index】</li>
</ul>
<h4 id="执行节点子任务-ExecutionVertex"><a href="#执行节点子任务-ExecutionVertex" class="headerlink" title="执行节点子任务 ExecutionVertex"></a>执行节点子任务 ExecutionVertex</h4><p>先看一下 ExecutionVertex 的创建过程：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">ExecutionVertex</span><span class="params">(</span></span></div><div class="line">      ExecutionJobVertex jobVertex,</div><div class="line">      <span class="keyword">int</span> subTaskIndex,</div><div class="line">      IntermediateResult[] producedDataSets,</div><div class="line">      FiniteDuration timeout,</div><div class="line">      <span class="keyword">long</span> createTimestamp) &#123;</div><div class="line">   <span class="keyword">this</span>.jobVertex = jobVertex;</div><div class="line">   <span class="keyword">this</span>.subTaskIndex = subTaskIndex;</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.resultPartitions = <span class="keyword">new</span> LinkedHashMap&lt;IntermediateResultPartitionID, IntermediateResultPartition&gt;(producedDataSets.length, <span class="number">1</span>);</div><div class="line"></div><div class="line">   <span class="keyword">for</span> (IntermediateResult result : producedDataSets) &#123;</div><div class="line">      IntermediateResultPartition irp = <span class="keyword">new</span> IntermediateResultPartition(result, <span class="keyword">this</span>, subTaskIndex);</div><div class="line">      result.setPartition(subTaskIndex, irp);</div><div class="line"></div><div class="line">      resultPartitions.put(irp.getPartitionId(), irp);</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.inputEdges = <span class="keyword">new</span> ExecutionEdge[jobVertex.getJobVertex().getInputs().size()][];</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.priorExecutions = <span class="keyword">new</span> CopyOnWriteArrayList&lt;Execution&gt;();</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.currentExecution = <span class="keyword">new</span> Execution(</div><div class="line">      getExecutionGraph().getExecutionContext(),</div><div class="line">      <span class="keyword">this</span>,</div><div class="line">      <span class="number">0</span>,</div><div class="line">      createTimestamp,</div><div class="line">      timeout);</div><div class="line"></div><div class="line">   <span class="comment">// create a co-location scheduling hint, if necessary</span></div><div class="line">   CoLocationGroup clg = jobVertex.getCoLocationGroup();</div><div class="line">   <span class="keyword">if</span> (clg != <span class="keyword">null</span>) &#123;</div><div class="line">      <span class="keyword">this</span>.locationConstraint = clg.getLocationConstraint(subTaskIndex);</div><div class="line">   &#125;</div><div class="line">   <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">this</span>.locationConstraint = <span class="keyword">null</span>;</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.timeout = timeout;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>逻辑总结如下：</p>
<ul>
<li>依据对应的 ExecutionJobGraph 生成的中间数据集 IntermediateResult 的个数生成一定个数的 partition，这里是一个 IntermediateResult 输出一个 partition</li>
<li>生成 Execution</li>
<li>配置资源相关</li>
</ul>
<p>下面重点介绍下其连接上游 ExecutionVertex 的过程：</p>
<p>connectSource 是连接的核心逻辑，逻辑如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ExecutionVertex line250</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">connectSource</span><span class="params">(<span class="keyword">int</span> inputNumber, IntermediateResult source, JobEdge edge, <span class="keyword">int</span> consumerNumber)</span> </span>&#123;</div><div class="line"></div><div class="line">   <span class="keyword">final</span> DistributionPattern pattern = edge.getDistributionPattern();</div><div class="line">   <span class="keyword">final</span> IntermediateResultPartition[] sourcePartitions = source.getPartitions();</div><div class="line"></div><div class="line">   ExecutionEdge[] edges;</div><div class="line"></div><div class="line">   <span class="keyword">switch</span> (pattern) &#123;</div><div class="line">      <span class="keyword">case</span> POINTWISE:</div><div class="line">         edges = connectPointwise(sourcePartitions, inputNumber);</div><div class="line">         <span class="keyword">break</span>;</div><div class="line"></div><div class="line">      <span class="keyword">case</span> ALL_TO_ALL:</div><div class="line">         edges = connectAllToAll(sourcePartitions, inputNumber);</div><div class="line">         <span class="keyword">break</span>;</div><div class="line"></div><div class="line">      <span class="keyword">default</span>:</div><div class="line">         <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Unrecognized distribution pattern."</span>);</div><div class="line"></div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">this</span>.inputEdges[inputNumber] = edges;</div><div class="line"></div><div class="line">   <span class="comment">// add the consumers to the source</span></div><div class="line">   <span class="comment">// for now (until the receiver initiated handshake is in place), we need to register the</span></div><div class="line">   <span class="comment">// edges as the execution graph</span></div><div class="line">   <span class="keyword">for</span> (ExecutionEdge ee : edges) &#123;</div><div class="line">      ee.getSource().addConsumer(ee, consumerNumber);</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>逻辑总结如下：</p>
<ul>
<li>获取 JobEdge 的数据分发策略：如果非 shuffle 操作就是 DistributionPattern.POINTWISE 否则是 DistributionPattern.ALL_TO_ALL具体见代码：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//StreamingJobGraphGenerator line370</span></div><div class="line">StreamPartitioner&lt;?&gt; partitioner = edge.getPartitioner();</div><div class="line"><span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardPartitioner) &#123;</div><div class="line">   downStreamVertex.connectNewDataSetAsInput(</div><div class="line">      headVertex,</div><div class="line">      DistributionPattern.POINTWISE,</div><div class="line">      ResultPartitionType.PIPELINED,</div><div class="line">      <span class="keyword">true</span>);</div><div class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> RescalePartitioner)&#123;</div><div class="line">   downStreamVertex.connectNewDataSetAsInput(</div><div class="line">      headVertex,</div><div class="line">      DistributionPattern.POINTWISE,</div><div class="line">      ResultPartitionType.PIPELINED,</div><div class="line">      <span class="keyword">true</span>);</div><div class="line">&#125; <span class="keyword">else</span> &#123;</div><div class="line">   downStreamVertex.connectNewDataSetAsInput(</div><div class="line">         headVertex,</div><div class="line">         DistributionPattern.ALL_TO_ALL,</div><div class="line">         ResultPartitionType.PIPELINED,</div><div class="line">         <span class="keyword">true</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>按照不同的分发策略连接上游</li>
</ul>
<p>DistributionPattern.ALL_TO_ALL 就是简单的全连接，这里就不介绍了，只介绍DistributionPattern.POINTWISE 策略。</p>
<p>该策略连接 execution vertex 与上游的 partitions，会先获取上游的 partition 数与 此 ExecutionJobVertex 的并发度，如果两者并发度相等，则是 一对一 连接：</p>
<img src="/2017/02/06/flink物理计划生成/execution-vertex-one-to-one.png" alt="execution-vertex-one-to-one.png" title="execution-vertex-one-to-one.png">
<p>如果 partition 数小于 并发数 ，子 task 只会连接一个上游 partition，具体关系如下图：</p>
<img src="/2017/02/06/flink物理计划生成/execution-one-many.png" alt="execution-one-many.png" title="execution-one-many.png">
<p>如果 partition 数大于并发数，子 task 会连接多个上游 partition，具体见下图：</p>
<img src="/2017/02/06/flink物理计划生成/execution-many-one.png" alt="execution-many-one.png" title="execution-many-one.png">
<p>到这里运行时执行计划 ExecutionGraph 的生成就介绍完了😄下节将先介绍 JobManager 的核心组件</p>
]]></content>
    
    <summary type="html">
    
      flink 物理计划生成，主要是 JobManager 运行时的抽象和管理，前一章节中有介绍过 flink 的逻辑计划抽象，在运行时，flink 进一步将逻辑计划抽象成与执行节点信息紧密相关的物理计划，来对作业的调度、执行进行管理
    
    </summary>
    
      <category term="Flink源码" scheme="https://danny0405.github.io/categories/Flink%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="Flink内部实现" scheme="https://danny0405.github.io/tags/Flink%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>flink 集群构建/逻辑计划</title>
    <link href="https://danny0405.github.io/2016/12/03/Flink%E5%9F%BA%E6%9C%AC%E7%BB%84%E4%BB%B6%E5%92%8C%E9%80%BB%E8%BE%91%E8%AE%A1%E5%88%92/"/>
    <id>https://danny0405.github.io/2016/12/03/Flink基本组件和逻辑计划/</id>
    <published>2016-12-03T03:59:30.000Z</published>
    <updated>2018-01-23T06:10:15.327Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概要和背景"><a href="#概要和背景" class="headerlink" title="概要和背景"></a>概要和背景</h2><p><em>flink</em>是一个被誉为 <em>the 4th G</em> 的计算框架，不同的框架特性及其代表项目列表如下：</p>
<table>
<thead>
<tr>
<th>第一代</th>
<th>第二代</th>
<th>第三代</th>
<th>第四代</th>
</tr>
</thead>
<tbody>
<tr>
<td>Batch</td>
<td><strong>Batch</strong> <strong>Interactive</strong></td>
<td><strong>Batch</strong> <strong>Interactive</strong> <strong>Near-Real-Time</strong> <strong>Interative-processing</strong></td>
<td><strong>Hybrid</strong> <strong>Interactive</strong> <strong>Real-Time-Streaming</strong> <strong>Native-Iterative-processing</strong></td>
</tr>
<tr>
<td></td>
<td>DAG Dataflows</td>
<td>RDD</td>
<td>Cyclic Dataflows</td>
</tr>
<tr>
<td>Hadoop MapReduce</td>
<td>TEZ</td>
<td>Spark</td>
<td>Flink</td>
</tr>
</tbody>
</table>
<p>本文主要介绍<em>flink</em>的核心组件以及逻辑计划的生成过程</p>
<p><em>参考代码分支 flink-1.1.2</em></p>
<h2 id="核心组件介绍"><a href="#核心组件介绍" class="headerlink" title="核心组件介绍"></a>核心组件介绍</h2><p><em>这里只介绍 on yarn 模式下的组件</em></p>
<p><em>flink</em> 的 on yarn 模式支持两种不同的类型：</p>
<ol>
<li>单作业单集群</li>
<li>多作业单集群</li>
</ol>
<p>首先介绍 <em>单作业单集群</em> 的架构，单作业单集群下一个正常的 <em>flink</em> 程序会拥有以下组件</p>
<hr>
<p>job Cli: 非 detatched 模式下的客户端进程，用以获取 yarn Application Master 的运行状态并将日志输出到终端</p>
<p>JobManager[JM]: 负责作业的运行时计划 ExecutionGraph 的生成、物理计划生成和作业调度</p>
<p>TaskManager[TM]: 负责被分发 task 的执行、心跳/状态上报、资源管理</p>
<hr>
<p>整体的架构大致如下图所示：</p>
<img src="/2016/12/03/Flink基本组件和逻辑计划/flink-on-yarn-arch.png" alt="flink on yarn" title="flink on yarn">
<p>下面以一次 Job 的提交过程描述 <em>flink</em> 各组件的作用及协同</p>
<h3 id="作业提交流程分析"><a href="#作业提交流程分析" class="headerlink" title="作业提交流程分析"></a>作业提交流程分析</h3><p>单作业单集群模式下，一个作业会启动一个 JM，并依据用户的参数传递启动相应数量的 TM，每个 TM 运行在 yarn 的一个 container 中，</p>
<p>一个通常的 flink on yarn 提交命令：<br><code>./bin/flink run -m yarn-cluster -yn 2 -j flink-demo-1.0.0-with-dependencies.jar —ytm 1024 -yst 4 -yjm 1024 —yarnname flink_demo_waimai_e</code><br><em>flink</em> 在收到这样一条命令后首先会通过 Cli 获取 flink 的配置，并解析命令行参数。</p>
<h4 id="配置加载"><a href="#配置加载" class="headerlink" title="配置加载"></a>配置加载</h4><p><code>CliFrontend.java</code> 是 flink 提交作业的入口</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//CliFrontend line144</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">CliFrontend</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">   <span class="keyword">this</span>(getConfigurationDirectoryFromEnv());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里会尝试加载 conf 文件夹下的所有 yaml 文件，配置文件的命名并没有强制限制</p>
<h4 id="参数解析"><a href="#参数解析" class="headerlink" title="参数解析"></a>参数解析</h4><p>解析命令行参数的第一步是路由用户的命令，然后交由<code>run</code>方法去处理</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//CliFrontend line993</span></div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">return</span> SecurityUtils.runSecured(<span class="keyword">new</span> SecurityUtils.FlinkSecuredRunner&lt;Integer&gt;() &#123;</div><div class="line">	    <span class="function">Override</span></div><div class="line">	    <span class="keyword">public</span> Integer <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</div><div class="line">	        <span class="keyword">return</span> CliFrontend.<span class="keyword">this</span>.run(params);</div><div class="line">		&#125;);</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">		<span class="keyword">return</span> handleError(e);</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>接下来是程序参数设置过程，<em>flink</em> 将 jar包路径和参数配置封装成了 <code>PackagedProgram</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//CliFrontend line223</span></div><div class="line">PackagedProgram program;</div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">   LOG.info(<span class="string">"Building program from JAR file"</span>);</div><div class="line">   program = buildProgram(options);</div><div class="line">&#125;</div><div class="line"><span class="keyword">catch</span> (Throwable t) &#123;</div><div class="line">   <span class="keyword">return</span> handleError(t);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="flink集群的构建"><a href="#flink集群的构建" class="headerlink" title="flink集群的构建"></a>flink集群的构建</h4><h5 id="集群类型的解析"><a href="#集群类型的解析" class="headerlink" title="集群类型的解析"></a>集群类型的解析</h5><p>获取参数后下一步就是集群的构建和部署，flink 通过 两个不同的 <code>CustomCommandLine</code> 来实现不同集群模式的解析，分别是 <code>FlinkYarnSessionCli</code>和 <code>DefaultCLI</code> 【吐槽一下 flink 类名的命名规范】解析命令行参数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//CliFrontend line125</span></div><div class="line"><span class="keyword">static</span> &#123;</div><div class="line">   <span class="comment">/** command line interface of the YARN session, with a special initialization here</span></div><div class="line">    *  to prefix all options with y/yarn. */</div><div class="line">   loadCustomCommandLine(<span class="string">"org.apache.flink.yarn.cli.FlinkYarnSessionCli"</span>, <span class="string">"y"</span>, <span class="string">"yarn"</span>);</div><div class="line">   customCommandLine.add(<span class="keyword">new</span> DefaultCLI());</div><div class="line">&#125;</div><div class="line">...</div><div class="line"><span class="comment">//line882 这里将决定Cli的类型</span></div><div class="line">CustomCommandLine&lt;?&gt; activeCommandLine = getActiveCustomCommandLine(options.getCommandLine());</div></pre></td></tr></table></figure>
<p>那么什么时候解析成 Yarn Cluster 什么时候解析成 Standalone 呢？由于<code>FlinkYarnSessionCli</code>被优先添加到<code>customCommandLine</code>,所以会先触发下面这段逻辑</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//FlinkYarnSessionCli line469</span></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isActive</span><span class="params">(CommandLine commandLine, Configuration configuration)</span> </span>&#123;</div><div class="line">   String jobManagerOption = commandLine.getOptionValue(ADDRESS_OPTION.getOpt(), <span class="keyword">null</span>);</div><div class="line">   <span class="keyword">boolean</span> yarnJobManager = ID.equals(jobManagerOption);</div><div class="line">   <span class="keyword">boolean</span> yarnAppId = commandLine.hasOption(APPLICATION_ID.getOpt());</div><div class="line">   <span class="keyword">return</span> yarnJobManager || yarnAppId || loadYarnPropertiesFile(commandLine, configuration) != <span class="keyword">null</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面可以看出如果用户传入了 <code>-m</code>参数或者<code>application id</code>或者配置了yarn properties 文件，则启动yarn cluster模式，否则是Standalone模式的集群</p>
<h5 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h5><p>flink通过<code>YarnClusterDescriptor</code>来描述yarn集群的部署配置，具体对应的配置文件为<code>flink-conf.yaml</code>，通过下面这段逻辑触发集群部署：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//AbstractYarnClusterDescriptor line372</span></div><div class="line"><span class="comment">/**</span></div><div class="line"> * This method will block until the ApplicationMaster/JobManager have been</div><div class="line"> * deployed on YARN.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">protected</span> YarnClusterClient <span class="title">deployInternal</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div></pre></td></tr></table></figure>
<p>大致列下过程：</p>
<ul>
<li>check yarn 集群队列资源是否满足请求</li>
<li>设置 AM Context、启动命令、submission context</li>
<li>如果开启高可用模式【通过反射调用 submission context 的两个方法修改属性】 keepContainersMethod    attemptFailuresValidityIntervalMethod 【和 Hadoop 的版本有关】第一个属性表示应用重试时是否保留 AM container，第二个属性表示 指定 间隔时间之内应用允许失败重启的次数</li>
<li>上传 用户 jar、flink-conf.yaml、lib 目录下所有的 jar 包、logback log4j配置文件 到 HDFS</li>
<li>通过 yarn client submit am context</li>
<li>将yarn client 及相关配置封装成 YarnClusterClient 返回</li>
</ul>
<p>真正在 AM 中运行的主类是 <code>YarnApplicationMasterRunner</code>，它的 <code>run</code>方法做了如下工作：</p>
<ul>
<li>启动JobManager ActorSystem</li>
<li>启动 flink ui</li>
<li>启动<code>YarnFlinkResourceManager</code>来负责与yarn的ResourceManager交互，管理yarn资源</li>
<li>启动 actor System supervise 进程</li>
</ul>
<p>到这里 JobManager 已经启动起来，那么 TaskManager是什么时候起动的呢？</p>
<p>在 <code>YarnFlinkResourceManager</code>启动的时候会预先执行一段逻辑【Akka actor的 preStart 方法】：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">preStart</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="comment">// we start our leader retrieval service to make sure we get informed</span></div><div class="line">        <span class="comment">// about JobManager leader changes</span></div><div class="line">        leaderRetriever.start(<span class="keyword">new</span> LeaderRetrievalListener() &#123;</div><div class="line"></div><div class="line">		    <span class="meta">@Override</span></div><div class="line">		    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">notifyLeaderAddress</span><span class="params">(String leaderAddress, UUID leaderSessionID)</span> </span>&#123;</div><div class="line">		        self().tell(</div><div class="line">						<span class="keyword">new</span> NewLeaderAvailable(leaderAddress, leaderSessionID),</div><div class="line">						ActorRef.noSender());</div><div class="line">		    &#125;</div></pre></td></tr></table></figure>
<p>这段逻辑会先尝试获取 JobManager 的地址并给自己发送一个路由消息<code>NewLeaderAvailable</code>，然后<code>YarnFlinkResourceManager</code>会把自己注册到 <code>JobManager</code> 中，接着<code>JobManager</code>会发送一个回调命令：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//JobManager line358</span></div><div class="line">sender ! decorateMessage(<span class="keyword">new</span> <span class="type">RegisterResourceManagerSuccessful</span>(self, taskManagerResources))</div></pre></td></tr></table></figure>
<p>接着会触发这样一段逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//FlinkResourceManager line555</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">checkWorkersPool</span><span class="params">()</span> </span>&#123;</div><div class="line">   <span class="keyword">int</span> numWorkersPending = getNumWorkerRequestsPending();</div><div class="line">   <span class="keyword">int</span> numWorkersPendingRegistration = getNumWorkersPendingRegistration();</div><div class="line"></div><div class="line">   <span class="comment">// sanity checks</span></div><div class="line">   Preconditions.checkState(numWorkersPending &gt;= <span class="number">0</span>,</div><div class="line">      <span class="string">"Number of pending workers should never be below 0."</span>);</div><div class="line">   Preconditions.checkState(numWorkersPendingRegistration &gt;= <span class="number">0</span>,</div><div class="line">      <span class="string">"Number of pending workers pending registration should never be below 0."</span>);</div><div class="line"></div><div class="line">   <span class="comment">// see how many workers we want, and whether we have enough</span></div><div class="line">   <span class="keyword">int</span> allAvailableAndPending = startedWorkers.size() +</div><div class="line">      numWorkersPending + numWorkersPendingRegistration;</div><div class="line"></div><div class="line">   <span class="keyword">int</span> missing = designatedPoolSize - allAvailableAndPending;</div><div class="line"></div><div class="line">   <span class="keyword">if</span> (missing &gt; <span class="number">0</span>) &#123;</div><div class="line">      requestNewWorkers(missing);</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>将所有的 TS 起动起来，这样一个 flink 集群便构建出来了。下面附图解释下这个流程：</p>
<img src="/2016/12/03/Flink基本组件和逻辑计划/flink-cluster-start-flow.png" alt="flink-cluster-start-flow" title="flink-cluster-start-flow">
<ol>
<li>flink cli 解析本地环境配置，启动 <code>ApplicationMaster</code></li>
<li>在 <code>ApplicationMaster</code> 中启动 <code>JobManager</code></li>
<li>在 <code>ApplicationMaster</code> 中启动<code>YarnFlinkResourceManager</code></li>
<li><code>YarnFlinkResourceManager</code>给<code>JobManager</code>发送注册信息</li>
<li><code>YarnFlinkResourceManager</code>注册成功后，<code>JobManager</code>给<code>YarnFlinkResourceManager</code>发送注册成功信息</li>
<li><code>YarnFlinkResourceManage</code>知道自己注册成功后像<code>ResourceManager</code>申请和<code>TaskManager</code>数量对等的 container</li>
<li>在container中启动<code>TaskManager</code></li>
<li><code>TaskManager</code>将自己注册到<code>JobManager</code>中</li>
</ol>
<p><em>接下来便是程序的提交和运行</em></p>
<p>程序在<code>CliFrontend</code>中被提交后，会触发这样一段逻辑</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ClusterClient 304</span></div><div class="line">	<span class="function"><span class="keyword">public</span> JobSubmissionResult <span class="title">run</span><span class="params">(PackagedProgram prog, <span class="keyword">int</span> parallelism)</span></span></div><div class="line">			<span class="keyword">throws</span> ProgramInvocationException</div><div class="line">	&#123;</div><div class="line">		Thread.currentThread().setContextClassLoader(prog.getUserCodeClassLoader());</div><div class="line">		...</div><div class="line">		<span class="keyword">else</span> <span class="keyword">if</span> (prog.isUsingInteractiveMode()) &#123;</div><div class="line">			LOG.info(<span class="string">"Starting program in interactive mode"</span>);</div><div class="line">			ContextEnvironmentFactory factory = <span class="keyword">new</span> ContextEnvironmentFactory(<span class="keyword">this</span>, prog.getAllLibraries(),</div><div class="line">					prog.getClasspaths(), prog.getUserCodeClassLoader(), parallelism, isDetached(),</div><div class="line">					prog.getSavepointPath());</div><div class="line">			ContextEnvironment.setAsContext(factory);</div><div class="line"></div><div class="line">			<span class="keyword">try</span> &#123;</div><div class="line">				<span class="comment">// invoke main method</span></div><div class="line">				prog.invokeInteractiveModeForExecution();</div><div class="line">				...</div><div class="line">			&#125;</div><div class="line">			<span class="keyword">finally</span> &#123;</div><div class="line">				ContextEnvironment.unsetContext();</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">else</span> &#123;</div><div class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"PackagedProgram does not have a valid invocation mode."</span>);</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>注意到有一段<code>prog.invokeInteractiveModeForExecution()</code>，这是客户端生成初步逻辑计划的核心逻辑，下面将详细介绍</p>
<h3 id="客户端逻辑计划"><a href="#客户端逻辑计划" class="headerlink" title="客户端逻辑计划"></a>客户端逻辑计划</h3><p>上面提到<code>prog.invokeInteractiveModeForExecution()</code>这段逻辑会触发客户端逻辑计划的生成，那么是怎样一个过程呢？其实这里只是调用了用户jar包的主函数，真正的触发生成过程由用户代码的执行来完成。例如用户写了这样一段 flink 代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">object FlinkDemo extends App with Logging&#123;</div><div class="line">  <span class="function">override def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>=&#123;</div><div class="line">    val properties = <span class="keyword">new</span> Properties</div><div class="line">    properties.setProperty(<span class="string">"bootstrap.servers"</span>, DemoConfig.kafkaBrokerList)</div><div class="line"></div><div class="line"> properties.setProperty(<span class="string">"zookeeper.connect"</span>,<span class="string">"host01:2181,host02:2181,host03:2181/kafka08"</span>)</div><div class="line">    properties.setProperty(<span class="string">"group.id"</span>, <span class="string">"flink-demo-waimai-e"</span>)</div><div class="line"></div><div class="line">    val env = StreamExecutionEnvironment.getExecutionEnvironment</div><div class="line">    env.enableCheckpointing(<span class="number">5000L</span>, CheckpointingMode.EXACTLY_ONCE) <span class="comment">//checkpoint every 5 seconds.</span></div><div class="line">    val stream = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer08[String](<span class="string">"log.waimai_e"</span>, <span class="keyword">new</span> SimpleStringSchema, properties)).setParallelism(<span class="number">2</span>)</div><div class="line">    val counts = stream.name(<span class="string">"log.waimai_e"</span>).map(toPoiIdTuple(_)).filter(_._2 != <span class="keyword">null</span>)</div><div class="line">      .keyBy(<span class="number">0</span>)</div><div class="line">      .timeWindow(Time.seconds(<span class="number">5</span>))</div><div class="line">      .sum(<span class="number">1</span>)</div><div class="line"></div><div class="line">    counts.addSink(sendToKafka(_))</div><div class="line">    env.execute()</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>注意到这样一段<code>val env = StreamExecutionEnvironment.getExecutionEnvironment</code>，这段代码会获取客户端的环境配置，它首先会转到这样一段逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//StreamExecutionEnvironment 1256</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> StreamExecutionEnvironment <span class="title">getExecutionEnvironment</span><span class="params">()</span> </span>&#123;</div><div class="line">   <span class="keyword">if</span> (contextEnvironmentFactory != <span class="keyword">null</span>) &#123;</div><div class="line">      <span class="keyword">return</span> contextEnvironmentFactory.createExecutionEnvironment();</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="comment">// because the streaming project depends on "flink-clients" (and not the other way around)</span></div><div class="line">   <span class="comment">// we currently need to intercept the data set environment and create a dependent stream env.</span></div><div class="line">   <span class="comment">// this should be fixed once we rework the project dependencies</span></div><div class="line"></div><div class="line">   ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</div></pre></td></tr></table></figure>
<p><code>ExecutionEnvironment.getExecutionEnvironment();</code>获取环境的逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ExecutionEnvironment line1137</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutionEnvironment <span class="title">getExecutionEnvironment</span><span class="params">()</span> </span>&#123;</div><div class="line">   <span class="keyword">return</span> contextEnvironmentFactory == <span class="keyword">null</span> ?</div><div class="line">         createLocalEnvironment() : contextEnvironmentFactory.createExecutionEnvironment();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里的<code>contextEnvironmentFactory</code>是一个静态成员，早在<code>ContextEnvironment.setAsContext(factory)</code>已经触发过初始化了，其中包含了如下的环境信息:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ContextEnvironmentFactory line51</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">ContextEnvironmentFactory</span><span class="params">(ClusterClient client, List&lt;URL&gt; jarFilesToAttach,</span></span></div><div class="line">      List&lt;URL&gt; classpathsToAttach, ClassLoader userCodeClassLoader, <span class="keyword">int</span> defaultParallelism,</div><div class="line">      <span class="keyword">boolean</span> isDetached, String savepointPath)</div><div class="line">&#123;</div><div class="line">   <span class="keyword">this</span>.client = client;</div><div class="line">   <span class="keyword">this</span>.jarFilesToAttach = jarFilesToAttach;</div><div class="line">   <span class="keyword">this</span>.classpathsToAttach = classpathsToAttach;</div><div class="line">   <span class="keyword">this</span>.userCodeClassLoader = userCodeClassLoader;</div><div class="line">   <span class="keyword">this</span>.defaultParallelism = defaultParallelism;</div><div class="line">   <span class="keyword">this</span>.isDetached = isDetached;</div><div class="line">   <span class="keyword">this</span>.savepointPath = savepointPath;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中的 client 就是上面生成的 <code>YarnClusterClient</code>，其它的意思较明显，就不多做解释了。</p>
<p>用户在执行<code>val env = StreamExecutionEnvironment.getExecutionEnvironment</code>这样一段逻辑后会得到一个<code>StreamContextEnvironment</code>，其中封装了 streaming 的一些执行配置 【buffer time out等】，另外保存了上面提到的 <code>ContextEnvironmen</code>t 的引用。</p>
<p>到这里关于 streaming 需要的执行环境信息已经设置完成。</p>
<h4 id="初步逻辑计划-StreamGraph-的生成"><a href="#初步逻辑计划-StreamGraph-的生成" class="headerlink" title="初步逻辑计划 StreamGraph 的生成"></a>初步逻辑计划 StreamGraph 的生成</h4><p>接下来用户代码执行到<code>val stream = env.addSource(new FlinkKafkaConsumer08</code>，这段逻辑实际会生成一个<code>DataStream</code>抽象，<code>DataStream</code>是flink关于streaming抽象的最核心抽象，后续所有的算子转换都会在<code>DataStream</code>上来完成，上面的<code>addSource</code>操作会触发下面这段逻辑:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> &lt;OUT&gt; <span class="function">DataStreamSource&lt;OUT&gt; <span class="title">addSource</span><span class="params">(SourceFunction&lt;OUT&gt; function, String sourceName, TypeInformation&lt;OUT&gt; typeInfo)</span> </span>&#123;</div><div class="line"></div><div class="line">   <span class="keyword">if</span>(typeInfo == <span class="keyword">null</span>) &#123;</div><div class="line">      <span class="keyword">if</span> (function <span class="keyword">instanceof</span> ResultTypeQueryable) &#123;</div><div class="line">         typeInfo = ((ResultTypeQueryable&lt;OUT&gt;) function).getProducedType();</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">         <span class="keyword">try</span> &#123;</div><div class="line">            typeInfo = TypeExtractor.createTypeInfo(</div><div class="line">                  SourceFunction.class,</div><div class="line">                  function.getClass(), <span class="number">0</span>, <span class="keyword">null</span>, <span class="keyword">null</span>);</div><div class="line">         &#125; <span class="keyword">catch</span> (<span class="keyword">final</span> InvalidTypesException e) &#123;</div><div class="line">            typeInfo = (TypeInformation&lt;OUT&gt;) <span class="keyword">new</span> MissingTypeInfo(sourceName, e);</div><div class="line">         &#125;</div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">boolean</span> isParallel = function <span class="keyword">instanceof</span> ParallelSourceFunction;</div><div class="line"></div><div class="line">   clean(function);</div><div class="line">   StreamSource&lt;OUT, ?&gt; sourceOperator;</div><div class="line">   <span class="keyword">if</span> (function <span class="keyword">instanceof</span> StoppableFunction) &#123;</div><div class="line">      sourceOperator = <span class="keyword">new</span> StoppableStreamSource&lt;&gt;(cast2StoppableSourceFunction(function));</div><div class="line">   &#125; <span class="keyword">else</span> &#123;</div><div class="line">      sourceOperator = <span class="keyword">new</span> StreamSource&lt;&gt;(function);</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">return</span> <span class="keyword">new</span> DataStreamSource&lt;&gt;(<span class="keyword">this</span>, typeInfo, sourceOperator, isParallel, sourceName);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简要总结下上面的逻辑：</p>
<ul>
<li>获取数据源 source 的 output 信息 TypeInformation</li>
<li>生成 StreamSource sourceOperator</li>
<li>生成 DataStreamSource【封装了 sourceOperator】，并返回</li>
<li>将 StreamTransformation 添加到算子列表 transformations 中【只有 转换 transform 操作才会添加算子，其它都只是暂时做了 transformation 的叠加封装】</li>
<li>后续会在 DataStream 上做操作</li>
</ul>
<p>该输出<code>DataStreamSource</code>继承自<code>SingleOutputStreamOperator</code>具体的继承关系如下：</p>
<img src="/2016/12/03/Flink基本组件和逻辑计划/flink-datastream-extend.png" alt="flink-datastream-extend" title="flink-datastream-extend">
<p>而生成的 StreamSource operator 走的是另一套继承接口：</p>
<img src="/2016/12/03/Flink基本组件和逻辑计划/stream-operator-extend.png" alt="stream-operator-extend" title="stream-operator-extend">
<p>DataStreamSource 是一个 DataStream <strong>数据流</strong>抽象，StreamSource 是一个 StreamOperator <strong>算子</strong>抽象，在 flink 中一个 DataStream 封装了一次数据流转换，一个 StreamOperator 封装了一个函数接口，比如 map、reduce、keyBy等。<em>关于算子的介绍会另起一节：flink算子的生命周期</em></p>
<p>可以看到在 DataStream 上可以进行一系列的操作(map filter 等)，来看一个常规操作比如 map 会发生什么：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//DataStream line503</span></div><div class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">map</span><span class="params">(MapFunction&lt;T, R&gt; mapper)</span> </span>&#123;</div><div class="line"></div><div class="line">   TypeInformation&lt;R&gt; outType = TypeExtractor.getMapReturnTypes(clean(mapper), getType(),</div><div class="line">         Utils.getCallLocationName(), <span class="keyword">true</span>);</div><div class="line"></div><div class="line">   <span class="keyword">return</span> transform(<span class="string">"Map"</span>, outType, <span class="keyword">new</span> StreamMap&lt;&gt;(clean(mapper)));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>一个map操作会触发一次 transform，那么transform做了什么工作呢？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//DataStream line1020</span></div><div class="line"><span class="meta">@PublicEvolving</span></div><div class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">transform</span><span class="params">(String operatorName, TypeInformation&lt;R&gt; outTypeInfo, OneInputStreamOperator&lt;T, R&gt; operator)</span> </span>&#123;</div><div class="line"></div><div class="line">   <span class="comment">// read the output type of the input Transform to coax out errors about MissingTypeInfo</span></div><div class="line">   transformation.getOutputType();</div><div class="line"></div><div class="line">   OneInputTransformation&lt;T, R&gt; resultTransform = <span class="keyword">new</span> OneInputTransformation&lt;&gt;(</div><div class="line">         <span class="keyword">this</span>.transformation,</div><div class="line">         operatorName,</div><div class="line">         operator,</div><div class="line">         outTypeInfo,</div><div class="line">         environment.getParallelism());</div><div class="line"></div><div class="line">   <span class="meta">@SuppressWarnings</span>(&#123; <span class="string">"unchecked"</span>, <span class="string">"rawtypes"</span> &#125;)</div><div class="line">   SingleOutputStreamOperator&lt;R&gt; returnStream = <span class="keyword">new</span> SingleOutputStreamOperator(environment, resultTransform);</div><div class="line"></div><div class="line">   getExecutionEnvironment().addOperator(resultTransform);</div><div class="line"></div><div class="line">   <span class="keyword">return</span> returnStream;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>transform()</code> 生成了一个 <code>StreamTransformation</code>并以此作为成员变量封装成另一个 DataStream 返回，<code>StreamTransformation</code>是 flink关于数据流转换的核心抽象，只有需要 transform 的流才会生成新的DataStream 算子，后面会详细解释，注意上面有这一行<code>getExecutionEnvironment().addOperator(resultTransform)</code>flink会将transformation维护起来：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//StreamExecutionEnvironment line 1237</span></div><div class="line"><span class="meta">@Internal</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addOperator</span><span class="params">(StreamTransformation&lt;?&gt; transformation)</span> </span>&#123;</div><div class="line">   Preconditions.checkNotNull(transformation, <span class="string">"transformation must not be null."</span>);</div><div class="line">   <span class="keyword">this</span>.transformations.add(transformation);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>所以，用户的一连串操作 map join 等实际上在 DataStream 上做了转换，并且flink将这些 <code>StreamTransformation</code> 维护起来，一直到最后，用户执行 <code>env.execute()</code>这样一段逻辑，StreamGraph 的构建才算真正开始…</p>
<p>用户在执行<code>env.execute()</code>会触发这样一段逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//StreamContextEnvironment line51   </span></div><div class="line"><span class="function"><span class="keyword">public</span> JobExecutionResult <span class="title">execute</span><span class="params">(String jobName)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">      Preconditions.checkNotNull(<span class="string">"Streaming Job name should not be null."</span>);</div><div class="line"></div><div class="line">      StreamGraph streamGraph = <span class="keyword">this</span>.getStreamGraph();</div><div class="line">      streamGraph.setJobName(jobName);</div><div class="line"></div><div class="line">      transformations.clear();</div><div class="line"></div><div class="line">      <span class="comment">// execute the programs</span></div><div class="line">      <span class="keyword">if</span> (ctx <span class="keyword">instanceof</span> DetachedEnvironment) &#123;</div><div class="line">         LOG.warn(<span class="string">"Job was executed in detached mode, the results will be available on completion."</span>);</div><div class="line">         ((DetachedEnvironment) ctx).setDetachedPlan(streamGraph);</div><div class="line">         <span class="keyword">return</span> DetachedEnvironment.DetachedJobExecutionResult.INSTANCE;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">         <span class="keyword">return</span> ctx.getClient().runBlocking(streamGraph, ctx.getJars(), ctx.getClasspaths(), ctx.getUserCodeClassLoader(), ctx.getSavepointPath());</div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这段代码做了两件事情：</p>
<ul>
<li>首先使用 <code>StreamGraphGenerator</code> 产生 StreamGraph</li>
<li>使用 Client 运行 stream graph</li>
</ul>
<p>那么<code>StreamGraphGenerator</code> 做了哪些操作呢？</p>
<p><code>StreamGraphGenerator</code>会依据添加算子时保存的 transformations 信息生成 job graph 中的节点，并创建节点连接，分流操作 如 union,select,split 不会添加边，只会创建虚拟节点或在上有节点添加 selector</p>
<p>这里会将 StreamTransformation 转换为 StreamNode，StreamNode 保存了算子的信息【会另外介绍】，如下图所示</p>
<p><img src="./transformation-to-node.png" width="535" height="300" alt="transformation-to-node.png" align="center"></p>
<p>到这里由 <code>StreamNode</code> 构成的 DAG 图 <code>StreamGraph</code>就生成了</p>
<p>不过 在提交给 client 的时候，flink 会做进一步的优化:</p>
<p> <code>StreamGraph</code> 将进一步转换为 <code>JobGraph</code>，这一步工作由 <code>StreamingJobGraphGenerator</code> 来完成，为什么要做这一步转换呢？主要因为有可以 chain 的算子，这里进一步将 StreamNode 转换为 JobVertex，主要工作是将可以 chain 的算子合并【这一步优化是默认打开的】，并设置资源，重试策略等，最终生成可以提交给 JobManager 的 JobGraph</p>
<h4 id="优化的逻辑计划-JobGraph-的生成"><a href="#优化的逻辑计划-JobGraph-的生成" class="headerlink" title="优化的逻辑计划 JobGraph 的生成"></a>优化的逻辑计划 JobGraph 的生成</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//StreamingJobGraphGenerator line181</span></div><div class="line"><span class="function"><span class="keyword">private</span> List&lt;StreamEdge&gt; <span class="title">createChain</span><span class="params">(</span></span></div><div class="line">      Integer startNodeId,</div><div class="line">      Integer currentNodeId,</div><div class="line">      Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes) &#123;</div><div class="line"></div><div class="line">   <span class="keyword">if</span> (!builtVertices.contains(startNodeId)) &#123;</div><div class="line"></div><div class="line">      List&lt;StreamEdge&gt; transitiveOutEdges = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</div><div class="line"></div><div class="line">      List&lt;StreamEdge&gt; chainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</div><div class="line">      List&lt;StreamEdge&gt; nonChainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</div><div class="line"></div><div class="line">      <span class="keyword">for</span> (StreamEdge outEdge : streamGraph.getStreamNode(currentNodeId).getOutEdges()) &#123;</div><div class="line">         <span class="keyword">if</span> (isChainable(outEdge)) &#123;</div><div class="line">            chainableOutputs.add(outEdge);</div><div class="line">         &#125; <span class="keyword">else</span> &#123;</div><div class="line">            nonChainableOutputs.add(outEdge);</div><div class="line">         &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">for</span> (StreamEdge chainable : chainableOutputs) &#123;</div><div class="line">         transitiveOutEdges.addAll(createChain(startNodeId, chainable.getTargetId(), hashes));</div><div class="line">      &#125;</div><div class="line">     ...</div></pre></td></tr></table></figure>
<p>上面的方法是算子 chain 的核心操作，简要概括下：</p>
<ul>
<li>如果从此 start node 开始未生成过 JobVertex，则执行 chain逻辑，由于是递归操作，会先深度优先遍历，将源节点开始到第一个不可 chain 的 StreamNode 之间的算子做 chain 操作【先算叶子节点的 chain，依次往根节点计算】</li>
<li>line 207 遇到不可 chain 的边，开始深度遍历生成 JobVertex</li>
<li>line 216 将 StreamNode 的输入输出配置，包括序列化配置等设置到上面的 StreamingConfig 中，并在 vertexConfigs 中保存起来，如果是 新生成的 JobVertex，起对应的 StreamingConfig 会以 start node id 为 key 进行保存</li>
<li>transitiveOutEdges 保存的该节点下游所有的 non chain_able  edges，最终的方法会返回此数据结构</li>
<li>连接 start node 和所有的 transitiveOutEdges 【在输入 JobVertex 创建 IntermediateDataSet，partition类型为 pipeline，生成 JobEdge】</li>
<li>如果是新生成JobVertex，继续设置config，包括 chain start，所有物理输出，及直接逻辑输出、chained config等</li>
<li>如果不是新生成 JobVertex，直接chain configs</li>
</ul>
<p>这里总结下JobGraph的构建过程，见下图:</p>
<img src="/2016/12/03/Flink基本组件和逻辑计划/flink-job-graph-create.png" alt="flink-job-graph-create" title="flink-job-graph-create">
<p>大致过程总结如下：</p>
<ul>
<li>由<code>DataStream</code>上的操作生成<code>StreamTransformation</code>列表</li>
<li>从<code>StreamTransformation</code>的生成关系创建<code>StreamNode</code>和<code>StreamEdge</code></li>
<li>做算子chain，合并成 <code>JobVertex</code>，并生成 <code>JobEdge</code></li>
</ul>
<p>一个 JobVertex 代表一个逻辑计划的节点，就是 DAG 图上的顶点，有点类似于 Storm 的 bolt 或 spout，生成一个 JobVertex 的逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//StreamingJobGenerator line258</span></div><div class="line"><span class="function"><span class="keyword">private</span> StreamConfig <span class="title">createJobVertex</span><span class="params">(</span></span></div><div class="line">      Integer streamNodeId,</div><div class="line">      Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes) &#123;</div><div class="line"></div><div class="line">   JobVertex jobVertex;</div><div class="line">   ...</div><div class="line">   JobVertexID jobVertexId = <span class="keyword">new</span> JobVertexID(hash);</div><div class="line"></div><div class="line">   <span class="keyword">if</span> (streamNode.getInputFormat() != <span class="keyword">null</span>) &#123;</div><div class="line">      jobVertex = <span class="keyword">new</span> InputFormatVertex(</div><div class="line">            chainedNames.get(streamNodeId),</div><div class="line">            jobVertexId);</div><div class="line">      TaskConfig taskConfig = <span class="keyword">new</span> TaskConfig(jobVertex.getConfiguration());</div><div class="line">      taskConfig.setStubWrapper(<span class="keyword">new</span> UserCodeObjectWrapper&lt;Object&gt;(streamNode.getInputFormat()));</div><div class="line">   &#125; <span class="keyword">else</span> &#123;</div><div class="line">      jobVertex = <span class="keyword">new</span> JobVertex(</div><div class="line">            chainedNames.get(streamNodeId),</div><div class="line">            jobVertexId);</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   jobVertex.setInvokableClass(streamNode.getJobVertexClass());</div><div class="line"></div><div class="line">   ...</div><div class="line"></div><div class="line">   <span class="keyword">return</span> <span class="keyword">new</span> StreamConfig(jobVertex.getConfiguration());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><em>这里有两段逻辑值得注意，第一是数据源节点的判断，第二是运行时执行类 InvokableClass 的设置</em></p>
<p><code>streamNode.getInputFormat()</code>是判断是否是数据源节点的逻辑，如果是数据源节点，这里会将用户代码【这里为 InputFormat.class 的子类】设置进 JobVertex 的配置中，并在 JobManager 执行提交作业命令的时候做初始化，会在 Flink 物理计划生成一节介绍。</p>
<p><code>jobVertex.setInvokableClass</code>是设置运行时的执行类，通过这个类再调用用户定义的 operator，是 flink task 中真正被执行的类，具体会在 flink-task-runtime 一节中详细介绍。</p>
<p>至此 JobGraph 生成，并扔给 JobManager 执行😝</p>
]]></content>
    
    <summary type="html">
    
      Flink 源码，包括基本组件和客户端逻辑计划的生成
    
    </summary>
    
      <category term="Flink源码" scheme="https://danny0405.github.io/categories/Flink%E6%BA%90%E7%A0%81/"/>
    
    
      <category term="Flink内部实现" scheme="https://danny0405.github.io/tags/Flink%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0/"/>
    
  </entry>
  
</feed>
